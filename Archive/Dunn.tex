Dunn : Regression Models for Method Comparison Data
In many models,  naïve assumptions are required to overcome issues of identifiabilty.
Precision is defined by the reciprocal of the variance of the random errors. 
Also it is assumed that the error variance is independent of the amount of material being measured. However, in practice, this is often not the case. Variability increases over the scale of measurements over many cases.
Estimators of scale parameters are estimable only if the analyst is prepared to make naïve, if not unacceptable, assumptions.
Equation 4 
$\psi$  and $\varepsilon$ are statistically independent of each other.
Contamination effect that arises from non-specificity /  specimen specific bias.
Random error is measured by ε. Homogenity of variances is assumed.
If there are no replicate measures,  both variances are completely confounded, and there is no way of telling them apart.
Scaling of new measurements is measured by β.
\subsection{Sampling Protocols}
Dunn discusses the sampling protocols in depth. Consider a random sample of N specimens. A simple design is a  set of measurements on each specimen using each of the two methods, yield 2N measurments. Dunn remarks that such a design would not yield much in the way of information.
The criticism projected at the correlation coefficient is only valid if one is specifically interested in assessing “agreement”. However, it should be used as an exploratory tool in the first instance.
 Exchangealility encompasses the qualities of similar precision.


Deming Regression
The appropriate estimates were derived by Kummel (1879), but were popularized in the context of medical statistics and clinical chemistry by Deming (1943).
Orthogonal regression describes the special case where the variance ratio is equal to one.
