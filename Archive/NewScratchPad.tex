
%====================================================================%
\newpage
\subsection*{Beckerman - Conclusions}


- Diagnostic methods are useful for assessing the
adequacy of assumptions underlying a modeling process
and for identifying unexpected characteristics of
the data that may seriously influence conclusions or
require special attention. Such methods may also
serve as the initial step in the determination of the
robustness of a sample.
- In recent years, deleting observations has become
a popular and extremely useful basis for studying
sensitivity in statistical problems. The deletion of observations,
however, is just one possible way in which
a postulated model might be perturbed meaningfully.
- The methodology discussed in this article is designed
to allow an assessment of the sensitivity of a mixedmodel
analysis to perturbations in many of the standard
assumptions. Results from sensitivity analyses
must be treated with caution, and the sensitivity itself
must become part of the conclusions. 
- When perturbing error variances, the cause of the sensitivity
can often be traced via hmax to a few influential observations,
and in such cases the proposed methodology
agrees well with case deletion. 
- The usefulness of case
deletion, however, seems limited to providing an understanding
only of certain aspects of the error component
of a mixed model. For example, case deletion
will not generally lead to the identification of influential
groups associated with a variance component. 
- As
briefly described in Section 3, the proposed methodology
can be adapted easily to handle such concerns.



%====================================================================%
\newpage

\subsection*{Roy’s Tests (Roy 2009)}
=============================
Roy 2009 devised an LME based Testing approach to the MCS problem, based on earlier work by Hamlett et al. 
Roy 2009 presents a series of three formal hypothesis tests for assessing agreement between two methods of measurement.
Roy also alludes to some of the current shortcomings of the approach.

### Components of Test

Comparing different model specifications with LRT tests

### Papers:
- Roy 2007
- Roy 2009
- Hamlett et al.
- Roy Leiva 2011


%====================================================================%
\newpage
\subsection*{lme4 package}

lme4 is a project led by Douglas Bates (one of the co-authors of nlme), looking at modernizing the code and
making room for trying new ideas. On the positive side, it seems to be a bit faster than nlme and it deals 
a lot better with cross-classified random factors. lme4 doesn’t deal with covariance and correlation structures yet.

One difference between the two packages is that nlme reports standard deviations instead of variances for
the random effects.

### Links 
- http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf
- http://www.unt.edu/rss/class/Jon/Benchmarks/LinearMixedModels_JDS_Dec2010.pdf

%====================================================================%
\newpage
http://medicina.med.up.pt/im/trabalhos_10_11/Sites/Turma4/article.pdf


ABSTRACT
Our main aim is to assess if the Bland Altman‟s method has been correctly applied in articles and
proceeding papers. From the 18360 documents indexed on ISI which quote the method, we excluded
1071 because we were interested in documents that could have applied the method, which means
that only articles and proceeding papers were considered, in a total of 17289 documents. From those,
we randomly selected a sample of 70 articles (to guarantee that all documents could be read and
analyzed and, simultaneously, to have an amount big enough to enable us to draw valid conclusions)
using SPSS program and assigned 35 to each class, 3 for each student. The data were collected
using a checklist and each document was analyzed twice by two different students to verify the interobserver
reproducibility of the checklist. After that we built a data base on SPSS with the collected
data and used a chi-square statistical test to data analysis. In the results we estimated, with a
confidence interval of 95%, the proportion of articles that applied the method and the proportion of
articles that applied each of the checklist‟s conditions, sorted by impact factor of the journals where
they were published and publication year. We found that 91% of our sample indeed applied Bland
Altman‟s method, but only 14% have correctly applied the method, fulfilling all of the checklist‟s
conditions. On the other hand, we can only take conclusion about one of the checklist‟s items (the
second assumption) because only in that case we obtained a p-value less than 0.05, being able to
say that the more recent articles verify the second assumption more often than the older ones, and
that was in line with what we have expected.

%====================================================================%
\newpage
MethComp Package - Compute Lin's Total deviation index

### Description

This index calculates a value such that a certain fraction of difference between methods will be numerically smaller than this.

### Usage

<pre><code>
TDI( y1, y2, p = 0.05, boot = 1000, alpha = 0.05 )
</code></pre>

### Arguments

* y1	- Measurements by one method.
* y2	- Measurements by the other method
* p	- The fraction of items with differences numerically exceeding the TDI
* boot	- If numerical, this is the number of bootstraps. If FALSE no confidence interval for the TDI is produced.
* alpha	 - 1 - confidence degree.




%====================================================================%
\newpage
\subsection*{ A Multi-Rate nonparametric test of agreement and corresponding agreement plot}

- Published in: Computational Statistics and Data Analysis 54(2010)109-119
- Author: Alan D. Hutson, University of Buffalo

Survival analytical techniques were used to assess agreement of a quantitative variable

*J Llorca, M Delgado-Rodríguez - Journal of clinical epidemiology*

### Overview
1. **Background and Objective:** Survival–agreement plots have been suggested as a new graphical approach to assess agreement in
quantitative variables. We propose that survival analytical techniques can complement this method, providing a new analytical insight
for agreement.
2. **Methods:** Two survival–agreement plots are used to detect the bias between to measurements of the same variable. The presence of
bias is tested with log-rank test, and its magnitude with Cox regression.
3. **Results:** An example on C-reactive protein determinations shows how survival analytical methods would be interpreted in the context
of assessing agreement.
4. **Conclusion:** Log-rank test, Cox regression, or other analytical methods could be used to assess agreement in quantitative variables;
correct interpretations require good clinical sense

<hr>
- In clinical or epidemiologic research, the measurement of variables always implies some degree of error.
Because it is impossible to control the various sources of variation, the assessment of the reliability of a
measurement is essential. 
- Otherwise, concordance analysis must take into account the "clinical" interpretation 
of the measurement under study, because its practical usefulness is of central importance. In this article, 
we propose a new approach to assess the reliability of a quantitative measurement. We use a graphical approach 
familiar to statisticians and data analysts of the biomedical area, associating to it the useful feature of 
interpretation based on the proportion of concordant cases. 
- We believe that the proposed graphical approach 
can serve as a complement, or as a alternative, to the Altman-Bland method for agreement analysis. It allows
a simple interpretation of agreement that takes into account the "clinical" importance of the differences 
between observers or methods. 
-In addition, it allows the analysis of reliability or agreement, by means of survival analysis techniques.

\subsubsection*{Discussion}
The proposed approach presents advantages and disadvantages compared to the Altman-Bland proposal or to
mountain plot. These two approaches, depending on the research interests, can complement or serve as alternative
one to another. For small data sets, the Altman-Bland approach should be used, and the incorporation of any index
would be of little value to the analysis.In general, the use of measures of agreement—particularly
in the case of a quantitative variable—is difficult, because these measures are calculated, and interpreted, almost
strictly, from a statistical standpoint. The proposed approach has the advantage of expressing, by means of a graphic, an
index that is easily interpreted, and depends upon the degree of relevance of the agreement, as judged by the researcher.

Moreover, the form of the resulting step function can also yield much information: very high steps indicating that a
Fig. 2. Proportion of discordance between systolic blood pressure by two observrs (J and R) and a semiautomatic device (S) until “tolerance” limits.
better agreement will be reached more rapidly, that is, for smaller differences. However, in choosing the module of the difference,
for example, we loose sight of some characteristics of the differences, very much evident in the Altman-Bland
approach or in the mountain plot. These characteristics can have a great impact in the study of agreement [21]. 

The average of the differences can serve as an estimate of the bias among the methods. Besides the bias, a tendency in the
differences as a function of the magnitude of the measurement or an increase of the differences indicating a greater
error due to the measurement are not considered in the proposed approach, because it does not take into account
the magnitude of the measurement. Nevertheless, if the magnitude of the bias does not have “clinical” relevance, it
might be that the absence of this information may not impair the analysis.

On the other hand, if considering the magnitude of the measurement is of importance, the proposed graphic could
take it into account through the calculation of relative differences.

For instance, in the X-axis of the graphic, instead of showing the modules of the differences, we would present the
differences relative to the magnitude of the measurement,
as follows:
%% EQUATION |AB| / [(AB)/2]
Therefore, the graphic would be completely adimensional, what could be an advantage, for the sake of comparison.
Although our proposal is descriptive in nature, it allows the use of inference resources, by means of tests associated to
the Kaplan-Meier analysis. It is possible, for example, to use the log-rank test to evaluate whether the difference between
two curves of agreement, for a certain categorical covariate is statistically significant. The sample size must
be always taken into account, in particular, when the sample is subdivided.

\newpage

This approach takes advantage of readily avilable tests of uniformity found in most statistical software packages.
Such tests include the KS d statistic, the Anderson Darling Statistic and the Cramer-Von Mises statistical test for univariate data.

An important aspect of this approach is the "Agreement Region".


%======================================================================================%
http://iopscience.iop.org/0026-1394/44/5/015


% - http://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/balt?action=AttachFile&do=get&target=balt.pdf
% - Eric Fèvre

\subsection*{Tukey Mean Plot}
% - Including the Tukey Mean-Difference (Bland–Altman) Plot in a Statistics Course
% - Marcin Kozak1 andAgnieszka Wnuk2
% - http://onlinelibrary.wiley.com/doi/10.1111/test.12032/abstract
The Tukey mean-difference plot, also called the Bland–Altman plot, is a recognized graphical tool in the exploration of biometrical data. We show that this technique deserves a place on an introductory statistics course by encouraging students to think about the kind of graph they wish to create, rather than just creating the default graph for the variables types they have. This graphical technique is described, and two examples are presented: one dealing with official agricultural data of Poland and the other one with an experiment on anorexia. Our opinion is that the plot is so easy and yet efficient in visualizing paired data that it should be included in statistics courses to support understanding and interpretation of data and their analysis. 
© 2014 The Authors. Teaching Statistics © 2014 Teaching Statistics Trust

% http://medicina.med.up.pt/im/trabalhos_10_11/Sites/Turma4/article.pdf


A replicate measurement case for the conversion problem may be investigated by using a data simulated on the

here we add make three copies of the both data sets, and add a small amount of noise to a numeric vector ( colloquially known as 
"jitter" to each)

jitter(lewis)


% https://books.google.ie/books?id=FJd6BAAAQBAJ&pg=PA186&lpg=PA186&dq=bland+altman+conversion+problems&source=bl&ots=ir2u2Qdl10&sig=WhKvuvY63S5moXaPW1tZ4VzUDgk&hl=en&sa=X&ei=lkDzVMr7IOOc7gaS0YAI&ved=0CDQQ6AEwAw#v=onepage&q=bland%20altman%20conversion%20problems&f=false

%% page 47

William Clark described the Clarke EGA in Baibetes care for thie first time The EGA aims to quanitify the clinical signiginciance of blood glucose estimaites
between self-blood glucouse monitors as comapred to glouceome determination

\subsection*{Bayesian Approach: }
consistent with philosophy of hypothesis testing free stats, but it is unclear what Bland and Altman had in mind.
Schluter(2009) develops a multivariate hierarchical Bayesian approach to Bland-Altman’s methodology, presenting two methods of analysis that complement pre-existing literature. 


\newpage

\section*{MAY 2012 : Research Notes}
The matter of how well two methods of measurement are said to be “in agreement” is known as a method comparison study, and is prevalent in medical statistics literature.  Barnhart et al give examples of the diversity of applications of method comparison studies; blood plasma data, body fat, oximetry data etc. 
Often the questions extends to whether or not the two methods of measurements have sufficiently levels of agreement that they could be used interchangeably. However, as pointed out by Mantha et al, interchangeability is not the goal of many studies.
A useful, and broadly consistent, set of definitions of what this “agreement” entail is put forth by Barnhart et al and Roy (2009), and shall be discussed in detail.
Shared with previous contributions (Bland and Altman, Carstensen) is the condition that there should no systematic tendency for one of the methods to consistently provide a value higher that than of the other method. If such a tendency did exist, we would refer to it as an inter-method bias.
In earlier literature, the emphasis was placed up on single measurements simultaneously by each of the methods of measurement. Several different approaches, such as the Bland-Altman plot, and Deming Regression have been proposed. 
Altman and Bland (1983) developed a simple graphical approach for assessing agreement,  whereby the case-wise means are plotted against the case-wise differences. The mean of the case-wise differences, i.e. the inter-method bias is then drawn as a horizontal line across the plot. 
Bland and Altman (1986) extended this graphical plot with what has become the most influential approach to MCS ; the Limits of Agreement. The Limits of Agreement are a tolerance interval for the case-wise differences defined as the average difference ±2Sd. Two method of measurement are said to be in agreement if the limits of agreement are not “clinically important”.
Because the results obtained are summary statistics, confidence intervals are required for generalization to the population of case-wise differences. Formulations of confidence intervals for derived values are provided by Bland-Altman (1999).
Alternative Approaches
Kelly (1985) advocates the use of a generalization of regression analysis, known as Structural Equation Modelling to assess both intra-method agreement and inter-method agreement. This approach was criticised by Bland and Altman on the basis that it did not sufficiently address the issue of precision.
\subsection*{Criticism of the Bland-Altman approach.}
Arguably, for the single replicate case, the established methodologies are sufficient for assessing agreement between two methods.
The Bland Altman approach has come in for criticism from several authors.  Graham Dunn criticizes the lack of formal testing.  There is little guidance on how to deal with outliers.
Indeed a precise set of instructions on how to correctly use the methodology has not been set out. In a survey of published papers, Mantha et al criticizes the absence of pre-specified threshold values for limits of agreement.
The use of structural equation modelling as an alternative to the Bland-Altman methodology was posited by several authors (Kelly 1985, Dunn, Hopkins) . Indeed Kelly’s approach generated an exchange of letters.                                                
An early contribution to MCS literature was made by Grubbs 1948, who proposed ANOVA models to determine the appropriate estimates. Grubb’s approach was later extended by Carstensen et al 2008 to account for replicate measurements, and to allow for the computation of Limits of Agreement.
\subsection*{MCS with Replicate measurements}
In subsequent contributions, the matter of assessing agreement in the presence  of replicate measurements was addressed. Some approaches extended already established approaches (Bland-Altman 1999).  Other contributions were based on methodologies not seen previously in Method comparison Study Literature  (for example, Carstensen et al 2008 and Roy 2009, using LME models). 
A review of recent literature demonstrates how useful and effective the use of LME models are.
More recent contributions to method comparison literature are Total Deviation Index and the Coverage Probability, introduced by Lin 2000, and extended by Lin 2002 and Choudhary &Nagaraja 2007
Intra-method agreements/repeatability
The matter of intra-method agreement, also known as repeatability, has received greater attention in recent contributions. Intra-method agreement describes a method of measurement giving a consistent set of measurements for the same item under identical conditions, such that variability of these measurements can be assumed to be caused by random error only. Many authors argue that the
Bland and Altman 1999 propose the coefficient of repeatability (CR) as a measure of intra-method agreement. Recent literature has set out as a one of the conditions for agreement that two methods must have comparable coefficients of repeatability, or an equivalent condition thereof (Roy 2009).
\subsection*{Deming Regression}
Deming Regression describes a type of regression model where error is assumed for both variables, rather than just one of them. One of the key drawbacks is that the variance ratio, the ratio of the residual variances must be specified( equivalently the ratio of the respective coefficient of variability).
Orthogonal regression is a special case of Deming Regression whereby the residual variances are assumed to be equal.  In the case of single measurements, a practitioner would be forced to make this assumption. If it was found the coefficients of repeatability were known for both methods, the variance ratio could be computed accordingly as the ratio of CRs.  
This begets the question as to how should a Deming regression be performed, given that there are replicates measurements for each item. Linear Mixed Effects models are the most plausible solution to this.
\subsection*{Probability Based Approaches}
More recently, agreement between two measurement systems can be determined using the total deviation index (TDI) or the coverage probability (CP) criteria, as proposed by Lin (2000) and Lin et al. (2002). Based on Lin et al's approach, an LME model was proposed by Choudhary (2007). 
Roy 2009 paper

Roy (2009) proposes a suite of hypothesis tests for assessing the agreement of two methods of measurement, when replicate measurements are obtained for each item, using a LME approach. (An item would commonly be a patient).  
Two methods of measurement can be said to be in agreement if there is no significant difference between in three key respects. Firstly, there is no inter-method bias between the two methods, i.e. there is no persistent tendency for one method to give higher values than the other.
Secondly, both methods of measurement have the same  within-subject variability. In such a case the variance of the replicate measurements would consistent for both methods.
Lastly, the methods have equal between-subject variability.  Put simply, for the mean measurements for each case, the variances of the mean measurements from both methods are equal.
\subsection*{Testing for Inter-method Bias}
Firstly, a practitioner would investigate whether a significant inter-method bias is present between the methods. This bias is specified as a fixed effect in the LME model.  For a practitioner who has a reasonable level of competency in R and undergraduate statistics (in particular simple linear regression model) this is a straight-forward procedure.
Reference Model (Ref.Fit)
Conventionally LME models can be tested using Likelihood Ratio Tests, wherein a reference model is compared to a nested model.

\begin{verbatim}
> Ref.Fit = lme(y ~ meth-1, data = dat,   #Symm , Symm#
+     random = list(item=pdSymm(~ meth-1)), 
+     weights=varIdent(form=~1|meth),
+     correlation = corSymm(form=~1 | item/repl), 
+     method="ML")
\end{verbatim}
Roy(2009) presents two nested models that specify the condition of equality as required, with a third nested model for an additional test. There three formulations share the same structure, and can be specified by making slight alterations of the code for the Reference Model.
Nested Model (Between-Item Variability)

\begin{verbatim}
> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
+     random = list(item=pdCompSymm(~ meth-1)),
+     correlation = corSymm(form=~1 | item/repl), 
+     method="ML")
\end{verbatim}




Nested Model (Within –item Variability)
\begin{verbatim}
> NMW.fit = lme(y ~ meth-1, data = dat,   #Symm , CS# 
+     random = list(item=pdSymm(~ meth-1)),
+     weights=varIdent(form=~1|meth), 
+     correlation = corCompSymm(form=~1 | item/repl), 
+     method="ML")
\end{verbatim}

Nested Model (Overall Variability)
Additionally there is a third nested model, that can be used to test overall variability, substantively a a joint test for between-item and within-item variability. The motivation for including such a test in the suite is not clear, although it does circumvent the need for multiple comparison procedures in certain circumstances, hence providing a simplified procedure for non-statisticians.
\begin{verbatim}
> NMO.fit = lme(y ~ meth-1, data = dat,   #CS , CS# 
+     random = list(item=pdCompSymm(~ meth-1)), 
+     correlation = corCompSymm(form=~1 | item/repl), 
+     method="ML")
\end{verbatim}

ANOVAs  for  Original Fits
The likelihood Ratio test is very simple to implement in R. All that is required it to specify the reference model and the relevant nested mode as arguments to the command anova().
The figure below displays the three tests described by Roy (2009).

\begin{verbatim}
> testB    = anova(Ref.Fit,NMB.fit)                          # Between-Subject Variabilities
> testW   = anova(Ref.Fit,NMW.fit)                        # Within-Subject Variabilities
> testO     = anova(Ref.Fit,NMO.fit)                        # Overall Variabilities
\end{verbatim}

\subsection*{Using ML or REML Fitting}
Noticeably Roy (2009) uses ML estimation when specifying the LME models. No explanation is given, although plausibly it is due to the constraints of the computational environment being used.
Both West et al (2010) and Pinheiro\& Bates (2000) compare ML and REML estimation, describing what types of tests are appropriate for each.  When variance components are being tested, REML estimation is in fact the correct approach. 
However, Choudhary \& Nagaraja(2007) point out  that for a joint test of Fixed and Random effects that ML estimation is the appropriate estimation method.


Comparison of ML and REML fits
Fit 1 (ML)

Dataset: Blood RS

Fixed : 127.3126 , 143.0275

AIC: 4075.594

Between Subject Variability

[■(〖30.28301〗^2&0.83×30.28301×31.55595@..&〖31.35595〗^2 )]	Fit1r (REML)

Dataset: Blood RS

Fixed : 127.3126 , 143.0275

AIC: 4068.172

Between Subject Variability

[■(〖30.10195〗^2&0.83×30.10195×31.16573@..&〖31.16573〗^2 )]


\begin{verbatim}
# Systolic blood pressure measurements made 
# simultaneously by two observers (J and R) 
# and an automatic blood pressure measuring
# machine (S), each making three observations 
# in quick succession (supplied by Dr E O'Brien)

Blood = matrix(data=c(100, 106, 107, 98, 98, 111, 122, 128, 124, 108, 110, 108, 108, 112, 110, 121, 
127, 128, 76, 84, 82, 76, 88, 82, 95, 94, 98,108, 104, 104, 110, 100, 106, 127, 127, 135,124, 112, 112, 128, 112, 114, 140, 131, 124,122, 140, 124, 124, 140, 126, 139, 142, 136,116, 108, 102, 118, 110, 102, 122, 112, 112,114, 110, 112, 112, 108, 112, 130, 129, 135,100, 108, 112, 100, 106, 112, 119, 122, 122,108, 92, 100, 108, 98, 100, 126, 113, 111,100, 106, 104, 102, 108, 106, 107, 113, 111,108, 112, 122, 108, 116, 120, 123, 125, 125,112, 112, 110, 114, 112, 110, 131, 129, 122,104, 108, 104, 104, 108, 104, 123, 126, 114,106, 108, 102, 104, 106, 102, 127, 119, 126,122, 122, 114, 118, 122, 114, 142, 133, 137,100, 102, 102, 102, 102, 100, 104, 116, 115,118, 118, 120, 116, 118, 118, 117, 113, 112,140, 134, 138, 138, 136, 134, 139, 127, 113,150, 148, 144, 148, 146, 144, 143, 155, 133,166, 154, 154, 164, 154, 148, 181, 170, 166,148, 156, 134, 136, 154, 132, 149, 156, 140,
174, 172, 166, 170, 170, 164, 173, 170, 154,174, 166, 150, 174, 166, 154, 160, 155, 170,140, 144, 144, 140, 144, 144, 158, 152, 154,128, 134, 130, 128, 134, 130, 139, 144, 141,
146, 138, 140, 146, 138, 138, 153, 150, 154,146, 152, 148, 146, 152, 148, 138, 144, 131,220, 218, 220, 220, 218, 220, 228, 228, 226,208, 200, 192, 204, 200, 190, 190, 183, 184,
94, 84, 86, 94, 84, 88, 103, 99, 106,114, 124, 116, 112, 126, 118, 131, 131, 124,126, 120, 122, 124, 120, 120, 131, 123, 124,124, 124, 132, 126, 126, 120, 126, 129, 125,110, 120, 128, 110, 122, 126, 121, 114, 125,90, 90, 94, 88, 88, 94, 97, 94, 96,106, 106, 110, 106, 108, 110, 116, 121, 127,218, 202, 208, 218, 200, 206, 215, 201, 207,130, 128, 130, 128, 126, 128, 141, 133, 146,136, 136, 130, 136, 138, 128, 153, 143, 138,100, 96, 88, 100, 96, 86, 113, 107, 102,100, 98, 88, 100, 98, 88, 109, 105, 97,
124, 116, 122, 126, 116, 122, 145, 102, 137,164, 168, 154, 164, 168, 154, 192, 178, 171,100, 102, 100, 100, 104, 102, 112, 116, 116,136, 126, 122, 136, 124, 122, 152, 144, 147,114, 108, 122, 114, 108, 122, 141, 141, 137,148, 120, 132, 146, 130, 132, 206, 188, 166,160, 150, 148, 160, 152, 146, 151, 147, 136,84, 92, 98, 86, 92, 98, 112, 125, 124,156, 162, 152, 156, 158, 152, 162, 165, 189,110, 98, 98, 108, 100, 98, 117, 118, 109,100, 106, 106, 100, 108, 108, 119, 131, 124,100, 102, 94, 100, 102, 96, 136, 116, 113,86, 74, 76, 88, 76, 76, 112, 115, 104,106, 100, 110, 106, 100, 108, 120, 118, 132,108, 110, 106, 106, 118, 106, 117, 118, 115,168, 188, 178, 170, 188, 182, 194, 191, 196,166, 150, 154, 164, 150, 154, 167, 160, 161,146, 142, 132, 144, 142, 130, 173, 161, 154,204, 198, 188, 206, 198, 188, 228, 218, 189,96, 94, 86, 96, 94, 84, 77, 89, 101,134, 126, 124, 132, 126, 124, 154, 156, 141,138, 144, 140, 140, 142, 138, 154, 155, 148,134, 136, 142, 136, 134, 140, 145, 154, 166,156, 160, 154, 156, 162, 156, 200, 180, 179,124, 138, 138, 122, 140, 136, 188, 147, 139,114, 110, 114, 112, 114, 114, 149, 217, 192,112, 116, 122, 112, 114, 124, 136, 132, 133,112, 116, 134, 114, 114, 136, 128, 125, 142,202, 220, 228, 200, 220, 226, 204, 222, 224,132, 136, 134, 134, 136, 132, 184, 187, 192,158, 162, 152, 158, 164, 150, 163, 160, 152,88, 76, 88, 90, 76, 86, 93, 88, 88,170, 174, 176, 172, 174, 178, 178, 181, 181,182, 176, 180, 184, 174, 178, 202, 199, 195,112, 114, 124, 112, 112, 126, 162, 166, 148,120, 118, 120, 118, 116, 120, 227, 227, 219,110, 108, 106, 110, 108, 106, 133, 127, 126,112, 112, 106, 112, 110, 106, 202, 190, 213,154, 134, 130, 156, 136, 132, 158, 121, 134,
116, 112, 94, 118, 114, 96, 124, 149, 137,108, 110, 114, 106, 110, 114, 114, 118, 126,106, 98, 100, 104, 100, 100, 137, 135, 134,122, 112, 112, 122, 114, 114, 121, 123, 128), 
nrow = 85, ncol = 9, byrow = TRUE,
dimnames = list(NULL, c("J1","J2","J3","R1","R2","R3","S1","S2","S3")) )
\end{verbatim}

\begin{verbatim}
#####################################################################
#Preparing the Blood Data
library(nlme)
blood = groupedData( y ~ meth | item ,
data = data.frame( y = c(Blood), item = c(row(Blood)),
meth = rep(c("J","R","S"), rep(nrow(Blood)*3, 3)),
repl = rep(rep(c(1:3), rep(nrow(Blood), 3)), 3) ),
labels = list(BP = "Systolic Blood Pressure", method = "Measurement Device"),
order.groups = FALSE )

#pick out two of the three methods ( use J and S ) 

dat = subset(blood, subset = meth != "R")

\end{verbatim}

\newpage


\subsection{Agreement R Pacakge}

- Coverage Probability 
- TDI

<hr>
<pre><code>
#Initialise Packages

install.packages("Agreement")
</code></pre>
### Agreement: Statistical Tools for Measuring Agreement

- This package computes several statistics for measuring agreement, for example, mean square deviation (MSD), total deviation index (TDI) or concordance correlation coefficient (CCC). 
- It can be used for both continuous data and categorical data for multiple raters and multiple readings cases.
<hr>
Agreement Measurements Summary 

Under the normal or log-normal distribution, each of the agreement measurements (MSD, CCC, TDI, and CP) 
basically measures the same information but from different perspectives. Note that the asymptotic variances 
of MSD, CCC, preci- sion, and accuracy were derived from the covariance matrix of
the sample moments, in which the normality assumption was used, even though the point estimates do not 
depend on the normality assumption. None of the methods proposed in this article is expected to be 
robust against outliers and/or large deviation from normality. The interpretation of MSD is 
difficult to understand. The TDI is desirable as an alternative because of its straightforward
interpretation. 

\newpage
Contents
Dunn’s Criticism of BA	1
Which method of measurement is more precise?	1
Bland –Altman 1999	2
What are Tolerance Intervals?	3
Schluter’s Bayesian Approach:	3
Escaramis et TDI approach	3
Reporting of MCS	5
Intra-method agreements/repeatability	6
Deming Regression	6
Probability Based Approaches	6
Roy 2009 paper	7
Using ML or REML Fitting	9


What is Agreement

The matter of how well two methods of measurement are said to be “in agreement” is known as a method comparison study, and is prevalent in medical statistics literature.  Barnhart et al give examples of the diversity of applications of method comparison studies; blood plasma data, body fat, oximetry data etc. 
Often the questions extends to whether or not the two methods of measurements have sufficiently levels of agreement that they could be used interchangeably. However, as pointed out by Mantha et al, interchangeability is not the goal of many studies.
A useful, and broadly consistent, set of definitions of what this “agreement” entail is put forth by Barnhart et al and Roy (2009), and shall be discussed in detail.
Shared with previous contributions (Bland and Altman, Carstensen) is the condition that there should no systematic tendency for one of the methods to consistently provide a value higher that than of the other method. If such a tendency did exist, we would refer to it as an inter-method bias.
In earlier literature, the emphasis was placed up on single measurements simultaneously by each of the methods of measurement. Several different approaches, such as the Bland-Altman plot, and Deming Regression have been proposed. 
Bland  and Altman’s Approach
Altman and Bland (1983) developed a simple graphical approach for assessing agreement,  whereby the case-wise means are plotted against the case-wise differences. The mean of the case-wise differences, i.e. the inter-method bias is then drawn as a horizontal line across the plot. 
Bland and Altman (1986) extended this graphical plot with what has become the most influential approach to MCS ; the Limits of Agreement. The Limits of Agreement are a tolerance interval for the case-wise differences defined as the average difference ±2Sd. Two method of measurement are said to be in agreement if the limits of agreement are not “clinically important”.
Because the results obtained are summary statistics, confidence intervals are required for generalization to the population of case-wise differences. Formulations of confidence intervals for derived values are provided by Bland-Altman (1999).
Dunn’s Criticism of BA
In trying to prevent widespread misuse of statistical methods, Bland and Altman have, in the view of Dunn, gone too far.
Dunn criticizes the use of the Bland-Altman methodology on the basis that it fails to identify the underlying model, and furthermore, usage of this approach requires assumptions that are "subject of serious and valid criticism"
Importantly, Dunn suggests the Bland-Altman approach as exploratory tool.
Which method of measurement is more precise?
Dunn contends that the notion of equivalence used by Bland and Altman is too restrictive, and that the Bland-Altman approach fails to address the issue of determining which of the two methods is more precise. Indeed, as many methods comparison studies are motivated by the need to compare a new method of measurement against an established one, the question of which of the two methods is more precise is definitely of interest.
Formal analysis and estimation of relative precision should be based on fitting the appropriate statistical model to the data as a whole. An advantage of which is the assumptions used are open to scrutiny.


Dunn’s Remarks on other matters	
Dunn contends that "treating method comparison studies and repeatability as separate issues is inviting trouble".

Grubbs (1948) was interested with the burning times of fuses,  later he discussed the velocity of artilerry shells (Grubbs 1973).In both cases, it is only possible to get one measurement .

Dunn 2000, and Carroll & Ruppert do not recommend the use of orthogonal regression, on the basis that variance ratio must be guessed.  In the case of replicated measurements, the variance ratio can be determined. However, it is an inefficient approach, not using the available data to the fullest.
Bland –Altman 1999
How Bland and Altman have extended the Difference Plot (with Limits of agreement) to the case of replicate measurements.
When replicate measurements are taken with each method on each item, more complex methodologies are required. Bland & Altman(1999) describe a number of approaches to adopt in such a case. The first approach is to take the average measurement over the replicates measurements on each subject by each method.
The second approach is to simply treat these replicate measurements as independent measurements in their own right, and use the pre-existing Bland-Altman approach for single measurements.
The proposed approach shares the same overall structure as their earlier work. However complex calculations are now required to compute the variance of case-wise differences. This is compounded by the fact that different approaches are required depending on whether or not there are equal or unequal numbers of replicates.
What have they missed? Carstensen et al discuss in depth the shortfalls in this approach.
Carstensen et al (2008) address the flaws in both approaches, demonstrating how the first approach would lead to an underestimation of the variance of case-wise differences. The second case would also lead to incorrect estimates, although it is pointed out that the second approach is much better than the first.

What are Tolerance Intervals?
A tolerance interval is a statistical interval within which, with some confidence level, a specified proportion of a population falls.
The Engineering Statistics Handbook describes the difference: Confidence limits are limits within which we expect a given population parameter, such as the mean, to lie. Statistical tolerance limits are limits within which we expect a stated proportion of the population to lie.
It is useful to make the distinction between tolerance intervals and confidence intervals clear. The confidence interval describes a single-valued population parameter, commonly the mean, with a specified confidence level. The tolerance interval, on the other hand, describes the range of data values that includes a specific proportion of the population.
As discussed in Vardeman (1992), the tolerance interval is not as widely used as the confidence interval and prediction interval, largely because of the emphasis placed on these in undergraduate teaching. Furthermore, Vardeman(1992) argues this lack of awareness can lead to misuse of confidence intervals where other types of intervals are more appropriate.
Curiously Carstensen et al (2008) describe the Limits of agreement as a prediction interval, although stating that it is formulated in correctly for that purpose.

Why Tolerance Intervals are appropriate?
It is clear from the definition of Tolerance intervals that they function precisely as Bland-Altman intend.

Schluter’s Bayesian Approach: 
consistent with philosophy of hypothesis testing free stats, but it is unclear what Bland and Altman had in mind.
Schluter(2009) develops a multivariate hierarchical Bayesian approach to Bland-Altman’s methodology, presenting two methods of analysis that complement pre-existing literature.
Escaramis et TDI approach

Escaramis et al propose to estimate the TDI by constructing a probability interval of the difference in paired measurements between methods, demonstrating how the statistical tolerance interval (TI) procedure as a natural way to make inferences about interval estimates.  
Alternative Approaches
Kelly (1985) advocates the use of a generalization of regression analysis, known as Structural Equation Modelling to assess both intra-method agreement and inter-method agreement. This approach was criticised by Bland and Altman on the basis that it did not sufficiently address the issue of precision.
Criticism of the Bland-Altman approach.
Arguably, for the single replicate case, the established methodologies are sufficient for assessing agreement between two methods.
The Bland Altman approach has come in for criticism from several authors.  Graham Dunn criticizes the lack of formal testing.  There is little guidance on how to deal with outliers.
Reporting of MCS
Indeed a precise set of instructions on how to correctly use the methodology has not been set out. In a survey of published papers, Mantha et al criticizes the absence of pre-specified threshold values for limits of agreement.
The use of structural equation modelling as an alternative to the Bland-Altman methodology was posited by several authors (Kelly 1985, Dunn, Hopkins) . Indeed Kelly’s approach generated an exchange of letters.                                                
An early contribution to MCS literature was made by Grubbs 1948, who proposed ANOVA models to determine the appropriate estimates. Grubb’s approach was later extended by Carstensen et al 2008 to account for replicate measurements, and to allow for the computation of Limits of Agreement.
MCS with Replicate measurements
In subsequent contributions, the matter of assessing agreement in the presence  of replicate measurements was addressed. Some approaches extended already established approaches (Bland-Altman 1999).  Other contributions were based on methodologies not seen previously in Method comparison Study Literature  (for example, Carstensen et al 2008 and Roy 2009, using LME models). 
A review of recent literature demonstrates how useful and effective the use of LME models are.
More recent contributions to method comparison literature are Total Deviation Index and the Coverage Probability, introduced by Lin 2000, and extended by Lin 2002 and Choudhary &Nagaraja 2007


Intra-method agreements/repeatability
The matter of intra-method agreement, also known as repeatability, has received greater attention in recent contributions. Intra-method agreement describes a method of measurement giving a consistent set of measurements for the same item under identical conditions, such that variability of these measurements can be assumed to be caused by random error only. Many authors argue that the
Bland and Altman 1999 propose the coefficient of repeatability (CR) as a measure of intra-method agreement. Recent literature has set out as a one of the conditions for agreement that two methods must have comparable coefficients of repeatability, or an equivalent condition thereof (Roy 2009).
Deming Regression
Deming Regression describes a type of regression model where error is assumed for both variables, rather than just one of them. One of the key drawbacks is that the variance ratio, the ratio of the residual variances must be specified( equivalently the ratio of the respective coefficient of variability).
Orthogonal regression is a special case of Deming Regression whereby the residual variances are assumed to be equal.  In the case of single measurements, a practitioner would be forced to make this assumption. If it was found the coefficients of repeatability were known for both methods, the variance ratio could be computed accordingly as the ratio of CRs.  
This begets the question as to how should a Deming regression be performed, given that there are replicates measurements for each item. Linear Mixed Effects models are the most plausible solution to this.
Probability Based Approaches
More recently, agreement between two measurement systems can be determined using the total deviation index (TDI) or the coverage probability (CP) criteria, as proposed by Lin (2000) and Lin et al. (2002). Based on Lin et al's approach, an LME model was proposed by Choudhary (2007). 
Roy 2009 paper

Roy (2009) proposes a suite of hypothesis tests for assessing the agreement of two methods of measurement, when replicate measurements are obtained for each item, using a LME approach. (An item would commonly be a patient).  
Two methods of measurement can be said to be in agreement if there is no significant difference between in three key respects. Firstly, there is no inter-method bias between the two methods, i.e. there is no persistent tendency for one method to give higher values than the other.
Secondly, both methods of measurement have the same  within-subject variability. In such a case the variance of the replicate measurements would consistent for both methods.
Lastly, the methods have equal between-subject variability.  Put simply, for the mean measurements for each case, the variances of the mean measurements from both methods are equal.
Testing for Inter-method Bias
Firstly, a practitioner would investigate whether a significant inter-method bias is present between the methods. This bias is specified as a fixed effect in the LME model.  For a practitioner who has a reasonable level of competency in R and undergraduate statistics (in particular simple linear regression model) this is a straight-forward procedure.
Reference Model (Ref.Fit)
Conventionally LME models can be tested using Likelihood Ratio Tests, wherein a reference model is compared to a nested model.
> Ref.Fit = lme(y ~ meth-1, data = dat,   #Symm , Symm#
+     random = list(item=pdSymm(~ meth-1)), 
+     weights=varIdent(form=~1|meth),
+     correlation = corSymm(form=~1 | item/repl), 
+     method="ML")

Roy(2009) presents two nested models that specify the condition of equality as required, with a third nested model for an additional test. There three formulations share the same structure, and can be specified by making slight alterations of the code for the Reference Model.
Nested Model (Between-Item Variability)
> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
+     random = list(item=pdCompSymm(~ meth-1)),
+     correlation = corSymm(form=~1 | item/repl), 
+     method="ML")





Nested Model (Within –item Variability)
> NMW.fit = lme(y ~ meth-1, data = dat,   #Symm , CS# 
+     random = list(item=pdSymm(~ meth-1)),
+     weights=varIdent(form=~1|meth), 
+     correlation = corCompSymm(form=~1 | item/repl), 
+     method="ML")


Nested Model (Overall Variability)
Additionally there is a third nested model, that can be used to test overall variability, substantively a a joint test for between-item and within-item variability. The motivation for including such a test in the suite is not clear, although it does circumvent the need for multiple comparison procedures in certain circumstances, hence providing a simplified procedure for non-statisticians.
> NMO.fit = lme(y ~ meth-1, data = dat,   #CS , CS# 
+     random = list(item=pdCompSymm(~ meth-1)), 
+     correlation = corCompSymm(form=~1 | item/repl), 
+     method="ML")


ANOVAs  for  Original Fits
The likelihood Ratio test is very simple to implement in R. All that is required it to specify the reference model and the relevant nested mode as arguments to the command anova().
The figure below displays the three tests described by Roy (2009).
> testB    = anova(Ref.Fit,NMB.fit)                          # Between-Subject Variabilities
> testW   = anova(Ref.Fit,NMW.fit)                        # Within-Subject Variabilities
> testO     = anova(Ref.Fit,NMO.fit)                        # Overall Variabilities



Using ML or REML Fitting
Noticeably Roy (2009) uses ML estimation when specifying the LME models. No explanation is given, although plausibly it is due to the constraints of the computational environment being used.
Both West et al (2010) and Pinheiro & Bates (2000) compare ML and REML estimation, describing what types of tests are appropriate for each.  When variance components are being tested, REML estimation is in fact the correct approach. 
However, Choudhary & Nagaraja(2007) point out  that for a joint test of Fixed and Random effects that ML estimation is the appropriate estimation method.


Comparison of ML and REML fits
Fit 1 (ML)

Dataset: Blood RS

Fixed : 127.3126 , 143.0275

AIC: 4075.594

Between Subject Variability

[■(〖30.28301〗^2&0.83×30.28301×31.55595@..&〖31.35595〗^2 )]	Fit1r (REML)

Dataset: Blood RS

Fixed : 127.3126 , 143.0275

AIC: 4068.172

Between Subject Variability

[■(〖30.10195〗^2&0.83×30.10195×31.16573@..&〖31.16573〗^2 )]



# Systolic blood pressure measurements made 
# simultaneously by two observers (J and R) 
# and an automatic blood pressure measuring
# machine (S), each making three observations 
# in quick succession (supplied by Dr E O'Brien)

Blood = matrix(data=c(100, 106, 107, 98, 98, 111, 122, 128, 124, 108, 110, 108, 108, 112, 110, 121, 
127, 128, 76, 84, 82, 76, 88, 82, 95, 94, 98,108, 104, 104, 110, 100, 106, 127, 127, 135,124, 112, 112, 128, 112, 114, 140, 131, 124,122, 140, 124, 124, 140, 126, 139, 142, 136,116, 108, 102, 118, 110, 102, 122, 112, 112,114, 110, 112, 112, 108, 112, 130, 129, 135,100, 108, 112, 100, 106, 112, 119, 122, 122,108, 92, 100, 108, 98, 100, 126, 113, 111,100, 106, 104, 102, 108, 106, 107, 113, 111,108, 112, 122, 108, 116, 120, 123, 125, 125,112, 112, 110, 114, 112, 110, 131, 129, 122,104, 108, 104, 104, 108, 104, 123, 126, 114,106, 108, 102, 104, 106, 102, 127, 119, 126,122, 122, 114, 118, 122, 114, 142, 133, 137,100, 102, 102, 102, 102, 100, 104, 116, 115,118, 118, 120, 116, 118, 118, 117, 113, 112,140, 134, 138, 138, 136, 134, 139, 127, 113,150, 148, 144, 148, 146, 144, 143, 155, 133,166, 154, 154, 164, 154, 148, 181, 170, 166,148, 156, 134, 136, 154, 132, 149, 156, 140,
174, 172, 166, 170, 170, 164, 173, 170, 154,174, 166, 150, 174, 166, 154, 160, 155, 170,140, 144, 144, 140, 144, 144, 158, 152, 154,128, 134, 130, 128, 134, 130, 139, 144, 141,
146, 138, 140, 146, 138, 138, 153, 150, 154,146, 152, 148, 146, 152, 148, 138, 144, 131,220, 218, 220, 220, 218, 220, 228, 228, 226,208, 200, 192, 204, 200, 190, 190, 183, 184,
94, 84, 86, 94, 84, 88, 103, 99, 106,114, 124, 116, 112, 126, 118, 131, 131, 124,126, 120, 122, 124, 120, 120, 131, 123, 124,124, 124, 132, 126, 126, 120, 126, 129, 125,110, 120, 128, 110, 122, 126, 121, 114, 125,90, 90, 94, 88, 88, 94, 97, 94, 96,106, 106, 110, 106, 108, 110, 116, 121, 127,218, 202, 208, 218, 200, 206, 215, 201, 207,130, 128, 130, 128, 126, 128, 141, 133, 146,136, 136, 130, 136, 138, 128, 153, 143, 138,100, 96, 88, 100, 96, 86, 113, 107, 102,100, 98, 88, 100, 98, 88, 109, 105, 97,
124, 116, 122, 126, 116, 122, 145, 102, 137,164, 168, 154, 164, 168, 154, 192, 178, 171,100, 102, 100, 100, 104, 102, 112, 116, 116,136, 126, 122, 136, 124, 122, 152, 144, 147,114, 108, 122, 114, 108, 122, 141, 141, 137,148, 120, 132, 146, 130, 132, 206, 188, 166,160, 150, 148, 160, 152, 146, 151, 147, 136,84, 92, 98, 86, 92, 98, 112, 125, 124,156, 162, 152, 156, 158, 152, 162, 165, 189,110, 98, 98, 108, 100, 98, 117, 118, 109,100, 106, 106, 100, 108, 108, 119, 131, 124,100, 102, 94, 100, 102, 96, 136, 116, 113,86, 74, 76, 88, 76, 76, 112, 115, 104,106, 100, 110, 106, 100, 108, 120, 118, 132,108, 110, 106, 106, 118, 106, 117, 118, 115,168, 188, 178, 170, 188, 182, 194, 191, 196,166, 150, 154, 164, 150, 154, 167, 160, 161,146, 142, 132, 144, 142, 130, 173, 161, 154,204, 198, 188, 206, 198, 188, 228, 218, 189,96, 94, 86, 96, 94, 84, 77, 89, 101,134, 126, 124, 132, 126, 124, 154, 156, 141,138, 144, 140, 140, 142, 138, 154, 155, 148,134, 136, 142, 136, 134, 140, 145, 154, 166,156, 160, 154, 156, 162, 156, 200, 180, 179,124, 138, 138, 122, 140, 136, 188, 147, 139,114, 110, 114, 112, 114, 114, 149, 217, 192,112, 116, 122, 112, 114, 124, 136, 132, 133,112, 116, 134, 114, 114, 136, 128, 125, 142,202, 220, 228, 200, 220, 226, 204, 222, 224,132, 136, 134, 134, 136, 132, 184, 187, 192,158, 162, 152, 158, 164, 150, 163, 160, 152,88, 76, 88, 90, 76, 86, 93, 88, 88,170, 174, 176, 172, 174, 178, 178, 181, 181,182, 176, 180, 184, 174, 178, 202, 199, 195,112, 114, 124, 112, 112, 126, 162, 166, 148,120, 118, 120, 118, 116, 120, 227, 227, 219,110, 108, 106, 110, 108, 106, 133, 127, 126,112, 112, 106, 112, 110, 106, 202, 190, 213,154, 134, 130, 156, 136, 132, 158, 121, 134,
116, 112, 94, 118, 114, 96, 124, 149, 137,108, 110, 114, 106, 110, 114, 114, 118, 126,106, 98, 100, 104, 100, 100, 137, 135, 134,122, 112, 112, 122, 114, 114, 121, 123, 128), 
nrow = 85, ncol = 9, byrow = TRUE,
dimnames = list(NULL, c("J1","J2","J3","R1","R2","R3","S1","S2","S3")) )



#####################################################################
#Preparing the Blood Data
library(nlme)
blood = groupedData( y ~ meth | item ,
data = data.frame( y = c(Blood), item = c(row(Blood)),
meth = rep(c("J","R","S"), rep(nrow(Blood)*3, 3)),
repl = rep(rep(c(1:3), rep(nrow(Blood), 3)), 3) ),
labels = list(BP = "Systolic Blood Pressure", method = "Measurement Device"),
order.groups = FALSE )

#pick out two of the three methods ( use J and S ) 

dat = subset(blood, subset = meth != "R")





Contents
Dunn 2000 – S E M E	2
Regression based approach	3
Lai & Shiao (2005)	3
Comparing Two Clinical Measurements: a LME approach (Journal of Applied Statistics,2005)	3
What is Agreement	4
Bland  and Altman’s Approach	4
Criticism of Bland-Altman Approach	4
Dunn’s Criticism of BA	5
Which method of measurement is more precise?	5
Bland –Altman 1999	5
What are Tolerance Intervals?	6
Schluter’s Bayesian Approach:	7
Escaramis et TDI approach	7
Reporting of MCS	8
Intra-method agreements/repeatability	9
Deming Regression	9
Probability Based Approaches	9
Roy 2009 paper	10
Using ML or REML Fitting	12



Why do we not follow the error-in-variable methodology presented by Dunn(2000)

In Roy’s work, how much of a distinction is made between repeated and replicated measurements? What cases are covered by this work? If it is the case that replicated measurements are considered only, how can it be extended to cover the case of repeated measures (i.e. where a time series effect is present)?

Can Roy’s approach be extended to cover the case of “Dependent Replicates”? What sort of additional tests can be added to Roy’s methodology in such a case.

Roy considers the question of agreement and inter-changeability. Dunn(2000) considers the question of which method is better. Equivalently this is a question of comparative precision. Can Roy’s methodology be altered to provide one-sided tests?

Lai  Shaio et al use a LME approach to consider MCS problems. Is there anything in this paper that warrants inclusion in the SIM submission. 
What are the Key features? The use of extra covariates not found in other approaches.  Is it worth mentioning it as an early approach using LMEs.

Pitman Morgan testing evaluates the consistency in variances for two variables. How does it work in the case of replicated measurements?

The SIM paper is structured into the following sections : 1- Introduction to Problem, 2 – using the LME approach, 3- Extending current methodologies, 4- Implementation on a worked example, 5- discussion.

For the SIM paper – consult the “Guidelines for Authors” on the appropriate typefaces for Vectors and Matrices.



Dunn 2000 – S E M E
Error-in-variable regression 
Dunn&Roberts (1999) reviewed the application of Error-in-variable models for comparing equivalence in measurement methods.
Dunn(2002) proposes the use of Error-in-variable regression. However, the approaches proposed by this paper instead use an LME framework. The LME framework for method comparison studies has seen increased prominence over recent years (for examples, Lai  et al and Carstensen et al).
Dunn on Agreement
Dunn(2000) considers the notion of “Agreement”, as posited by Bland and Altman’s work, as too restrictive, detailing a series of arguments in support of this contention. Firstly the measurements from both methods may be based on different scales (for example Celsius and Fahrenheit) or even different variables.

Agreement and Equivalence.
Hawkins(2002) describes the matter in terms of whether or not the two methods of measurement are equivalent, pointing out that the US Food and Drug Administration use the phrase “substantially equivalent” in this context.
In response to Bland and Altman's discussion about agreement, Dunn 2000 poses searching questions about the purpose of a method comparison study. While it is useful to know if two methods of measurement are "in agreement", Dunn considers the question of which of the two methods of measurement to be the more precise to be a pertinent question.
Levels of Equivalence
Hawkins(2002) develops the notion of three levels of equivalency. At the lowest level, mean equivalency describes the case where no inter-method bias exists between two methods of measurement.  
Regression based approach
Hawkins(2002) considers standard regression techniques for the purposes of method comparison studies.  He argues that an analyst can identify deviations from model assumptions, and devise solutions accordingly.
The Morgan-Pitman procedure is a well-known test for the equality of variances.
Lai & Shiao (2005)

Comparing Two Clinical Measurements: a LME approach (Journal of Applied Statistics,2005)

Lai and Shaio (2005) use an LME framework to extend the LOA approach to assess the “equivalence” of two methods of measurement.
This approach is justified by the prevalence of statistical software packages equipped to deal with LME models. Additionally a substantial corpus of research has been developed in recent years to assess diagnostics and influence analysis for LME models (Haslett & Hayes, Zewotir, Schabenberger as some examples).
LME models can effectively be used to assess the impacts of possible factors on what causes differences. 
Using Absolute Differences
As pointed out in several of Bland and Altman’s papers, the use of absolute difference (|D|) would be a more useful approach for assessing agreement. Based on the assumption that case-wise differences are normally distributed, it follows that the absolute differences are half-normally distributed.
Lai and Shaio remark, however, that no commercially available software is available that can assist with such an approach.
What is Agreement

The matter of how well two methods of measurement are said to be “in agreement” is known as a method comparison study, and is prevalent in medical statistics literature.  Barnhart et al give examples of the diversity of applications of method comparison studies; blood plasma data, body fat, oximetry data etc. 
Often the questions extends to whether or not the two methods of measurements have sufficiently levels of agreement that they could be used interchangeably. However, as pointed out by Mantha et al, interchangeability is not the goal of many studies.
A useful, and broadly consistent, set of definitions of what this “agreement” entail is put forth by Barnhart et al and Roy (2009), and shall be discussed in detail.
Shared with previous contributions (Bland and Altman, Carstensen) is the condition that there should no systematic tendency for one of the methods to consistently provide a value higher that than of the other method. If such a tendency did exist, we would refer to it as an inter-method bias.
In earlier literature, the emphasis was placed up on single measurements simultaneously by each of the methods of measurement. Several different approaches, such as the Bland-Altman plot, and Deming Regression have been proposed. 
Bland  and Altman’s Approach
Altman and Bland (1983) developed a simple graphical approach for assessing agreement,  whereby the case-wise means are plotted against the case-wise differences. The mean of the case-wise differences, i.e. the inter-method bias is then drawn as a horizontal line across the plot. 
Bland and Altman (1986) extended this graphical plot with what has become the most influential approach to MCS ; the Limits of Agreement. The Limits of Agreement are a tolerance interval for the case-wise differences defined as the average difference ±2Sd. Two method of measurement are said to be in agreement if the limits of agreement are not “clinically important”.
Because the results obtained are summary statistics, confidence intervals are required for generalization to the population of case-wise differences. Formulations of confidence intervals for derived values are provided by Bland-Altman (1999).
Criticism of Bland-Altman Approach
The BA approach is easy to use, but limited in explanatory capacity to attribute the possible difference of the two measurement errors (Lai&Shaio,2005).
It is difficult to make any further inferences if two methods are found to be different (Lai&Shaio,2005).
The Classical BA approach assumes independent observations, which is not the case in many practical settings (Lai&Shaio,2005).
Dunn’s Criticism of BA
In trying to prevent widespread misuse of statistical methods, Bland and Altman have, in the view of Dunn, gone too far.
Dunn criticizes the use of the Bland-Altman methodology on the basis that it fails to identify the underlying model, and furthermore, usage of this approach requires assumptions that are "subject of serious and valid criticism"
Importantly, Dunn suggests the Bland-Altman approach as exploratory tool.
Which method of measurement is more precise?
Dunn contends that the notion of equivalence used by Bland and Altman is too restrictive, and that the Bland-Altman approach fails to address the issue of determining which of the two methods is more precise. Indeed, as many methods comparison studies are motivated by the need to compare a new method of measurement against an established one, the question of which of the two methods is more precise is definitely of interest.
Formal analysis and estimation of relative precision should be based on fitting the appropriate statistical model to the data as a whole. An advantage of which is the assumptions used are open to scrutiny.
Dunn’s Remarks on other matters	

Grubbs (1948) was interested with the burning times of fuses,  later he discussed the velocity of artilerry shells (Grubbs 1973).In both cases, it is only possible to get one measurement .

Dunn 2000, and Carroll & Ruppert do not recommend the use of orthogonal regression, on the basis that variance ratio must be guessed.  In the case of replicated measurements, the variance ratio can be determined. However, it is an inefficient approach, not using the available data to the fullest.
Bland –Altman 1999
How Bland and Altman have extended the Difference Plot (with Limits of agreement) to the case of replicate measurements.
When replicate measurements are taken with each method on each item, more complex methodologies are required. Bland & Altman(1999) describe a number of approaches to adopt in such a case. The first approach is to take the average measurement over the replicates measurements on each subject by each method.
The second approach is to simply treat these replicate measurements as independent measurements in their own right, and use the pre-existing Bland-Altman approach for single measurements.
The proposed approach shares the same overall structure as their earlier work. However complex calculations are now required to compute the variance of case-wise differences. This is compounded by the fact that different approaches are required depending on whether or not there are equal or unequal numbers of replicates.
What have they missed? Carstensen et al discuss in depth the shortfalls in this approach.
Carstensen et al (2008) address the flaws in both approaches, demonstrating how the first approach would lead to an underestimation of the variance of case-wise differences. The second case would also lead to incorrect estimates, although it is pointed out that the second approach is much better than the first.
Outliers 
Bland and Altman (1999) do not recommend excluding outliers from analyses, but it may be useful to assess their influence on the results by re-performing the analysis with the outliers temporarily omitted.

Repeatability
Dunn (2000) contends that "treating method comparison studies and repeatability as separate issues is inviting trouble".
Repeatability or test-retest reliability is the variation in measurements taken by a single person or instrument on the same item and under the same conditions. 
A less-than-perfect test-retest reliability causes test-retest variability. Such variability can be caused by, for example, intra-individual variability and intra-observer variability. 
A measurement may be said to be repeatable when this variation is smaller than some agreed limit.
What are Tolerance Intervals?
A tolerance interval is a statistical interval within which, with some confidence level, a specified proportion of a population falls.
The Engineering Statistics Handbook describes the difference: Confidence limits are limits within which we expect a given population parameter, such as the mean, to lie. Statistical tolerance limits are limits within which we expect a stated proportion of the population to lie.
It is useful to make the distinction between tolerance intervals and confidence intervals clear. The confidence interval describes a single-valued population parameter, commonly the mean, with a specified confidence level. The tolerance interval, on the other hand, describes the range of data values that includes a specific proportion of the population.
As discussed in Vardeman (1992), the tolerance interval is not as widely used as the confidence interval and prediction interval, largely because of the emphasis placed on these in undergraduate teaching. Furthermore, Vardeman(1992) argues this lack of awareness can lead to misuse of confidence intervals where other types of intervals are more appropriate.
Curiously Carstensen et al (2008) describe the Limits of agreement as a prediction interval, although stating that it is formulated in correctly for that purpose.

Why Tolerance Intervals are appropriate?
It is clear from the definition of Tolerance intervals that they function precisely as Bland-Altman intend.

Schluter’s Bayesian Approach: 
consistent with philosophy of hypothesis testing free stats, but it is unclear what Bland and Altman had in mind.
Schluter(2009) develops a multivariate hierarchical Bayesian approach to Bland-Altman’s methodology, presenting two methods of analysis that complement pre-existing literature.
Escaramis et TDI approach

Escaramis et al propose to estimate the TDI by constructing a probability interval of the difference in paired measurements between methods, demonstrating how the statistical tolerance interval (TI) procedure as a natural way to make inferences about interval estimates.  
Alternative Approaches
Kelly (1985) advocates the use of a generalization of regression analysis, known as Structural Equation Modelling to assess both intra-method agreement and inter-method agreement. This approach was criticised by Bland and Altman on the basis that it did not sufficiently address the issue of precision.
Criticism of the Bland-Altman approach.
Arguably, for the single replicate case, the established methodologies are sufficient for assessing agreement between two methods.
The Bland Altman approach has come in for criticism from several authors.  Graham Dunn criticizes the lack of formal testing.  There is little guidance on how to deal with outliers.
Reporting of MCS
Indeed a precise set of instructions on how to correctly use the methodology has not been set out. In a survey of published papers, Mantha et al criticizes the absence of pre-specified threshold values for limits of agreement.
The use of structural equation modelling as an alternative to the Bland-Altman methodology was posited by several authors (Kelly 1985, Dunn, Hopkins) . Indeed Kelly’s approach generated an exchange of letters.                                                
An early contribution to MCS literature was made by Grubbs 1948, who proposed ANOVA models to determine the appropriate estimates. Grubb’s approach was later extended by Carstensen et al 2008 to account for replicate measurements, and to allow for the computation of Limits of Agreement.
MCS with Replicate measurements
In subsequent contributions, the matter of assessing agreement in the presence  of replicate measurements was addressed. Some approaches extended already established approaches (Bland-Altman 1999).  Other contributions were based on methodologies not seen previously in Method comparison Study Literature  (for example, Carstensen et al 2008 and Roy 2009, using LME models). 
A review of recent literature demonstrates how useful and effective the use of LME models are.
More recent contributions to method comparison literature are Total Deviation Index and the Coverage Probability, introduced by Lin 2000, and extended by Lin 2002 and Choudhary &Nagaraja 2007


Intra-method agreements/repeatability
The matter of intra-method agreement, also known as repeatability, has received greater attention in recent contributions. Intra-method agreement describes a method of measurement giving a consistent set of measurements for the same item under identical conditions, such that variability of these measurements can be assumed to be caused by random error only. Many authors argue that the
Bland and Altman 1999 propose the coefficient of repeatability (CR) as a measure of intra-method agreement. Recent literature has set out as a one of the conditions for agreement that two methods must have comparable coefficients of repeatability, or an equivalent condition thereof (Roy 2009).
Deming Regression
Deming Regression describes a type of regression model where error is assumed for both variables, rather than just one of them. One of the key drawbacks is that the variance ratio, the ratio of the residual variances must be specified( equivalently the ratio of the respective coefficient of variability).
Orthogonal regression is a special case of Deming Regression whereby the residual variances are assumed to be equal.  In the case of single measurements, a practitioner would be forced to make this assumption. If it was found the coefficients of repeatability were known for both methods, the variance ratio could be computed accordingly as the ratio of CRs.  
This begets the question as to how should a Deming regression be performed, given that there are replicates measurements for each item. Linear Mixed Effects models are the most plausible solution to this.
Probability Based Approaches
More recently, agreement between two measurement systems can be determined using the total deviation index (TDI) or the coverage probability (CP) criteria, as proposed by Lin (2000) and Lin et al. (2002). Based on Lin et al's approach, an LME model was proposed by Choudhary (2007). 
Roy 2009 paper

Roy (2009) proposes a suite of hypothesis tests for assessing the agreement of two methods of measurement, when replicate measurements are obtained for each item, using a LME approach. (An item would commonly be a patient).  
Two methods of measurement can be said to be in agreement if there is no significant difference between in three key respects. Firstly, there is no inter-method bias between the two methods, i.e. there is no persistent tendency for one method to give higher values than the other.
Secondly, both methods of measurement have the same  within-subject variability. In such a case the variance of the replicate measurements would consistent for both methods.
Lastly, the methods have equal between-subject variability.  Put simply, for the mean measurements for each case, the variances of the mean measurements from both methods are equal.
Testing for Inter-method Bias
Firstly, a practitioner would investigate whether a significant inter-method bias is present between the methods. This bias is specified as a fixed effect in the LME model.  For a practitioner who has a reasonable level of competency in R and undergraduate statistics (in particular simple linear regression model) this is a straight-forward procedure.
Reference Model (Ref.Fit)
Conventionally LME models can be tested using Likelihood Ratio Tests, wherein a reference model is compared to a nested model.
> Ref.Fit = lme(y ~ meth-1, data = dat,   #Symm , Symm#
+     random = list(item=pdSymm(~ meth-1)), 
+     weights=varIdent(form=~1|meth),
+     correlation = corSymm(form=~1 | item/repl), 
+     method="ML")

The Variance-Covariance Matrix can be easily extracted using the “getVarCov()” function.
> getVarCov(Ref.Fit)
Random effects variance covariance matrix
methJ     methS
methJ     923.98    785.23
methS     785.23    971.29

Standard Deviations: 30.397 31.166



Another useful function is the “VarCorr()” function.
> VarCorr(Ref.Fit)
item = pdSymm(meth - 1) 
Variance  StdDev    Corr 
methJ    923.97612 30.396975 methJ
methS    971.29242 31.165565 0.829
Residual  37.40853  6.116251  

Roy’s Testing Approach
Roy(2009) presents two nested models that specify the condition of equality as required, with a third nested model for an additional test. There three formulations share the same structure, and can be specified by making slight alterations of the code for the Reference Model.

Nested Model (Between-Item Variability)
> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
+     random = list(item=pdCompSymm(~ meth-1)),
+     correlation = corSymm(form=~1 | item/repl), 
+     method="ML")


Nested Model (Within –item Variability)
> NMW.fit = lme(y ~ meth-1, data = dat,   #Symm , CS# 
+     random = list(item=pdSymm(~ meth-1)),
+     weights=varIdent(form=~1|meth), 
+     correlation = corCompSymm(form=~1 | item/repl), 
+     method="ML")


Nested Model (Overall Variability)
Additionally there is a third nested model, that can be used to test overall variability, substantively a a joint test for between-item and within-item variability. The motivation for including such a test in the suite is not clear, although it does circumvent the need for multiple comparison procedures in certain circumstances, hence providing a simplified procedure for non-statisticians.
> NMO.fit = lme(y ~ meth-1, data = dat,   #CS , CS# 
+     random = list(item=pdCompSymm(~ meth-1)), 
+     correlation = corCompSymm(form=~1 | item/repl), 
+     method="ML")


ANOVAs  for  Original Fits
The likelihood Ratio test is very simple to implement in R. All that is required it to specify the reference model and the relevant nested mode as arguments to the command anova().
The figure below displays the three tests described by Roy (2009).
> testB    = anova(Ref.Fit,NMB.fit)                          # Between-Subject Variabilities
> testW   = anova(Ref.Fit,NMW.fit)                        # Within-Subject Variabilities
> testO     = anova(Ref.Fit,NMO.fit)                        # Overall Variabilities



\subsection*{One Way Testing for Comparative Precision}

Precision and Repeatability
Bland and Altman (1999) and Barnhart et al (2008) make the connection between precision and repeatability clear. As repeatability and within-item variance are equivalent, we can determine the most precise method of measurement from two candidate methods by comparing their respective within-item variances.
Within-Item Variability
In assessing the comparative precision of two methods, a useful approach is to consider the ratio of the within-item standard deviations of both methods.
To provide a basis for one-way testing, a 95% confidence interval of this ratio can be computed, and conclusions can be based thereon.
The ratio of the within-item variabilities is easily determinable from the reference model. Of more interest is a confidence interval for this ratio. A ratio of greater than 1 indicates that the test method is less precise.
The following piece of code, implemented using the intervals() command,  reveals a ratio of the within-item standard deviations for test method S and reference method J to be 1.49, with a 95% confidence interval of (1.29,1.72).
Necessarily one would conclude that the reference method J has a smaller within-item variance than the test method S, i.e. more precise.

Variance function:
lower     est.    upper
S 1.290990 1.490806 1.721549

(Remark :  The ratio of standard deviations, and not of variances,  is used because that is what is given immediately from the code output.  Also it is the ratio of the coefficients of repeatability).
Between-item Variability
Computing the ratio of the between-item variability is straightforward, but unlike within-item variability, we are not given a confidence interval for that ratio. Instead we are given the between-item standard deviations, and the confidence intervals for those estimates.
Pinheiro and Bates (2000, page 93-95) give a description of how confidence intervals for the variance-covariance estimates are computed.


lower       est.      upper
sd(methJ)        26.101544 30.3969754 35.3992892
sd(methS)        26.700456 31.1655647 36.3773715

# Ratio = 31.1655647 / 30.3969754 =  1.025285


What is to be resolved is the correct approach for computing the necessary confidence intervals for the ratio of standard deviations, based on the given information. 
Would it suffice to use the general structure for variance ratios (i.e. divide the ratio by F quantiles)?
Another issue to be resolved is the part that between-item variance plays, in determining which of two method of measurement are most accurate. 
Repeated & Replicate Measurements

The analysis is simple because we expect the mean difference between replicates to
be zero;  we do not usually expect second measurements of the same samples to differ
systematically from first measurements. Indeed, such a systematic difference would
indicate that the values were not true replicates. 
Likelihood Ratio Tests
For small sample sizes, likelihood ratio tests tend to be too liberal when comparing models with nested fixed effects structures and should be used with caution.

We recommend using the Wald-type tests provided by the anova method with a single argument, as these tend to have significance levels close to nominal, even for small samples (Pinheiro Bates)

Using ML or REML Fitting
Noticeably Roy (2009) uses ML estimation when specifying the LME models. No explanation is given, although plausibly it is due to the constraints of the computational environment being used.
Both West et al (2010) and Pinheiro & Bates (2000) compare ML and REML estimation, describing what types of tests are appropriate for each.  When variance components are being tested, REML estimation is in fact the correct approach. 
If two compared models have the same fixed effects structure, the likelihood ratio test based on REML is meaningful  (Pinheiro Bates)

However, Choudhary & Nagaraja(2007) point out  that for a joint test of Fixed and Random effects that ML estimation is the appropriate estimation method.
Comparison of ML and REML fits
Fit 1 (ML)

Dataset: Blood RS

Fixed : 127.3126 , 143.0275

AIC: 4075.594

Between Subject Variability

%[■(〖30.28301〗^2&0.83×30.28301×31.55595@..&〖31.35595〗^2 )]	Fit1r (REML)

Dataset: Blood RS

Fixed : 127.3126 , 143.0275

AIC: 4068.172

Between Subject Variability

%[■(〖30.10195〗^2&0.83×30.10195×31.16573@..&〖31.16573〗^2 )]


\begin{verbatim}
# Systolic blood pressure measurements made 
# simultaneously by two observers (J and R) 
# and an automatic blood pressure measuring
# machine (S), each making three observations 
# in quick succession (supplied by Dr E O'Brien)

Blood = matrix(data=c(100, 106, 107, 98, 98, 111, 122, 128, 124, 108, 110, 108, 108, 112, 110, 121, 
127, 128, 76, 84, 82, 76, 88, 82, 95, 94, 98,108, 104, 104, 110, 100, 106, 127, 127, 135,124, 112, 112, 128, 112, 114, 140, 131, 124,122, 140, 124, 124, 140, 126, 139, 142, 136,116, 108, 102, 118, 110, 102, 122, 112, 112,114, 110, 112, 112, 108, 112, 130, 129, 135,100, 108, 112, 100, 106, 112, 119, 122, 122,108, 92, 100, 108, 98, 100, 126, 113, 111,100, 106, 104, 102, 108, 106, 107, 113, 111,108, 112, 122, 108, 116, 120, 123, 125, 125,112, 112, 110, 114, 112, 110, 131, 129, 122,104, 108, 104, 104, 108, 104, 123, 126, 114,106, 108, 102, 104, 106, 102, 127, 119, 126,122, 122, 114, 118, 122, 114, 142, 133, 137,100, 102, 102, 102, 102, 100, 104, 116, 115,118, 118, 120, 116, 118, 118, 117, 113, 112,140, 134, 138, 138, 136, 134, 139, 127, 113,150, 148, 144, 148, 146, 144, 143, 155, 133,166, 154, 154, 164, 154, 148, 181, 170, 166,148, 156, 134, 136, 154, 132, 149, 156, 140,
174, 172, 166, 170, 170, 164, 173, 170, 154,174, 166, 150, 174, 166, 154, 160, 155, 170,140, 144, 144, 140, 144, 144, 158, 152, 154,128, 134, 130, 128, 134, 130, 139, 144, 141,
146, 138, 140, 146, 138, 138, 153, 150, 154,146, 152, 148, 146, 152, 148, 138, 144, 131,220, 218, 220, 220, 218, 220, 228, 228, 226,208, 200, 192, 204, 200, 190, 190, 183, 184,
94, 84, 86, 94, 84, 88, 103, 99, 106,114, 124, 116, 112, 126, 118, 131, 131, 124,126, 120, 122, 124, 120, 120, 131, 123, 124,124, 124, 132, 126, 126, 120, 126, 129, 125,110, 120, 128, 110, 122, 126, 121, 114, 125,90, 90, 94, 88, 88, 94, 97, 94, 96,106, 106, 110, 106, 108, 110, 116, 121, 127,218, 202, 208, 218, 200, 206, 215, 201, 207,130, 128, 130, 128, 126, 128, 141, 133, 146,136, 136, 130, 136, 138, 128, 153, 143, 138,100, 96, 88, 100, 96, 86, 113, 107, 102,100, 98, 88, 100, 98, 88, 109, 105, 97,
124, 116, 122, 126, 116, 122, 145, 102, 137,164, 168, 154, 164, 168, 154, 192, 178, 171,100, 102, 100, 100, 104, 102, 112, 116, 116,136, 126, 122, 136, 124, 122, 152, 144, 147,114, 108, 122, 114, 108, 122, 141, 141, 137,148, 120, 132, 146, 130, 132, 206, 188, 166,160, 150, 148, 160, 152, 146, 151, 147, 136,84, 92, 98, 86, 92, 98, 112, 125, 124,156, 162, 152, 156, 158, 152, 162, 165, 189,110, 98, 98, 108, 100, 98, 117, 118, 109,100, 106, 106, 100, 108, 108, 119, 131, 124,100, 102, 94, 100, 102, 96, 136, 116, 113,86, 74, 76, 88, 76, 76, 112, 115, 104,106, 100, 110, 106, 100, 108, 120, 118, 132,108, 110, 106, 106, 118, 106, 117, 118, 115,168, 188, 178, 170, 188, 182, 194, 191, 196,166, 150, 154, 164, 150, 154, 167, 160, 161,146, 142, 132, 144, 142, 130, 173, 161, 154,204, 198, 188, 206, 198, 188, 228, 218, 189,96, 94, 86, 96, 94, 84, 77, 89, 101,134, 126, 124, 132, 126, 124, 154, 156, 141,138, 144, 140, 140, 142, 138, 154, 155, 148,134, 136, 142, 136, 134, 140, 145, 154, 166,156, 160, 154, 156, 162, 156, 200, 180, 179,124, 138, 138, 122, 140, 136, 188, 147, 139,114, 110, 114, 112, 114, 114, 149, 217, 192,112, 116, 122, 112, 114, 124, 136, 132, 133,112, 116, 134, 114, 114, 136, 128, 125, 142,202, 220, 228, 200, 220, 226, 204, 222, 224,132, 136, 134, 134, 136, 132, 184, 187, 192,158, 162, 152, 158, 164, 150, 163, 160, 152,88, 76, 88, 90, 76, 86, 93, 88, 88,170, 174, 176, 172, 174, 178, 178, 181, 181,182, 176, 180, 184, 174, 178, 202, 199, 195,112, 114, 124, 112, 112, 126, 162, 166, 148,120, 118, 120, 118, 116, 120, 227, 227, 219,110, 108, 106, 110, 108, 106, 133, 127, 126,112, 112, 106, 112, 110, 106, 202, 190, 213,154, 134, 130, 156, 136, 132, 158, 121, 134,
116, 112, 94, 118, 114, 96, 124, 149, 137,108, 110, 114, 106, 110, 114, 114, 118, 126,106, 98, 100, 104, 100, 100, 137, 135, 134,122, 112, 112, 122, 114, 114, 121, 123, 128), 
nrow = 85, ncol = 9, byrow = TRUE,
dimnames = list(NULL, c("J1","J2","J3","R1","R2","R3","S1","S2","S3")) )
\end{verbatim}

\begin{verbatim}
#####################################################################
#Preparing the Blood Data
library(nlme)
blood = groupedData( y ~ meth | item ,
data = data.frame( y = c(Blood), item = c(row(Blood)),
meth = rep(c("J","R","S"), rep(nrow(Blood)*3, 3)),
repl = rep(rep(c(1:3), rep(nrow(Blood), 3)), 3) ),
labels = list(BP = "Systolic Blood Pressure", method = "Measurement Device"),
order.groups = FALSE )

#pick out two of the three methods ( use J and S ) 

dat = subset(blood, subset = meth != "R")

\end{verbatim}




\newpage


Section 2 Notation

 
 
  
 
 
 
 
  
 


Section 3 Roy’s four tests

Test 1  Difference between the means of two methods. This test gives the bias and corresponding test statistic and p- value in the “solution for fixed effects” output.
Test 2  Difference of “between subject” variability of the two methods.
Test 3  Within subject variability  of the two methods ( repeated measures)
Test 4  Overall variability of the two methods.

 


 





Section 5 Variance Covariance Decomposition
PB 5.1.3 Decomposing the Within Group VC structure.
Decompose WGVC into a variance structure component and a correlation structure component.
 
(In PB, WGVC is denoted as  )

  describes the variance of the within-group errors 
  describes the correlation of the within-group errors 

5.1 nlme commands
Varfunc function is used to specify WG variance models. The two main arguments are ‘value’ and ‘form’.
Corstruct function is used to specify WG correlation models.
corCompSymm implements the CS correlation structure.

 
Section 6  -  A Roy 2009

This is the SAS code for fitting the linear mixed effects model to the data of Bland and Altman (1999, Table 1) under the null hypothesis that the two methods, the observer J and the blood pressure monitor S, do not have the same within-subject variabilities. 

proc mixed data=BloodPressure1 method=ml covtest;
classes subj m_var rep;
model y=m_var /s ddfm=kr;
random m_var /type=un subject=subj v vcorr g;
repeated m_var/type= un subject=rep(subj) r;


Here we give the SAS code for fitting the linear mixed effects model to the same data under the alternative hypothesis that the two methods have the same within-subject variabilities.*/

\begin{verbatim}
proc mixed data=BloodPressure1 method=ml covtest;
classes subj m_var rep;
model y=m_var /s ddfm=kr;
random m_var /type= un subject=subj v vcorr g;
repeated m_var/type= cs subject=rep(subj) r;
run;
\end{verbatim}
The difference is in the arguments to the “repeated” parameter.
Discussion of the code
The proc mixed statement invokes the procedure, here using the dataset named “BloodPressure1.”
The method = ml option tells SAS to use full maximum likelihood estimation. [If you omit this option, by default SAS uses restricted maximum likelihood - REML.]
The covtest option tells SAS to display tests for the variance components.  By default, SAS omits these tests.
The /s option [formally /solution] on the model statement tells SAS to display the estimated fixed effects (as well as the associated standard errors and hypothesis tests). [Random]
The /r option tells SAS to display the blocks of the estimated  matrix . [Repeated]

MAIN COMMANDS
GROUP:		 varies covariance parameters by groups  	(BXC)
 SUBJECT:	 identifies the subjects in the model 		(ARoy)
 TYPE: 		 specifies the covariance structure 		(Aroy)
 
Section 7 - RANDOM STATEMENT

The RANDOM statement specifies the random effect terms that are to be included in the model, along with a
covariance structure (TYPE= option) to specify how the random effects are related to each other. 

More than one RANDOM statement can be used to define a PROC MIXED model. 

This is useful when you have some random effects that are correlated, while there are others that need to be independent of those, as in hierarchical models. The SUBJECT= option can be used to specify different hierarchies.

The RANDOM statement defines the random effects constituting the   vector in the mixed model. 
It can be used to specify traditional variance component models (as in the VARCOMP procedure) and to specify random coefficients.
Using Mixed Models notation , the purpose of the RANDOM statement is to define the   matrix of the mixed model, the random effects in the  vector, and the structure of  . 
The  matrix is constructed exactly as the  matrix for the fixed effects, and the  matrix is constructed to correspond with the effects constituting  . 
The structure of  is defined by using the “TYPE=” option. 

..
random m_var /type=un subject=subj v vcorr g;
..

“ random m_var” identifies “m_var” as the random effect.
“ TYPE=UN “ specifies the covariance structure of   as unstructured.
“subject=subj” identifies the subjects in the mixed model.

“SUBJECT=effect” 		[or SUB=effect ]
This option identifies the subjects in your mixed model. Complete independence is assumed across subjects; thus, for the RANDOM statement, the SUBJECT= option produces a block-diagonal structure in G with identical blocks. 
The Z matrix is modified to accommodate this block diagonality. 
In fact, specifying a subject effect is equivalent to nesting all other effects in the RANDOM statement within the subject effect. 







\subsection*{Section 8 - REPEATED STATEMENT}

The REPEATED statement controls the covariance structure imposed upon the residuals or errors.

 In repeated measures models the SUBJECT= optional statement parameter is used to define which observations belong to the same subject, and which belong to different subjects, where different subjects are independent. 

The TYPE= optional statement parameter specifies the model for the covariance structure of the errors. 

The GROUP= optional statement parameter permits different levels of the GROUP effect to have different structure parameters, though the structure TYPE remains the same. 

Only one REPEATED statement is permitted in a PROC MIXED model.
The REPEATED statement is used to specify the   matrix in the mixed model. 
If no REPEATED statement is specified,  is assumed to be equal to  .
For many repeated measures models, no repeated effect is required in the REPEATED statement. Simply use the “SUBJECT=” option to define the blocks of   and the “TYPE=” option to define their covariance structure.
Model 1
….
repeated m_var/type= un subject=rep(subj) r;
….

Model 2
….
repeated m_var/type= cs subject=rep(subj) r;
….

“ TYPE=UN “ specifies the covariance structure of R as unstructured.
“ TYPE=CS “ specifies the covariance structure of R as compound symmetry structure.
Section 9 - The lme4 and nlme packages
One of the big differences between the lme4 package and the nlme package, or other software for fitting linear mixed models, is that lme4 is designed to handle models with non-nested random effects.

Section 9.1 : The optimization in lmer is done with respect to the elements of the variance-covariance matrix of the random effects relative to σ2. Given these values the conditional estimates of the fixed-effects parameters and of σ2 can be evaluated directly with some linear algebra. In the summary or show output of an lmer model there are two quantities called the MLdeviance and the REMLdeviance. 
Those are based on the same relative variances but different conditional estimates of σ2 (and hence different estimates of the elements of the variance-covariance of the random effects). It turns out that there is very little difference in the value of the profiled log-likelihood at the ML estimates and at the REML estimates. 
This is not to say that the log-likelihood is similar at the two (complete) sets of estimates - it is the profiled log-likelihoods that are similar and these are what are used to create the likelihood ratio test statistic, even when the models have been fit by REML.

\subsection*{Section 10 - Other Commands used in Roy’s code}

G requests that the estimated G matrix be displayed. PROC MIXED displays blanks for values that are 0. 
If you specify the SUBJECT= option, then the block of the  G matrix corresponding to the first subject is displayed. 
V requests that blocks of the estimated V matrix be displayed. The first block determined by the SUBJECT= effect is the default displayed block. PROC MIXED displays entries that are 0 as blanks in the table. 
VCORR displays the correlation matrix corresponding to the blocks of the estimated  matrix. 
The value-list specification is the same as in the V= option. 
Section 11 Bland Altmans Data
Roy includes a table from Bland and Altman’s 1999 paper which shows a set of systolic blood pressure data from a study in which simultaneous measurements were made by each of two experienced observers (denoted J and R) using a sphygmomanometer and by a semi-automatic blood pressure monitor (denoted S).

 Three sets of readings were made in quick succession.


 
SUBJECT=effect 	[or “SUB=effect” ]
This option identifies the subjects in your mixed model. 
Complete independence is assumed across subjects; therefore, the SUBJECT= option produces a block-diagonal structure in R with identical blocks. 
When the SUBJECT= effect consists entirely of classification variables, the blocks of  R correspond to observations sharing the same level of that effect. 
These blocks are sorted according to this effect as well. 

 



 
Bendix Carstensen et al 2008

R implementation
> lme( y ˜ meth + item,
+ random = list( item = pdIdent( ˜ meth-1 ) ),
+ weights = varIdent( form = ˜1 | meth ),
+ data=fat
+ )
SAS implementation

proc mixed data = rdata ;
class meth item ;
model y = meth item / s;
random meth * item ;
repeated item / group = meth ;
run ;
GROUP=effect	[ “GRP=effect” ]
defines an effect specifying heterogeneity in the covariance structure of R. 
All observations having the same level of the GROUP effect have the same covariance parameters. 
Each new level of the GROUP effect produces a new set of covariance parameters with the same structure as the original group. 
You should exercise caution in properly defining the GROUP effect, because strange covariance patterns can result with its misuse. Also, the GROUP effect can greatly increase the number of estimated covariance parameters, which can adversely affect the optimization process. 
 
Summary  
1)	Specify Maximum likelihood.
2)	Display the estimated fixed effects, with associated inference values.
3)	Call tests for the variance components.

getVarCov(..., type = random.effects) - extracts the covariance matrix of the random effects
lme for SAS PROC MIXED Users - Douglas Bates
The VarCorr function returns a table of variance estimates, the
corresponding standard deviations, and the estimated correlations.
The value of VarCorr is a character matrix, not a numeric matrix. 
In general the random statement in PROC MIXED is
easy to translate into nlme.  The repeated statement isn't.
The basic approach in lme is to define one or more factors that represent the grouping of the observations.  In the example above the data contain the information that the grouping is by the "Subject"
factor

 > formula(Oxboys)
 height ~ age | Subject

A full specification of the model in fm1 is
 > fm2 <- lme(fixed = height ~ age, data = Oxboys, random = ~ age | Subject)
 > VarCorr(fm2)
 Subject = pdLogChol(age) 
             Variance   StdDev   Corr  
 (Intercept) 65.3038159 8.081078 (Intr)
 age          2.8248081 1.680717 0.641 
 Residual     0.4354534 0.659889   

\newpage
\subsection*{Section 6  -  A Roy 2009}

This is the SAS code for fitting the linear mixed effects model to the data of Bland and Altman (1999, Table 1) under the null hypothesis that the two methods, the observer J and the blood pressure monitor S, do not have the same within-subject variabilities. 

proc mixed data=BloodPressure1 method=ml covtest;
classes subj m_var rep;
model y=m_var /s ddfm=kr;
random m_var /type=un subject=subj v vcorr g;
repeated m_var/type= un subject=rep(subj) r;


Here we give the SAS code for fitting the linear mixed effects model to the same data under the alternative hypothesis that the two methods have the same within-subject variabilities.*/

proc mixed data=BloodPressure1 method=ml covtest;
classes subj m_var rep;
model y=m_var /s ddfm=kr;
random m_var /type= un subject=subj v vcorr g;
repeated m_var/type= cs subject=rep(subj) r;
run;
The difference is in the arguments to the “repeated” parameter.
Discussion of the code
The proc mixed statement invokes the procedure, here using the dataset named “BloodPressure1.”
The method = ml option tells SAS to use full maximum likelihood estimation. [If you omit this option, by default SAS uses restricted maximum likelihood - REML.]
The covtest option tells SAS to display tests for the variance components.  By default, SAS omits these tests.
The /s option [formally /solution] on the model statement tells SAS to display the estimated fixed effects (as well as the associated standard errors and hypothesis tests). [Random]
The /r option tells SAS to display the blocks of the estimated  matrix . [Repeated]

MAIN COMMANDS
GROUP:		 varies covariance parameters by groups  	(BXC)
SUBJECT:	 identifies the subjects in the model 		(ARoy)
TYPE: 		 specifies the covariance structure 		(Aroy)

Section 7 - RANDOM STATEMENT

The RANDOM statement specifies the random effect terms that are to be included in the model, along with a
covariance structure (TYPE= option) to specify how the random effects are related to each other. 

More than one RANDOM statement can be used to define a PROC MIXED model. 

This is useful when you have some random effects that are correlated, while there are others that need to be independent of those, as in hierarchical models. The SUBJECT= option can be used to specify different hierarchies.

The RANDOM statement defines the random effects constituting the   vector in the mixed model. 
It can be used to specify traditional variance component models (as in the VARCOMP procedure) and to specify random coefficients.
Using Mixed Models notation , the purpose of the RANDOM statement is to define the   matrix of the mixed model, the random effects in the  vector, and the structure of  . 
The  matrix is constructed exactly as the  matrix for the fixed effects, and the  matrix is constructed to correspond with the effects constituting  . 
The structure of  is defined by using the “TYPE=” option. 

..
random m_var /type=un subject=subj v vcorr g;
..

“ random m_var” identifies “m_var” as the random effect.
“ TYPE=UN “ specifies the covariance structure of   as unstructured.
“subject=subj” identifies the subjects in the mixed model.

“SUBJECT=effect” 		[or SUB=effect ]
This option identifies the subjects in your mixed model. Complete independence is assumed across subjects; thus, for the RANDOM statement, the SUBJECT= option produces a block-diagonal structure in G with identical blocks. 
The Z matrix is modified to accommodate this block diagonality. 
In fact, specifying a subject effect is equivalent to nesting all other effects in the RANDOM statement within the subject effect. 







Section 8 - REPEATED STATEMENT

The REPEATED statement controls the covariance structure imposed upon the residuals or errors.

In repeated measures models the SUBJECT= optional statement parameter is used to define which observations belong to the same subject, and which belong to different subjects, where different subjects are independent. 

The TYPE= optional statement parameter specifies the model for the covariance structure of the errors. 

The GROUP= optional statement parameter permits different levels of the GROUP effect to have different structure parameters, though the structure TYPE remains the same. 

Only one REPEATED statement is permitted in a PROC MIXED model.
The REPEATED statement is used to specify the   matrix in the mixed model. 
If no REPEATED statement is specified,  is assumed to be equal to  .
For many repeated measures models, no repeated effect is required in the REPEATED statement. Simply use the “SUBJECT=” option to define the blocks of   and the “TYPE=” option to define their covariance structure.
Model 1
….
repeated m_var/type= un subject=rep(subj) r;
….

Model 2
….
repeated m_var/type= cs subject=rep(subj) r;
….

“ TYPE=UN “ specifies the covariance structure of R as unstructured.
“ TYPE=CS “ specifies the covariance structure of R as compound symmetry structure.
Section 9 - The lme4 and nlme packages
One of the big differences between the lme4 package and the nlme package, or other software for fitting linear mixed models, is that lme4 is designed to handle models with non-nested random effects.

Section 9.1 : The optimization in lmer is done with respect to the elements of the variance-covariance matrix of the random effects relative to σ2. Given these values the conditional estimates of the fixed-effects parameters and of σ2 can be evaluated directly with some linear algebra. In the summary or show output of an lmer model there are two quantities called the MLdeviance and the REMLdeviance. 
Those are based on the same relative variances but different conditional estimates of σ2 (and hence different estimates of the elements of the variance-covariance of the random effects). It turns out that there is very little difference in the value of the profiled log-likelihood at the ML estimates and at the REML estimates. 
This is not to say that the log-likelihood is similar at the two (complete) sets of estimates - it is the profiled log-likelihoods that are similar and these are what are used to create the likelihood ratio test statistic, even when the models have been fit by REML.

\subsection*{Section 10 - Other Commands used in Roy’s code}

G requests that the estimated G matrix be displayed. PROC MIXED displays blanks for values that are 0. 
If you specify the SUBJECT= option, then the block of the  G matrix corresponding to the first subject is displayed. 
V requests that blocks of the estimated V matrix be displayed. The first block determined by the SUBJECT= effect is the default displayed block. PROC MIXED displays entries that are 0 as blanks in the table. 
VCORR displays the correlation matrix corresponding to the blocks of the estimated  matrix. 
The value-list specification is the same as in the V= option. 
Section 11 Bland Altmans Data
Roy includes a table from Bland and Altman’s 1999 paper which shows a set of systolic blood pressure data from a study in which simultaneous measurements were made by each of two experienced observers (denoted J and R) using a sphygmomanometer and by a semi-automatic blood pressure monitor (denoted S).

Three sets of readings were made in quick succession.



SUBJECT=effect 	[or “SUB=effect” ]
This option identifies the subjects in your mixed model. 
Complete independence is assumed across subjects; therefore, the SUBJECT= option produces a block-diagonal structure in R with identical blocks. 
When the SUBJECT= effect consists entirely of classification variables, the blocks of  R correspond to observations sharing the same level of that effect. 
These blocks are sorted according to this effect as well. 






Bendix Carstensen et al 2008

R implementation
> lme( y ˜ meth + item,
+ random = list( item = pdIdent( ˜ meth-1 ) ),
+ weights = varIdent( form = ˜1 | meth ),
+ data=fat
+ )
\subsection*{SAS implementation}

proc mixed data = rdata ;
class meth item ;
model y = meth item / s;
random meth * item ;
repeated item / group = meth ;
run ;
GROUP=effect	[ “GRP=effect” ]
defines an effect specifying heterogeneity in the covariance structure of R. 
All observations having the same level of the GROUP effect have the same covariance parameters. 
Each new level of the GROUP effect produces a new set of covariance parameters with the same structure as the original group. 
You should exercise caution in properly defining the GROUP effect, because strange covariance patterns can result with its misuse. Also, the GROUP effect can greatly increase the number of estimated covariance parameters, which can adversely affect the optimization process. 

Summary  
1)	Specify Maximum likelihood.
2)	Display the estimated fixed effects, with associated inference values.
3)	Call tests for the variance components.

getVarCov(..., type = random.effects) - extracts the covariance matrix of the random effects
lme for SAS PROC MIXED Users - Douglas Bates
The VarCorr function returns a table of variance estimates, the
corresponding standard deviations, and the estimated correlations.
The value of VarCorr is a character matrix, not a numeric matrix. 
In general the random statement in PROC MIXED is
easy to translate into nlme.  The repeated statement isn't.
The basic approach in lme is to define one or more factors that represent the grouping of the observations.  In the example above the data contain the information that the grouping is by the "Subject"
factor
\begin{verbatim}
> formula(Oxboys)
height ~ age | Subject

A full specification of the model in fm1 is
> fm2 <- lme(fixed = height ~ age, data = Oxboys, random = ~ age | Subject)
> VarCorr(fm2)
Subject = pdLogChol(age) 
Variance   StdDev   Corr  
(Intercept) 65.3038159 8.081078 (Intr)
age          2.8248081 1.680717 0.641 
Residual     0.4354534 0.659889   

\end{verbatim}


\subsection*{analytical chemistry}
Comparing analytical methods 
An ion-selective electrode (ISE) determination of sulphide from sulphate-reducing bacteria was compared with a gravimetric determination. 

The result, obtained were expressed in milligrams of sulphide.

Sulphide (ISE method): 108,12,152,3,106,11,128,12,160,128
Sulphide (gravimetry): 105,16,113,0,108,11,141,11,182,118

Comment on the suitability of the ISE method for this sulphide determination (Al-
Hitti, I. K., Moody, G. ]. and Thomas, J. D. R. 1983. Analyst 108: 43).
