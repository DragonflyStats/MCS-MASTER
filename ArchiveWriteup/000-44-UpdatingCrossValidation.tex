\documentclass[MAIN.tex]{subfiles}
\begin{document}




	\chapter{Updating Techniques and Cross Validation}
\section{Cross Validation} %5.4

Cross validation techniques for linear regression employ the use `leave one out' re-calculations. In such procedures the regression coefficients are estimated for $n-1$ covariates, with the $Q^{th}$ observation omitted.

Let $\hat{\beta}$ denote the least square estimate of $\beta$ based upon the full set of observations, and let
$\hat{\beta}^{-Q}$ denoted the estimate with the $Q^{th}$ case
excluded.


In leave-one-out cross validation, each observation is omitted in turn, and a regression model is fitted on the rest of the data. Cross validation is used to estimate the generalization error of a given model. alternatively it can be used for model selection by determining the candidate model that has the smallest generalization error.


Evidently leave-one-out cross validation has similarities with `jackknifing', a well known statistical technique. However cross validation is used to estimate generalization error, whereas the jackknife technique is used to estimate bias.

	\subsection{The Hat Matrix} %5.1
	
	The projection matrix $H$ (also known as the hat matrix), is a
	well known identity that maps the fitted values $\hat{Y}$ to the
	observed values $Y$, i.e. $\hat{Y} = HY$.
	
	\begin{equation}
	H =\quad X(X^{T}X)^{-1}X^{T}
	\end{equation}

	The hat matrix, also known as the projection matrix, is well known in classical linear models. The diagonal elements $h_{ii}$ are known as `leverages'. The properties of $\boldsymbol{H}$  ,such as symmetry and idempotency, are well known.
	
	
	\begin{equation*}
	\boldsymbol{H} =  \boldsymbol{X(X^{\prime}X)^{-1}X^{\prime}}
	\end{equation*}
	
	
	\begin{equation*}
	\boldsymbol{H} = \left[%
	\begin{array}{cc}
	h_{ii} & \boldsymbol{h}^{\prime}_{i}\\
	\boldsymbol{h}_{i} & \boldsymbol{H}_{(i)}\\
	\end{array}%
	\right]
	\end{equation*}
	
	$\boldsymbol{H}_{(i)}$ is an $(n-1) \times (n-1)$ matrix. It's inversion for each $i$ is computationally expensive.
	
	\begin{equation*}
	\boldsymbol{C} = \boldsymbol{H}^{-1} =\left[%
	\begin{array}{cc}
	c_{ii} & \boldsymbol{h}^{\prime}_{c}\\
	\boldsymbol{c}_{i} & \boldsymbol{C}_{(i)}\\
	\end{array}%
	\right]
	\end{equation*}
	

	$H$ describes the influence each observed value has on each fitted
	value. The diagonal elements of the $H$ are the `leverages', which
	describe the influence each observed value has on the fitted value
	for that same observation. The residuals ($R$) are related to the
	observed values by the following formula:
	\begin{equation}
	R = (I-H)Y
	\end{equation}
	
	The variances of $Y$ and $R$ can be expressed as:
	\begin{eqnarray}
	\mbox{var}(Y) = H\sigma^{2} \nonumber\\
	\mbox{var}(R) = (I-H)\sigma^{2}
	\end{eqnarray}
	
	Updating techniques allow an economic approach to recalculating
	the projection matrix, $H$, by removing the necessity to refit the
	model each time it is updated. However this approach is known for
	numerical instability in the case of down-dating.
	
	\section{Efficient updating theorem}
	
	It is convenient to write partitioned matrices in which the $i$-th case is isolated. The partitioned matrix is written as $ i = 1$, but the results apply in general.
	
	%Tewomir pg 158
	If $\boldsymbol{C^{\prime}}_{i}  = [c_{ii}, \boldsymbol{c^{\prime}}_{i}]$, such that  $\boldsymbol{C}_{i}$ is the
	$i$-th column of $\boldsymbol{H}^{-1}$ then
	
	
	\begin{itemize}
		\item $m_{i} = \frac{1}{c_{ii}}$\\
		\item $\breve{x}_{i} = \frac{1}{c_{ii}}\boldsymbol{X^{\prime}C}_{i}$\\
		\item $\breve{\boldsymbol{z}_{ji}} = \frac{1}{c_{ii}}\boldsymbol{Z^{\prime}}_{j}\boldsymbol{C}_{i}$\\
		\item $\breve{y}_{i} = \frac{1}{c_{ii}}\boldsymbol{y^{\prime}C}_{i}$\\
	\end{itemize}
	
	Once $\boldsymbol{H}^{-1}$ is determined, an efficient updating formula can be applied.
	
	
	
	\begin{equation}
	\boldsymbol{H}^{-1} = \boldsymbol{I} - \boldsymbol{Z}(\boldsymbol{D}^{-1} + \boldsymbol{ZZ})^{-1}\boldsymbol{Z^{\prime}}
	\end{equation}
	
	
	
	
	
\section{Cross Validation: Updating standard deviation} %5.4.1

The variance of a data set can be calculated using the following formula.
\begin{equation}
S^{2}=\frac{\sum_{i=1}^{n}(x_{i}^{2})-\frac{(\sum_{i=1}^{n}x_{i})^{2}}{n}}{n-1}
\end{equation}

While using bivariate data, the notation $Sxx$ and $Syy$ shall apply to the variance of $x$ and of $y$ respectively. The covariance term $Sxy$ is given by

\begin{equation}
Sxy=\frac{\sum_{i=1}^{n}(x_{i}y_{i})-\frac{(\sum_{i=1}^{n}x_{i})(\sum_{i=1}^{n}y_{i})}{n}}{n-1}
\end{equation}

Let the observation $j$ be omitted from the data set. The estimates for the variance identities can be updating using minor adjustments to the full sample estimates. Where $(j)$ denotes that the $j$th has been omitted, these identities are

\begin{equation}
Sxx^{(j)}=\frac{\sum_{i=1}^{n}(x_{i}^{2})-(x_{j})^{2}-\frac{((\sum_{i=1}^{n}x_{i})-x_{j})^{2}}{n-1}}{n-2}
\end{equation}
\begin{equation}
Syy^{(j)}=\frac{\sum_{i=1}^{n}(y_{i}^{2})-(y_{j})^{2}-\frac{((\sum_{i=1}^{n}y_{i})-y_{j})^{2}}{n-1}}{n-2}
\end{equation}
\begin{equation}
Sxy^{(j)}=\frac{\sum_{i=1}^{n}(x_{i}y_{i})-(y_{j}x_{j})-\frac{((\sum_{i=1}^{n}x_{i})-x_{j})(\sum_{i=1}^{n}y_{i})-y_{k})}{n-1}}{n-2}
\end{equation}

The updated estimate for the slope is therefore
\begin{equation}
\hat{\beta}_{1}^{(j)}=\frac{Sxy^{(j)}}{Sxx^{(j)}}
\end{equation}

It is necessary to determine the mean for $x$ and $y$ of the
remaining $n-1$ terms
\begin{equation}
\bar{x}^{(j)}=\frac{(\sum_{i=1}^{n}x_{i})-(x_{j})}{n-1},
\end{equation}

\begin{equation}
\bar{y}^{(j)}=\frac{(\sum_{i=1}^{n}y_{i})-(y_{j})}{n-1}.
\end{equation}

The updated intercept estimate is therefore

\begin{equation}
\hat{\beta}_{0}^{(j)}=\bar{y}^{(j)}-\hat{\beta}_{1}^{(j)}\bar{x}^{(j)}.
\end{equation}

%------------------------------------------------------------------------%
\newpage


\section{Updating Estimates} %5.5





	\subsection{Updating of Regression Estimates}
	Updating techniques are used in regression analysis to add or delete rows from a model, allowing the analyst the effect of the observation associated with that row. In time series problems, there will be scientific interest in the changing relationship between variables. In cases where there a single row is to be added or deleted, the procedure used is equivalent to a geometric rotation of a plane.
	
	Updating techniques are used in regression analysis to add or delete rows from a model, allowing the analyst the effect of the observation associated with that row.
	
	Consider a $p \times p$ matrix $X$, from which a row $x_{i}^{T}$ is to be added or deleted. \citet{CookWeisberg} sets $A = X^{T}X$,
	$a=-x_{i}^{T}$ and $b=x_{i}^{T}$, and writes the above equation as
	
	\begin{equation}
	(X^{T}X \pm x_{i}x_{i}^{T})^{-1} = \quad(X^{T}X )^{-1} \mp \quad
	\frac{(X^{T}X)^{-1}(x_{i}x_{i}^{T}(X^{T}X)^{-1}}{1-x_{i}^{T}(X^{T}X)^{-1}x_{i}}
	\end{equation}
	
	This approach allows an economic approach to recalculating the
	projection matrix, $V$, by removing the necessity to refit the
	model each time it is updated.
	
	This approach is known for numerical instability in the case of
	downdating.



\subsection{Updating Regression Estimates}
Let the observation $j$ be omitted from the data set. The estimates for the variance identities can be updating using minor adjustments to the full sample estimates. Where $(j)$ denotes that the $j$th has been omitted, these identities are


\begin{equation}
Sxx^{(j)}=\frac{\sum_{i=1}^{n}(x_{i}^{2})-(x_{j})^{2}-\frac{((\sum_{i=1}^{n}x_{i})-x_{j})^{2}}{n-1}}{n-2}
\end{equation}
\begin{equation}
Syy^{(j)}=\frac{\sum_{i=1}^{n}(y_{i}^{2})-(y_{j})^{2}-\frac{((\sum_{i=1}^{n}y_{i})-y_{j})^{2}}{n-1}}{n-2}
\end{equation}
\begin{equation}
Sxy^{(j)}=\frac{\sum_{i=1}^{n}(x_{i}y_{i})-(y_{j}x_{j})-\frac{((\sum_{i=1}^{n}x_{i})-x_{j})(\sum_{i=1}^{n}y_{i})-y_{k})}{n-1}}{n-2}
\end{equation}


The updated estimate for the slope is therefore
\begin{equation}
\hat{\beta}_{1}^{(j)}=\frac{Sxy^{(j)}}{Sxx^{(j)}}
\end{equation}


It is necessary to determine the mean for $x$ and $y$ of the
remaining $n-1$ terms
\begin{equation}
\bar{x}^{(j)}=\frac{(\sum_{i=1}^{n}x_{i})-(x_{j})}{n-1},
\end{equation}


\begin{equation}
\bar{y}^{(j)}=\frac{(\sum_{i=1}^{n}y_{i})-(y_{j})}{n-1}.
\end{equation}


The updated intercept estimate is therefore


\begin{equation}
\hat{\beta}_{0}^{(j)}=\bar{y}^{(j)}-\hat{\beta}_{1}^{(j)}\bar{x}^{(j)}.
\end{equation}






	



\section{Sherman Morrison Woodbury Formula} % 5.2

The `Sherman Morrison Woodbury' Formula is a well known result in
linear algebra;
\begin{equation}
(A+a^{T}B)^{-1} \quad = \quad A^{-1}-
A^{-1}a^{T}(I-bA^{-1}a^{T})^{-1}bA^{-1}
\end{equation}

This result is highly useful for analyzing regression diagnostics,
and for matrices inverses in general. Consider a $p \times p$
matrix $X$, from which a row $x_{i}^{T}$ is to be added or
deleted. \citet{CookWeisberg} sets $A = X^{T}X$, $a=-x_{i}^{T}$
and $b=x_{i}^{T}$, and writes the above equation as

\begin{equation}
(X^{T}X \pm x_{i}x_{i}^{T})^{-1} = \quad(X^{T}X )^{-1} \mp \quad
\frac{(X^{T}X)^{-1}(x_{i}x_{i}^{T}(X^{T}X)^{-1}}{1-x_{i}^{T}(X^{T}X)^{-1}x_{i}}
\end{equation}

The projection matrix $H$ (also known as the hat matrix), is a
well known identity that maps the fitted values $\hat{Y}$ to the
observed values $Y$, i.e. $\hat{Y} = HY$.

\begin{equation}
H =\quad X(X^{T}X)^{-1}X^{T}
\end{equation}

$H$ describes the influence each observed value has on each fitted value. The diagonal elements of the $H$ are the `leverages', which describe the influence each observed value has on the fitted value for that same observation. The residuals ($R$) are related to the observed values by the following formula:
\begin{equation}
R = (I-H)Y
\end{equation}

The variances of $Y$ and $R$ can be expressed as:
\begin{eqnarray}
\mbox{var}(Y) = H\sigma^{2} \nonumber\\
\mbox{var}(R) = (I-H)\sigma^{2}
\end{eqnarray}

Updating techniques allow an economic approach to recalculating the projection matrix, $H$, by removing the necessity to refit the model each time it is updated. However this approach is known for
numerical instability in the case of down-dating.



\subsection{Hat Values for MCS regression}

With A as the averages and D as the casewise differences.
\begin{verbatim}
fit = lm(D~A)
\end{verbatim}

\begin{displaymath}
H = A \left(A^\top  A\right)^{-1} A^\top ,
\end{displaymath}

	




\section{Updating Estimates} %5.5



\subsection{Updating Standard deviation}
A simple, but useful, example of updating is the updating of the standard deviation when an observation is omitted, as practised in statistical process control analyzes. From first principles, the variance of a data set can be calculated using the following formula.
\begin{equation}
S^{2}=\frac{\sum_{i=1}^{n}(x_{i}^{2})-\frac{(\sum_{i=1}^{n}x_{i})^{2}}{n}}{n-1}
\end{equation}


While using bivariate data, the notation $Sxx$ and $Syy$ shall apply hither to the variance of $x$ and of $y$ respectively. The covariance term $Sxy$ is given by


\begin{equation}
Sxy=\frac{\sum_{i=1}^{n}(x_{i}y_{i})-\frac{(\sum_{i=1}^{n}x_{i})(\sum_{i=1}^{n}y_{i})}{n}}{n-1}.
\end{equation}









\subsection{Inference on intercept and slope}
\begin{equation}
\hat{\beta_{1}} \pm t_{(\alpha, n-2) }
\sqrt{\frac{S^2}{(n-1)S^{2}_{x}}}
\end{equation}

\begin{equation}
\frac{\hat{\beta_{0}}-\beta_{0}}{SE(\hat{\beta_{0}})}
\end{equation}
\begin{equation}
\frac{\hat{\beta_{1}}-\beta_{1}}{SE(\hat{\beta_{0}})}
\end{equation}



\subsection{Inference on correlation coefficient} This test of
the slope is coincidentally the equivalent of a test of the
correlation of the $n$ observations of $X$ and $Y$.
\begin{eqnarray}
H_{0}: \rho_{XY} = 0 \nonumber \\
H_{A}: \rho_{XY} \ne 0 \nonumber \\
\end{eqnarray}





\bibliographystyle{chicago}
\bibliography{DB-txfrbib}


\end{document}
