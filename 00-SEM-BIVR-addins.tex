Of the three criteria listed by \citet{barnhart}, the one that requires closest attention is the equality of within-item variances.


Between-item variance $d^2_i$ is fundamentally measures the variability of the item means as measured by method $i$, but does not contain information on the precision of that method. 

For conventional method comparison problems, both methods measures the same set of items using the same unit of measurement. Convergence to equality of between-item variance inevitable as the number of items $n$ increases. Significantly different estimates for $d^2_1$ and $d^2_2$ should not be expected for any practical problem.  

Therefore a violation of third criterium (i.e. different between-item variances) criterium is contingent upon, and a  
possible consequence of, the violation of the other two agreement criteria. However, a violation of the third criterium will not occur in isolation. 

As noted elsewhere, the matter of inter-method bias can be easily accounted for, once detected. Both between-items and within-items variances must be calculated such that sources of variances are properly assigned, and to compute limits of agreement. However, testing the within-item criterium is the most informative and therefore requires the most attention. 



%==================================================%


Structural Equation Model provides a statistically rigourous analysis, but the approach is undermined in several ways.
LME models have greater flexibility and can be adapted to any variant of the method comparison research question, whereas 
SEM is suitable for some specific cases only. Highly complex models can be developed using SEM, but to overcome the problem of identifiability, a large quantity of data must be gathered.
Often this is beyond what is practical in the main applications of method comparison studies, namely the medical sciences. Once simplifications are applied, there is little functional difference between SEM and LMEs.

