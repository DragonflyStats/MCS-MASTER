
\documentclass[12pt, a4paper]{report}

\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage{framed}
\usepackage{subfiles}
\usepackage{amsthm, amsmath}
\usepackage{amsbsy}
\usepackage{framed}
\usepackage[usenames]{color}
\usepackage{listings}
\lstset{% general command to set parameter(s)
basicstyle=\small, % print whole listing small
keywordstyle=\color{red}\itshape,
% underlined bold black keywords
commentstyle=\color{blue}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false,
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
frame=shadowbox,
rulesepcolor=\color{black},
,columns=fullflexible
} %
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{1.0cm}{0.75cm}{18.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
%\voffset=-2.5cm
%\oddsidemargin=1cm
%\textwidth = 520pt

\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
\author{Kevin O'Brien}
\title{LME Models for Method Comparison Studies}
%\tableofcontents

\chapter{A Simplified LME Framework for Method Comparison}

\section{A Review of Criteria for Agreement in Roy's Framework}

\citet{ARoy2009} proposes a suite of hypothesis tests for assessing the agreement of two methods of measurement, when replicate measurements are obtained for each item, using an LME framework. For this framework, \citet{ARoy2009} recalls three criteria from \citet{Barnhart} for two methods to be considered in agreement (which are presented here in a different order);

\begin{enumerate}
\item There be no significant inter-method bias between the two methods, i.e. there is no persistent tendency for one method to give higher values than the other.
\item There is no difference in the within-subject variabilities, i.e. no significant difference in the random error among the replications taken by the same method on the same subject. 
\item There is no significant difference in the between-subject variabilities, the variances of the mean measurements for each case from both methods are not significantly different.
\end{enumerate} 
\citet{ARoy2009} demonstrates a LME model specification, and a series of tests that look at each of these agreement criteria individually. If two methods of measuement lack agreement, the specific reason or reasons for this lack of agreement can be identified. \citet{ARoy2009} further proposes examination of the the overall variability by considering the second and third criteria be examined jointly. Should both the second and third criteria be fulfilled, then the overall variabilities of both methods would be equal.

\citet{ARoy2009} provides an adjusted set of criteria, considering two methods to be in agreement if three conditions are met. Firstly that there is no significant bias, i.e. the difference between the two
mean readings is not ``statistically significant". Secondly there is
agreement between the two methods by testing their
repeatability coefficients. Finally, that there is a high overall correlation coefficient between methods for agreement to be present.


%\section{Critiquing the Relevance of Third Criterion}
Criteria 1 and 2, as listed above, are extensions of key statistical properties of accuracy and precision, which are discussed by \citet{Barnhart} in the context of method comparison, and can be considered as essential when comparing methods of measurement, with no further discussion required.

%\citet{Barnhart} describes the sources of disagreement as differing population means, different between-subject variances, different within-subject variances between two methods and poor correlation between measurements of two methods.

The third criteria concerns the equality of between-method variances, and is the basis of one of Roy's three tests. \citet{ARoy2009} remarks that between-subject variabilities are needed when the interest lies in the true or real difference between the two methods giving different measurements on the same subject, and are crucial when one is concerned about the true difference between the two methods \citep{ARoy2009}. Constructively the null hypothesis of this test calls for the equality of variances of the mean measurements 
for both sets of measurements. From a theoretical point of view, this criterion is a valid requirement for method agreement. 

However, one should consider how a violation of this criterion could occur. When items are measured on the same scale by both methods, the resulting measurements should be numerically near to each other on a consistent basis, even in the absence of agreement, even with a very small data set. 

\citet{ARoy2009} points out that sometimes there can be significant difference in between-subject variabilities of the two methods. The presence of proportional bias will give rise to a significant difference in between-subject variance. While this is correct, the proportional bias must be very drastic for it to cause a difference in between subject variabilities. If proportional bias is not present, there must be a catastrophic failure of one or both of the measurement methods, i.e. they are systematically unable to yield a pair of measurements for an item that are numerically close together.

If this was the case, this phenomenon would be immediately identifiable in the exploratory phase of the analysis, i.e. from a visual inspection of the scatter-plot, and from fundamental summary statistics. A complicated model framework, such as an LME model, would not be required to determine this outcome. Furthermore, such is the complexity of LME models, that a solution may be intractable.

For comparing two methods of measurement, using different scales of measurement, (e.g. Celsium and Fahrenheit), a difference in between-subject variability is to be expected. However this scenario is outside of the scope of the typical method comparison study. 

As such, the test of this third criterion should not be conducted simultaneously with tests for the other two, as proposed by \citet{ARoy2009}. Instead violation of Criteria 3 should be determined early in the model building phase. No useful analysis can be carrried out if a violation exists. Violations of criteria 1 and 2 may exist in the context of two functioning measurement systems, as demonstrated in the examples provided by \citet{ARoy2009}.


\section{Towards A Simplified Model Framework}
\citet{BA99} recommended simultaneous estimation of repeatability and agreement
for replicate measurements. When no longer required to simultaneously test for a violation of the third criteria, we are afforded the opportunity to test the first two criteria with a LME model framework that is simpler than Roy's model. A violation of the first criterion can be detected by inspection of the estimate for the fixed effects. A violation of the second criterion can be detected by an inspection of the residuals.

An advantage of using a simplified LME framework is that it affords the opportunity to test Criteria 1 and 2 for more than two methods of measurement simultaneously. Both \citet{BA99} and \citet{ARoy2009} use as an example the simultaneous measurements of systolic blood pressure
were made by each of the two experienced observers (denoted by J and R) using a sphygmomanometer, and by a semi-automatic blood pressure monitor (denotedby S). Three sets (p = 3) of readings were made in quick succession on 85 subjects. 

The following approach shows that it can be quickly determined which pair of methods are in agreement.

\begin{description}
\item[Step 1] A LME model is fitted to the entire data set. A fixed effect is used to represent the measurement methods. The random error is collected in a random effect. For the sake of simplicity, one of the measurement methods can be chosen as the base case.

\item[Step 2] A test of criterion 1 can be implemented by an examination of the fixed effects. While the creators of the LME4 R package advised against the use of p-values, a set of confidence intervals can be determined.

\item[Step 3] A test of criterion 2 can be implemented by an examination of the residuals. An omnibus test for all $p$ methods together can be implemented as a Bartlett Test of homogeneity of variances. Pairwise comparison may be implemented using the standard $F-$test for equality of variance. Both tests are facilitated by most statistical software packages.
\end{description}

Further insights can be made using influence diagnostic procedures, such as Cook's Distance and DFBETAs, and will be reverted to.


The LME model proposed by \citet{ARoy2009} should be retained in the overall method comparison framework, as it is still strongly preferred for computing limits of agreement and the repeatability coefficient.
\citet{ARoy2009} states the importance of reporting repeatability when assessing measurement, because it measures the purest
random error that is not influenced by any other factors \citep{Barnhart}.
\citet{ARoy2009} advises against the use any kind of correlation coefficient for assessing agreement for data with replications, even though that framework allows for the computation of the overall correlation coefficient.


%Strong positive correlation between measurements set, to some degree, should be expected between both sets of measurements.

\newpage

\begin{verbatim}
## Step 1. Load R packages
library(dplyr);  library(magrittr);  library(broom)
library(lme4)
library(GGally)

## Step 2. Get the Blood Pressure Data
source("https://raw.githubusercontent.com/DragonflyStats/MCS-code/master/blooddata.R")

## Step 3. Fit the Simplified LME Model
MCSModel <- lmer(BP ~ method + (1|subject),data=blood)

## Step 4. Test the inter-method bias criterion, i.e. criterion 1.
summary(MCSModel)
ggcoef(MCSModel)  # visual analogue

## Step 5. Augment the Blood data set with additional estimates
blood <- augment(MCSModel,blood)

## Step 6. Compare Estimate for Residal Variances
blood %>% group_by(method) %>% summarize(Sigma.sq.W = var(.resid)) 

## Step 7. Omnibus Test for Equality of Variances
bartlett.test(.resid~method,data=blood)

## Step 8. Pairwise Test for Equality of Variances
blood.js <- blood %>% filter(method %in% c("J","S"))
var.test(.resid~method,data=blood.js)

\end{verbatim}

\bibliographystyle{chicago}
\bibliography{2017bib}
\end{document}
