
\documentclass[12pt, a4paper]{report}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage{framed}
\usepackage{subfiles}
\usepackage{amsthm, amsmath}
\usepackage{amsbsy}
\usepackage{framed}
\usepackage[usenames]{color}
\usepackage{listings}
\lstset{% general command to set parameter(s)
basicstyle=\small, % print whole listing small
keywordstyle=\color{red}\itshape,
% underlined bold black keywords
commentstyle=\color{blue}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false,
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
frame=shadowbox,
rulesepcolor=\color{black},
,columns=fullflexible
} %
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
\author{Kevin O'Brien}
\title{Mixed Models for Method Comparison Studies}
\tableofcontents

%----------------------------------------------------------------------------------------%
\newpage
\chapter{Method Comparison Studies}

\section{Introduction}
The problem of assessing the agreement between two or more methods
of measurement is ubiquitous in scientific research, and is
commonly referred to as a `method comparison study'. Published
examples of method comparison studies can be found in disciplines
as diverse as pharmacology \citep{ludbrook97}, anaesthesia
\citep{Myles}, and cardiac imaging methods \citep{Krumm}.
\smallskip

To illustrate the characteristics of a typical method comparison
study consider the data in Table I \citep{Grubbs73}. In each of
twelve experimental trials, a single round of ammunition was fired
from a 155mm gun and its velocity was measured simultaneously (and
independently) by three chronographs devices, identified here by
the labels `Fotobalk', `Counter' and `Terma'.
\smallskip


\newpage

\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrr}
  \hline
  Round& Fotobalk [F] & Counter [C]& Terma [T]\\
  \hline
  1 & 793.8 & 794.6 & 793.2 \\
  2 & 793.1 & 793.9 & 793.3 \\
  3 & 792.4 & 793.2 & 792.6 \\
  4 & 794.0 & 794.0 & 793.8 \\
  5 & 791.4 & 792.2 & 791.6 \\
  6 & 792.4 & 793.1 & 791.6 \\
  7 & 791.7 & 792.4 & 791.6 \\
  8 & 792.3 & 792.8 & 792.4 \\
  9 & 789.6 & 790.2 & 788.5 \\
  10 & 794.4 & 795.0 & 794.7 \\
  11 & 790.9 & 791.6 & 791.3 \\
  12 & 793.5 & 793.8 & 793.5 \\
   \hline
\end{tabular}
\caption{Velocity measurement from the three chronographs (Grubbs
1973).}
\end{center}
\end{table}

An important aspect of the these data is that all three methods of
measurement are assumed to have an attended measurement error, and
the velocities reported in Table 1.1 can not be assumed to be
`true values' in any absolute sense.

%While lack of
%agreement between two methods is inevitable, the question , as
%posed by \citet{BA83}, is 'do the two methods of measurement agree
%sufficiently closely?'

A method of measurement should ideally be both accurate and
precise. \citet{Barnhart} describes agreement as being a broader
term that contains both of those qualities. An accurate
measurement method will give results close to the unknown `true
value'. The precision of a method is indicated by how tightly
measurements obtained under identical conditions are distributed
around their mean measurement value. A precise and accurate method
will yield results consistently close to the true value. Of course
a method may be accurate, but not precise, if the average of its
measurements is close to the true value, but those measurements
are highly dispersed. Conversely a method that is not accurate may
be quite precise, as it consistently indicates the same level of
inaccuracy. The tendency of a method of measurement to
consistently give results above or below the true value is a
source of systematic bias. The smaller the systematic bias, the
greater the accuracy of the method.

% The FDA define precision as the closeness of agreement (degree of
% scatter) between a series of measurements obtained from multiple
% sampling of the same homogeneous sample under prescribed
% conditions. \citet{Barnhart} describes precision as being further
% subdivided as either within-run, intra-batch precision or
% repeatability (which assesses precision during a single analytical
% run), or between-run, inter-batch precision or repeatability
%(which measures precision over time).

In the context of the agreement of two methods, there is also a
tendency of one measurement method to consistently give results
above or below the other method. Lack of agreement is a
consequence of the existence of `inter-method bias'. For two
methods to be considered in good agreement, the inter-method bias
should be in the region of zero.

\subsection{Statement of a Model}
\citet{BXC2010} presents a useful formulation for comparing two methods $X$ and $Y$, in their measurement of item $i$, where the unknown `true value' is $\tau_i$. Other authors, such as \citet{kinsella}, present similar formulations of the same model, as well as modified models to account for multiple measurements by each methods on each item, known as replicate measurements.

\begin{eqnarray} X_i = \tau_i + \delta_i , \phantom{spacin} \delta_i \sim \mathcal{N}(0,\sigma^2_\delta)\\ Y_i = \alpha + \beta \tau_i + \epsilon_i, \phantom{spaci}  \epsilon_i \sim \mathcal{N}(0,\sigma^2_\epsilon)\end{eqnarray}

In some types of analysis, such as the conversion problems described by \citet{lewis}, an estimate for 
the scaling factor $\beta$ may also be sought. For the time being, we will restrict ourselves to problems where $\beta$ is assumed to be 1. 

\begin{eqnarray}
X_i = \tau_i + \delta_i , \phantom{spacin} \delta_i \sim \mathcal{N}(0,\sigma^2_\delta)\\
 Y_i = \alpha + \beta \tau_i + \epsilon_i, \phantom{spaci}  \epsilon_i \sim \mathcal{N}(0,\sigma^2_\epsilon)
\end{eqnarray}

In this formulation, $\alpha$ represents the inter-method bias, and can be estimated as $E(X-Y)$. That is to say, a simple estimate of the inter-method bias is given by the differences between pairs of measurements.  Table~\ref{FCTdata} is a good example of possible inter-method bias; the `Fotobalk' consistently recording
smaller velocities than the `Counter' method. A cursory inspection of the table will indicate a systematic tendency for the Counter method to result in higher measurements than the Fotobalk method. % Consequently one would conclude that there is lack of agreement % between the two methods.

The absence of inter-method bias is, by itself, not sufficient to establish that two measurement methods agree. The two methods must also have equivalent levels of precision. Should one method yield results considerably more variable than those of the other, they can not be considered to be in agreement. Hence, method comparison studies are required to take account of both inter-method bias and difference in precision of measurements.

% latex table generated in R 2.6.0 by xtable 1.5-5 package
% Wed Aug 26 15:22:41 2009
\begin{table}[h!]

\begin{center}

\begin{tabular}{rrrr}
  \hline
 Round& Fotobalk (F) & Counter (C) & Difference (F-C) \\
  \hline
1 & 793.8& 794.6 & -0.8 \\
  2 & 793.1 & 793.9 & -0.8 \\
  3 & 792.4 & 793.2 & -0.8 \\
  4 & 794.0 & 794.0 & 0.0 \\
  5 & 791.4 & 792.2 & -0.8 \\
  6 & 792.4 & 793.1 & -0.7 \\
  7 & 791.7 & 792.4 & -0.7 \\
  8 & 792.3 & 792.8 & -0.5 \\
  9 & 789.6 & 790.2 & -0.6 \\
  10 & 794.4 & 795.0 & -0.6 \\
  11 & 790.9 & 791.6 & -0.7 \\
  12 & 793.5 & 793.8 & -0.3 \\
   \hline
\end{tabular}
\caption{Difference between Fotobalk and Counter measurements.}
\label{FCTdata}\end{center}
\end{table}
Even without computing the mean difference, a cursory examination of the table will indicate that one method consistently provides a measurement less than the other.
\newpage
\section{Purpose of Method Comparison Studies}
\citet{BXC2010} provides a review of many descriptions of the purpose of Method Comparison studies, several of which are reproduced here.

\begin{quote}
``The question being answered is not always clear, but is usually epxressed as an attempt to quantify the agreement
between two methods" \citep{BA95}.

``Some lack of agreement between different methods of measurement is inevitable. What matters is the amount by which they disagree. We want to know by how much the new method is likely to differ from the old, so that it is not enough to cause problems in the mathematical interpretation we can preplace the old method by the new, or even use the two interchangeably" \citep{BA99}.


``It often happens that the same physical and chemical property can be measured in different ways. For example, one can determine For example, one can determine sodium in serum by flame atomic emission spectroscopy or by isotope dilution mass spectroscopy. The question arises as to which method is better" (Mandel, 1991).

``In areas of inter-laboratory quality control, method comparisons, assay validations and individual bio-equivalence, etc, the agreement between observations and target (reference) values is
of interest" \citep{lin2002}.

``The purpose of comparing two methods of measurement of a continuous biological variable is to uncover systematic differences, not to point to
similarities`" \citep{ludbrook97}.

``In the pharmaceutical industry, measurement methods that measure the quantity of prdocuts are regulated. The FDA (U.S. Food and Drug Administration) requires that the manufacturer show equivalency prior to approving the new or alternatice method in quality control" (Tan \& Inglewicz, 1999). 
\end{quote}

While several major commonalities are present in each definitions, there is a different emphasis for each, which will inevitably give rise to confusion. \citet{BXC2010} seems to endorse a simple phrasing of the research question that is proposed by \citet{BA83}, i.e. ``\textit{do the two methods of measurement agree sufficiently closely?}" with \citet{BXC2010} expressing the view that other considerations (for example, the ``equivalence" of two methods) to be treated as separate research questions. As such, we will revert to other research questions, such as ``equivalence of methods" later, focussing on agreement and repeatability of methods.

\section{Repeatability}
Repeatability is the ability of a measurement method to give consistent results for a particular subject, i.e. a measurement will agree with prior and subsequent measurements of the same subject. \citet{Barnhart} emphasizes the importance of repeatability as part of an overall method comparison study, a view endorsed by \citet{BXC2008}. Before there can be good agreement between two methods, a method must have good agreement with itself. If one method has poor repeatability in the sense of considerable
variability, then agreement between two methods is bound to be
poor \citep{ARoy2009}. \citet{Barnhart} remarks that it is important to report repeatability when assessing
measurement, because it measures the purest form of random error
not influenced by other factors, while further remarking `\textit{curiously replicate measurements are rarely made in method comparison studies, so that an important aspect of comparability is often overlooked}. \citet{BA99} strongly recommends the simultaneous estimation of repeatability and agreement be collecting replicated data.
However \citet{ARoy2009} notes the lack of convenience in such calculations. 	Repeatability is defined by the \citet{IUPAC} as `\textit{the closeness of agreement between independent results obtained with the same method on identical test material, under the same conditions (same
	operator, same apparatus, same laboratory and after short intervals of time)}'  and is determined by taking multiple measurements on a series of subjects.



% %	Test-retest variability is practically used, for example, in medical monitoring of conditions.

A measurement is said to be repeatable when this variation is smaller than some pre-specified limit. In these situations, there is often a predetermined ``critical difference", and for differences in monitored values that are smaller than this critical difference, the possibility of pre-test variability as a sole cause of the difference may be considered in addition to, for examples, changes in diseases or treatments. 


The British Standards Institute (1979) defines a coefficient of repeatability  as \emph{the value below which the difference between two single test results may be expected to lie within a specified probability.} In the absence of other indications, the probability is 95\%.

	\subsection{Repeatability and Gold Standards}
	Currently the phrase `gold standard' describes the most accurate method of measurement available. No other criteria are set out. Further to \citet{dunnSEME}, various gold standards have a varying levels of repeatability. Dunn cites the example of the sphygmomanometer, which is prone to measurement error. Consequently it can be said that a measurement method can be the `gold standard', yet have poor repeatability. 
	
	% % % Bronze Standard
	\citet{dunnSEME} recognizes  this problem. Hence, if the most accurate method is considered to have poor repeatability, it is referred to as a ``bronze standard".  Again, no formal definition of a `bronze standard' exists.
	
	The coefficient of repeatability may provide the basis of formulation a formal definition of a `gold standard'. For example, by determining the ratio of $CR$ to the sample mean $\bar{X}$. Advisably the sample size should specified in advance. A gold standard may be defined as the method with the lowest value of $\lambda = CR /\bar{X}$ with $\lambda < 0.1\%$. Similarly, a silver standard may be defined as the method with the lowest value of $\lambda $ with $0.1\% \leq \lambda < 1\%$. Such thresholds are solely for expository purposes.

\chapter{Review of Current Methodologies}
\section{Bland-Altman Approach}
The issue of whether two measurement methods comparable to the
extent that they can be used interchangeably with sufficient
accuracy is encountered frequently in scientific research.
Historically, comparison of two methods of measurement was carried
out by use of paired sample $t-$test, correlation coefficients or
simple linear regression. However, simple linear regression is unsuitable for method comparison studies due to the assumption that one variable is measured without error. In comparing two methods, both methods are assume to have attendant random error.

\citet{BA83} highlighted the inadequacies of these approaches for comparing two methods of measurement, and proposed methodologies with this specific application in mind. Although the authors also acknowledge the opportunity to apply other, more complex, approaches, but argue that simpler approaches is preferable, especially when the
results must be `explained to non-statisticians'.

Notwithstanding previous remarks about linear regression, the first step recommended, which the authors argue should be mandatory, is the construction of a scatter plot of the data. Scatterplots can facilitate an initial judgement and
helping to identify potential outliers, with the addition of the line of equality. In the case of good agreement, the observations would be distributed closely along this line. However, they are not useful for a thorough examination of the data. \citet{BritHypSoc} notes that
data points will tend to cluster around the line of equality, obscuring interpretation.


A scatter plot of the Grubbs data is shown in Figure 1.1. Visual inspection confirms the previous conclusion that inter-method bias is present, i.e. the Fotobalk device has a tendency to record a lower velocity.

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=125mm]{images/GrubbsScatter.jpeg}
  \caption{Scatter plot for Fotobalk and Counter methods.}\label{GrubbsScatter}
\end{center}
\end{figure}

\citet{Dewitte} notes that scatter plots were very seldom
presented in the Annals of Clinical Biochemistry. This apparently
results from the fact that the `Instructions for Authors' dissuade
the use of regression analysis, which conventionally is
accompanied by a scatter plot.

\subsection{Bland-Altman Plots}

In light of shortcomings associated with scatterplots,
\citet*{BA83} recommend a further analysis of the data. Firstly
case-wise differences of measurements of two methods $d_{i} =
y_{1i}-y_{2i}, \mbox{ for }i=1,2,\dots,n$, on the same subject
should be calculated, and then the average of those measurements, 
($a_{i} = (y_{1i} + y_{2i})/2 \mbox{ for }i=1,2,\dots, n$.

Following a technique known as the Tukey mean-difference plot, as noted by \citet{kozak2014including}
\citet{BA83} proposed that $a_i$ should be plotted against $d_i$, a plot now widely known as the Bland-Altman plot, and motivated this plot as follows:
\begin{quote}
``From this type of plot it is much easier to assess the magnitude
of disagreement (both error and bias), spot outliers, and see
whether there is any trend, for example an increase in (difference) for high values. This way of plotting the data is a very powerful way of displaying the results of a method comparison study."
\end{quote}

The case wise-averages capture several aspects of the data, such as expressing the range over which the values were taken, and assessing whether the assumptions of constant variance holds.
Case-wise averages also allow the case-wise differences to be presented on a two-dimensional plot, with better data visualization qualities than a one dimensional plot. \citet{BA86}
cautions that it would be the difference against either measurement value instead of their average, as the difference relates to both value. This approach has proved very popular, and the Bland-Altman plots is widely regarded as powerful graphical tool for making a visual assessment of the data.

The magnitude of the inter-method bias between the two methods is simply the average of the differences $\bar{d}$. This inter-method bias is represented with a line on the Bland-Altman plot. As the objective of the Bland-Altman plot is to advise on the agreement of two methods, the individual case-wise differences are also particularly relevant. The variances around this bias is estimated by the standard deviation of these differences $S_{d}$.

\subsubsection*{Rendering a Bland-Altma plot}
Construction of a Bland-Altman plot can be implemented easily with \texttt{R} packages such as Bendix Carstensen's \texttt{MethComp} package, which is designed to \textit{provide computational tools to manipulate, display and analyze data from method comparison studies} \citep{BXC2010}.

\subsection{Bland-Altman plots for the Grubbs data}

In the case of the Grubbs data the inter-method bias is $-0.61$ metres per second, and is indicated by the dashed line on Figure 1.2. By inspection of the plot, it is also possible to compare the precision of each method. Noticeably the differences tend to increase as the averages increase.


The Bland-Altman plot for comparing the `Fotobalk' and `Counter'
methods, which shall henceforth be referred to as the `F vs C'
comparison,  is depicted in Figure 1.2, using data from Table 1.3.
The presence and magnitude of the inter-method bias is indicated
by the dashed line.
\newpage

%Later it will be shown that case-wise differences are the sole
%component of the next part of the methodology, the limits of
%agreement.


\begin{table}[h!]
\renewcommand\arraystretch{0.7}%
\begin{center}
\begin{tabular}{|c||c|c||c|c|}
  \hline
 Round & Fotobalk  & Counter  & Differences  & Averages  \\
  &  [F] & [C] & [F-C] &  [(F+C)/2] \\
  \hline
1 & 793.8 & 794.6 & -0.8 & 794.2 \\
  2 & 793.1 & 793.9 & -0.8 & 793.5 \\
  3 & 792.4 & 793.2 & -0.8 & 792.8 \\
  4 & 794.0 & 794.0 & 0.0 & 794.0 \\
  5 & 791.4 & 792.2 & -0.8 & 791.8 \\
  6 & 792.4 & 793.1 & -0.7 & 792.8 \\
  7 & 791.7 & 792.4 & -0.7 & 792.0 \\
  8 & 792.3 & 792.8 & -0.5 & 792.5 \\
  9 & 789.6 & 790.2 & -0.6 & 789.9 \\
  10 & 794.4 & 795.0 & -0.6 & 794.7 \\
  11 & 790.9 & 791.6 & -0.7 & 791.2 \\
  12 & 793.5 & 793.8 & -0.3 & 793.6 \\
   \hline
\end{tabular}
\caption{Fotobalk and Counter methods: differences and averages.}
\end{center}
\end{table}

\begin{table}[h!]
\renewcommand\arraystretch{0.7}%
\begin{center}
\begin{tabular}{|c||c|c||c|c|}
  \hline
 Round & Fotobalk  & Terma  & Differences  & Averages  \\
  &  [F] & [T] & [F-T] &  [(F+T)/2] \\
  \hline
1 & 793.8 & 793.2 & 0.6 & 793.5 \\
  2 & 793.1 & 793.3 & -0.2 & 793.2 \\
  3 & 792.4 & 792.6 & -0.2 & 792.5 \\
  4 & 794.0 & 793.8 & 0.2 & 793.9 \\
  5 & 791.4 & 791.6 & -0.2 & 791.5 \\
  6 & 792.4& 791.6 & 0.8 & 792.0 \\
  7 & 791.7 & 791.6 & 0.1 & 791.6 \\
  8 & 792.3 & 792.4 & -0.1 & 792.3 \\
  9 & 789.6 & 788.5 & 1.1 & 789.0 \\
  10 & 794.4 & 794.7 & -0.3 & 794.5 \\
  11 & 790.9 & 791.3 & -0.4 & 791.1 \\
  12 & 793.5 & 793.5 & 0.0 & 793.5 \\

   \hline
\end{tabular}
\caption{Fotobalk and Terma methods: differences and averages.}
\end{center}
\end{table}

\newpage

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=120mm]{images/GrubbsBAplot-noLOA.jpeg}
  \caption{Bland-Altman plot For Fotobalk and Counter methods.}\label{GrubbsBA-noLOA}
\end{center}
\end{figure}



In Figure 1.3 Bland-Altman plots for the `F vs C' and `F vs T'
comparisons are shown, where `F vs T' refers to the comparison of
the `Fotobalk' and `Terma' methods. Usage of the Bland-Altman plot
can be demonstrate in the contrast between these comparisons. By inspection, there exists a larger inter-method bias in the `F vs C' comparison than in the `F vs T' comparison. Conversely there
appears to be less precision in `F vs T' comparison, as indicated
by the greater dispersion of covariates.

\begin{figure}[h!]
\begin{center}
  \includegraphics[height=90mm]{images/GrubbsDataTwoBAplots.jpeg}
  \caption{Bland-Altman plots for Grubbs' F vs C and F vs T comparisons.}\label{GrubbsDataTwoBAplots}
\end{center}
\end{figure}

\newpage


%\subfile{TechAcceptModel.tex}
\subsection{Adverse features}

Estimates for inter-method bias and variance of differences are only meaningful if there is uniform inter-bias and variability throughout the range of measurements. Fulfilment of these assumptions can be checked by visual inspection of the plot.The prototype Bland-Altman plots depicted in Figures 1.4, 1.5 and 1.6 are derived from simulated data, for the purpose of demonstrating how the plot would inform an analyst of features that would adversely affect use of the recommended approach.

Figure 1.4 demonstrates how the Bland-Altman plot would indicate
increasing variance of differences over the measurement range.
Fitted regression lines, for both the upper and lower half of the
plot, has been added to indicate the trend. Figure 1.5 is an
example of cases where the inter-method bias changes over the
measurement range. This is known as proportional bias, and is
defined by \citet{ludbrook97} as meaning that `one method gives
values that are higher (or lower) than those from the other by an
amount that is proportional to the level of the measured variable'. In both Figures 1.4 and 1.5, the assumptions necessary
for further analysis using the limits of agreement are violated.

Application of regression techniques to the Bland-Altman plot, and
subsequent formal testing for the constant variability of
differences is informative. The data set may be divided into two
subsets, containing the observations wherein the difference values
are less than and greater than the inter-method bias respectively.
For both of these fits, hypothesis tests for the respective slopes
can be performed. While both tests could be considered separately,
multiple comparison procedures, such as the Benjamini-Hochberg
\citep{BH} test, are advisable.

\begin{figure}[h!]
\begin{center}
  \includegraphics[height=90mm]{images/BAFanEffect.jpeg}
  \caption{Bland-Altman plot demonstrating the increase of variance over the range.}\label{BAFanEffect}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[height=90mm]{images/PropBias.jpeg}
  \caption{Bland-Altman plot indicating the presence of proportional bias.}\label{PropBias}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=125mm]{images/BAOutliers.jpeg}
  \caption{Bland-Altman plot indicating the presence of potential outliers.}\label{Outliers}
\end{center}
\end{figure}

\newpage


The Bland-Altman plot also can be used to identify outliers. An
outlier is an observation that is conspicuously different from the
rest of the data that it arouses suspicion that it occurs due to a
mechanism, or conditions, different to that of the rest of the
observations. \citet*{BA99} do not recommend excluding outliers from analyses,
but remark that recalculation of the inter-method bias estimate,
and further calculations based upon that estimate, are useful for
assessing the influence of outliers. The authors remark that `we
usually find that this method of analysis is not too sensitive to
one or two large outlying differences'. Figure 1.6 demonstrates how the Bland-Altman
plot can be used to visually inspect the presence of potential
outliers.

As a complement to the Bland-Altman plot, \citet{Bartko} proposes
the use of a bivariate confidence ellipse, constructed for a
predetermined level. \citet{AltmanEllipse} provides the relevant calculations for the
ellipse. This ellipse is intended as a visual
guidelines for the scatter plot, for detecting outliers and to
assess the within- and between-subject variances.

The minor axis relates to the between subject variability, whereas
the major axis relates to the error mean square, with the ellipse
depicting the size of both relative to each other.
Consequently Bartko's ellipse provides a visual aid to determining the
relationship between variances. If $\mbox{var}(a)$ is greater than $\mbox{var}(d)$, the orientation of the ellipse is horizontal. Conversely if $\mbox{var}(a)$ is less than $\mbox{var}(d)$, the orientation of the ellipse is vertical.


%(Furthermore \citet{Bartko}
%proposes formal testing procedures, that shall be discussed in due
%course.)

The Bland-Altman plot for the Grubbs data, complemented by Bartko's ellipse, is depicted in Figure 1.7.
The fourth observation is shown to be outside the bounds of the ellipse, indicating that it is a potential outlier.


\begin{figure}[h!]
  % Requires \usepackage{graphicx}
  \includegraphics[width=130mm]{images/GrubbsBartko.jpeg}
  \caption{Bartko's Ellipse for Grubbs' data.}\label{GrubbsBartko}
\end{figure}

The limitations of using bivariate approaches to outlier detection
in the Bland-Altman plot can demonstrated using Bartko's ellipse.
A covariate is added to the `F vs C' comparison that has a
difference value equal to the inter-method bias, and an average
value that markedly deviates from the rest of the average values
in the comparison, i.e. 786. Table 1.8 depicts a $95\%$ confidence
ellipse for this manipulated data set. By inspection of the
confidence interval, we would conclude that this extra
covariate is an outlier, in spite of the fact that this
observation is very close to the inter-method bias as determined by this approach.

\begin{figure}[h!]
  % Requires \usepackage{graphicx}
  \includegraphics[width=130mm]{images/GrubbsBartko2.jpeg}
  \caption{Bartko's Ellipse for Grubbs' data, with an extra covariate.}\label{GrubbsBartko2}
\end{figure}


Importantly, outlier classification must be informed by the logic of the
mechanism that produces the data. In the Bland-Altman plot, the horizontal displacement (i.e. the average) of any
observation is supported by two separate measurements. Any
observation should not be considered an outlier on the basis of a
noticeable horizontal displacement from the main cluster, as in
the case with the extra covariate. Conversely, the fourth
observation, from the original data set, should be considered an
outlier, as it has a noticeable vertical displacement from the
rest of the observations.

%Grubbs' test is a statistical test used for detecting outliers in a
%univariate data set that is assumed to be normally distributed.

%\citet{Grubbs} defined an outlier as a co-variate that appears to
%deviate markedly from other members of the sample in which it
%occurs.

In classifying whether a observation from a univariate data set is
an outlier, many formal tests are available, such as the Grubbs test for outliers. In assessing
whether a covariate in a Bland-Altman plot is an outlier, this
test is useful when applied to the case-wise difference values treated as a
univariate data set. The null hypothesis of the Grubbs test procedure is the absence
of any outliers in the data set. Conversely, the alternative hypotheses is that there is at least one outlier
present.

The test statistic for the Grubbs test ($G$) is the largest
absolute deviation from the sample mean divided by the standard
deviation of the differences,
\begin{equation}
G =  \displaystyle\max_{i=1,\ldots, n}\frac{\left \vert d_i -
\bar{d}\right\vert}{S_{d}}.
\end{equation}

For the `F vs C' comparison it is the fourth observation gives
rise to the test statistic, $G = 3.64$. The critical value is
calculated using Student's $t$ distribution and the sample size,
\[
U = \frac{n-1}{\sqrt{n}} \sqrt{\frac{t_{\alpha/(2n),n-2}^2}{n - 2
+ t_{\alpha/(2n),n-2}^2}}.
\]
For this test $U = 0.75$. The conclusion of this test is that the fourth observation in the `F vs C' comparison is an outlier, with $p-$value = 0.003, in accordance with the previous result of Bartko's ellipse.


\section{Limits of Agreement}
% introduces
A third element of the Bland-Altman approach, an interval known
as `limits of agreement' is introduced in \citet*{BA86}
(sometimes referred to in literature as 95\% limits of agreement).
Limits of agreement are used to assess whether the two methods of
measurement can be used interchangeably. \citet{BA86} refer to
this as the `equivalence' of two measurement methods. The specific question to which limits of
agreement are intended as the answer to must be
established clearly. \citet*{BA95} comment that the limits of agreement show `how
far apart measurements by the two methods were likely to be for
most individuals', a definition echoed in their 1999 paper:

\begin{quote}``We can then say that nearly all pairs
of measurements by the two methods will be closer together than
these extreme values, which we call 95\% limits of agreement.
These values define the range within which most differences
between measurements by the two methods will lie."
\end{quote}

The limits of agreement (LoA) are computed by the following
formula:
\[
LoA = \bar{d} \pm 1.96 s_{d}
\]
with $\bar{d}$ as the estimate of the inter method bias, $s_{d}$
as the standard deviation of the differences and 1.96 (sometimes rounded to 2) is the 95\%
quantile for the standard normal distribution. The limits of agreement methodology assumes a constant level of bias throughout the range of measurements. Importantly the authors recommend prior determination of what would constitute acceptable
agreement, and that sample sizes should be predetermined to give an accurate conclusion. However \citet{mantha} highlight inadequacies in the correct application of limits of agreement, resulting in contradictory estimates of limits of agreement in various papers.

%\begin{quote}
%``How far apart measurements can be without causing difficulties
%will be a question of judgment. Ideally, it should be defined in
%advance to help in the interpretation of the method comparison and
%to choose the sample size \citep{BA86}".
%\end{quote}


For the Grubbs `F vs C' comparison, these limits
of agreement are calculated as -0.132 for the upper bound, and
-1.08 for the lower bound. Figure 1.9 shows the resultant
Bland-Altman plot, with the limits of agreement shown in dashed
lines.


\begin{figure}[h!]
\begin{center}
  \includegraphics[width=125mm]{images/GrubbsBAplot-LOA.jpeg}
  \caption{Bland-Altman plot with limits of agreement}\label{GrubbsBAplot-noLOA}
\end{center}
\end{figure}

%But as \citet*{BA86} point out this may not be the case. Variants of the limits of agreement that overcome this
% problem shall be introduced in due course.

\subsection{Inferences on Bland-Altman estimates}
\citet*{BA99} advises on how to calculate confidence intervals for the inter-method bias and limits of agreement.
For the inter-method bias, the confidence interval is a simply that of a mean: $\bar{d} \pm t_{(\alpha/2,n-1)} S_{d}/\sqrt{n}$.
The confidence
intervals and standard error for the limits of agreement follow from the variance of the limits of agreement, which is shown to be

 \[
  \mbox{Var}(LoA) = (\frac{1}{n}+\frac{1.96^{2}}{2(n-1)})s_{d}^{2}.
 \]

If $n$ is sufficiently large this can be following approximation
can be used
 \[
  \mbox{Var}(LoA) \approx 1.71^{2}\frac{s_{d}^{2}}{n}.
 \]
Consequently the standard errors of both limits can be
approximated as $1.71$ times the standard error of the
differences.

A $95\%$ confidence interval can be determined, by means of the
\emph{t} distribution with $n-1$ degrees of freedom. However, \citet*{BA99} comment that such calculations  may be `somewhat optimistic' on account of the associated assumptions not being realized.

%\subsubsection{Small Sample Sizes} The limits of agreement are
%estimates derived from the sample studied, and will differ from
%values relevant to the whole population, hence the importance of a
%suitably large sample size. A different sample would give
%different limits of agreement. Student's t-distribution is a well
%known probability distribution used in statistical inference for
%normally distributed populations when the sample size is small
%\citep{student,Fisher3}. Consequently, using 't' quantiles , as
%opposed to standard normal quantiles, may give a more appropriate
%calculation for limits of agreement when the sample size is small.
%For sample size $n=12$ the `t' quantile is 2.2 and the limits of
%agreement are (-0.074,-1.143).


\subsection{Formal definition of limits of agreement}
\citet{BA99} note the similarity of limits of agreement to
confidence intervals, but are clear that they are not the same
thing. Interestingly, they describe the limits as `being like a
reference interval'.

Limits of agreement have very similar construction to Shewhart
control limits. The Shewhart chart is a well known graphical
methodology used in statistical process control. Consequently
there is potential for misinterpreting the limits of agreement as
they were Shewhart control limits. 
%Importantly the
%parameters used to determine the Shewhart limits are time ordered, based on the process's historical values, a key difference with Bland-Altman limits of agreement.

\citet{BXC2008} regards the limits of agreement as a prediction
interval for the difference between future measurements with the
two methods on a new individual, but states that it does not fit
the formal definition of a prediction interval, since the
definition does not consider the errors in estimation of the
parameters. Prediction intervals, which are often used in
regression analysis, are estimates of an interval in which future
observations will fall, with a certain probability, given what has
already been observed. \citet{BXC2008} offers an alternative
formulation, a $95\%$ prediction interval for the difference
\[
\bar{d} \pm t_{(0.025, n-1)}s_{d} \sqrt{1+\frac{1}{n}}
\]

\noindent where $n$ is the number of subjects. Carstensen is
careful to consider the effect of the sample size on the interval
width, adding that only for 61 or more subjects is the
quantile less than 2.

\citet{luiz} offers an alternative description of limits of
agreement, this time as tolerance limits. A tolerance interval for
a measured quantity is the interval in which a specified fraction
of the population's values lie, with a specified level of
confidence. \citet{Barnhart} describes them as a probability
interval, and offers a clear description of how they should be
used; `if the absolute limit is less than an acceptable difference
$d_{0}$, then the agreement between the two methods is deemed
satisfactory'.

The prevalence of contradictory definitions of what limits of agreement strictly are will inevitably attenuate the poor standard of reporting using limits of agreement, as mentioned by \citet{mantha}.

%At least 100 historical
%values must be used to determine the acceptable value (i.e the
%process mean) and the process standard deviation. The principle
%that the mean and variance of a large sample of a homogeneous
%population is a close approximation of the population's mean and
%variance justifies this.

%\begin{figure}[h!]
%\begin{center}
%  \includegraphics[width=125mm]{GrubbsLOAwCIs.jpeg}
%  \caption{Limits of agreement with confidence intervals}\label{LOAwCIs}
%\end{center}
%\end{figure}

%\newpage
%\section{Agreement Indices}
%\citet{Barnhart} provided an overview of several agreement
%indices, including the limits of agreement. Other approaches, such
%as mean squared deviation, the tolerance deviation index and
%coverage probability are also discussed.

\subsection{Alternative Agreement Indices}
As an alternative to limits of agreement, \citet{lin2002} proposes the use of
the mean square deviation in assessing agreement. The mean square
deviation is defined as the expectation of the squared differences
of two readings. The MSD is usually used for the case of two
measurement methods $X$ and $Y$, each making one measurement for
the same subject, and is given by
\[
MSDxy = E[(x - y)^2]  = (\mu_{x} - \mu_{y})^2 + (\sigma_{x} -
\sigma_{y})^2 + 2\sigma_{x}\sigma_{y}(1-\rho_{xy}).
\]


\citet{Barnhart} advises the use of a predetermined upper limit
for the MSD value, $MSD_{ul}$, to define satisfactory agreement.
However, a satisfactory upper limit may not be easily
determinable, thus creating a drawback to this methodology.


Alternative indices, proposed by \citet{Barnhart}, are the square root of the MSD and the expected absolute difference (EAD). 
\[
EAD = E(|x - y|) = \frac{\sum |x_{i}- y_{i}|}{n}
\]


Both of these indices can be interpreted intuitively, since their units are the same as that of the original measurements. Also they can be compared to the maximum acceptable absolute difference between two methods of measurement $d_{0}$. For the sake of brevity, the EAD will be considered solely.

The EAD can be used to supplement the inter-method bias in an
initial comparison study, as the EAD is informative as a measure
of dispersion, is easy to calculate and requires no distributional
assumptions. A consequence of using absolute differences is that high variances would result in a higher EAD value. 

% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Mon Feb 23 21:12:33 2015
% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Mon Feb 23 21:13:45 2015
% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Mon Feb 23 22:10:26 2015
%\begin{table}[ht]
%	\centering
%	\begin{tabular}{r| rrrr}
%		\hline
%		\item & X & Y & U & V \\ 
%		\hline
%		1 & 101.83 & 102.52 & 98.05 & 99.53 \\ 
%		2 & 101.68 & 102.69 & 99.17 & 96.53 \\ 
%		3 & 97.89 & 99.01 & 100.31 & 97.55 \\ 
%		4 & 98.15 & 99.57 & 100.35 & 96.03 \\ 
%		5 & 99.94 & 100.85 & 99.51 & 99.00 \\ 
%		6 & 98.85 & 98.86 & 98.50 & 100.76 \\ 
%		7 & 99.86 & 97.85 & 100.66 & 99.37 \\ 
%		8 & 101.57 & 100.21 & 99.66 & 108.87 \\ 
%		9 & 100.12 & 99.85 & 99.70 & 105.16 \\ 
%		10 & 99.49 & 98.77 & 101.55 & 94.31 \\ 
%		\hline
%	\end{tabular}
%\end{table}




\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& U & V & $U-V$ & $|U-V|$ \\ 
		\hline
		1 & 98.05 & 99.53 & -1.49 & 1.49 \\ 
		2 & 99.17 & 96.53 & 2.64 & 2.64 \\ 
		3 & 100.31 & 97.55 & 2.75 & 2.75 \\ 
		4 & 100.35 & 96.03 & 4.32 & 4.32 \\ 
		5 & 99.51 & 99.00 & 0.51 & 0.51 \\ 
		6 & 98.50 & 100.76 & -2.26 & 2.26 \\ 
		7 & 100.66 & 99.37 & 1.29 & 1.29 \\ 
		8 & 99.66 & 108.87 & -9.21 & 9.21 \\ 
		9 & 99.70 & 105.16 & -5.45 & 5.45 \\ 
		10 & 101.55 & 94.31 & 7.24 & 7.24 \\ 
		\hline
	\end{tabular}
	\caption{Example data set}
	\label{EADdata}
\end{table}

To illustrate the use of EAD, consider table ~\ref{EADdata}. The inter-method bias is 0.03, which is quite close to zero, which is desirable in the context of agreement. However, an identity plot would indicate very poor agreement, as the points are noticeably distant from the line of equality.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{EAD-UV}
\caption{Identity Plot for example data}
\label{fig:EADidentity}
\end{figure}

The limits of agreement are $[-9.61, 9.68]$, a wide interval for this data. As with the identity plot, this would indicate lack of agreement. As with inter-method bias, an EAD value close to zero is desirable. However, from table ~\ref{EADdata}, the EAD can be computed as 3.71. The Bland-Altman plot remains a useful part of the analysis. In \ref{fig:EAD1}, it is clear there is a systematic decrease in differences across the range of measurements.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/EAD1}
	\caption{Bland-Altman Plot for UV comparison}
	\label{fig:EAD1}
\end{figure}

\citet{Barnhart} remarks that a comparison of EAD and MSD , using
simulation studies, would be interesting, while further adding
that `\textit{It will be of interest to investigate the benefits of these
possible new unscaled agreement indices}'. For the Grubbs' `F vs C' and `F vs T' comparisons, the inter-method bias, difference variances, limits of agreement and EADs are shown
in Table 1.5. The corresponding Bland-Altman plots for `F vs C' and `F vs T' comparisons were depicted previously on Figure 1.3. While the inter-method bias for the `F vs T' comparison is smaller, the EAD penalizes the comparison for having a greater variance of differences. Hence the EAD values for both comparisons are much closer.
\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c|c|}
  \hline
 & F vs C & F vs T  \\
  \hline
Inter-method bias & -0.61 & 0.12 \\ \hline
Difference variance & 0.06 & 0.22  \\ \hline 
Limits of agreement & (-1.08,	-0.13) & (-0.81,1.04) \\
  EAD & 0.61 & 0.35  \\ \hline 
   \hline
\end{tabular}
\caption{Agreement indices for Grubbs' data comparisons.}
\end{center}
\end{table}

Further to  \citet{lin2000} and \citet{lin2002}, individual agreement between two measurement methods may be
assessed using the the coverage probability (CP) criteria or the total deviation index (TDI). If $d_{0}$ is predetermined as the maximum acceptable absolute difference between two methods of measurement, the probability that the absolute difference of two measures being less than $d_{0}$ can be computed. This is known as the coverage probability (CP).

\begin{equation}
CP = P(|x_{i} - y_{i}| \leq d_{0})
\end{equation}

If $\pi_{0}$ is set as the predetermined coverage probability, the
boundary under which the proportion of absolute differences is
$\pi_{0}$ may be determined. This boundary is known as the `total
deviation index' (TDI). Hence the TDI is the $100\pi_{0}$
percentile of the absolute difference of paired observations.
\subsection{Prevalence of the Bland-Altman plot}
%---------------------------------------------%

\citet*{BA86}, which further develops the Bland-Altman methodology,
was found to be the sixth most cited paper of all time by the
\citet{BAcite}. \cite{Dewitte} describes the rate at which
prevalence of the Bland-Altman plot has developed in scientific
literature. \citet{Dewitte} reviewed the use of Bland-Altman plots
by examining all articles in the journal `Clinical Chemistry'
between 1995 and 2001. This study concluded that use of the
Bland-Altman plot increased over the years, from 8\% in 1995 to
14\% in 1996, and 31-36\% in 2002.

The Bland-Altman Plot has since become expected, and
often obligatory, approach for presenting method comparison
studies in many scientific journals \citep{hollis}. Furthermore
\citet{BritHypSoc} recommend its use in papers pertaining to
method comparison studies for the journal of the British
Hypertension Society.

\chapter{Improper MCS Techniques}
	
	\subsection{Paired sample T-test} \citet{Bartko} discusses the use
	of the well known paired sample $t$ test to test for inter-method
	bias; $H: \mu_{D}=0$. The test statistic is distributed a $t$
	random variable with $n-1$ degrees of freedom and is calculated as
	follows;
	
	\begin{equation}
	t^{*} = \bar{D}/ \frac{S_{D}}{\sqrt{n}}
	\end{equation}
	
	where $\bar{D}$ and $S_{D}$ is the average of the differences of
	the $n$ observations.
	\newpage
	\begin{itemize}
		% http://www.jerrydallal.com/LHSP/compare.htm
		\item Paired t tests test only whether the mean responses are the same. Certainly, we want the means to be the same, but this is only a small part of the story. The means can be equal while the (random) differences between measurements can be huge.
		\item The correlation coefficient measures linear agreement--whether the measurements go up-and-down together. Certainly, we want the measures to go up-and-down together, but the correlation coefficient itself is deficient in at least three ways as a measure of agreement.
		The correlation coefficient can be close to 1 (or equal to 1!) even when there is considerable bias between the two methods. For example, if one method gives measurements that are always 10 units higher than the other method, the correlation will be 1 exactly, but the measurements will always be 10 units apart.
		\item The magnitude of the correlation coefficient is affected by the range of subjects/units studied. 
		\item The correlation coefficient can be made smaller by measuring samples that are similar to each other and larger by measuring samples that are very different from each other. The magnitude of the correlation says nothing about the magnitude of the differences between the paired measurements which, when you get right down to it, is all that really matters.
		\item The usual significance test involving a correlation coefficient-- whether the population value is 0--is irrelevant to the comparability problem. What is important is not merely that the correlation coefficient be different from 0. Rather, it should be close to (ideally, equal to) 1!
	\end{itemize}
	\newpage
	\subsubsection*{intra-class correlation coefficient}
	\begin{itemize}
		\item The intra-class correlation coefficient has a name guaranteed to cause the eyes to glaze over and shut the mouth of anyone who isn't an analyst. The ICC, which takes on values between 0 and 1, is based on analysis of variance techniques. It is close to 1 when the differences between paired measurements is very small compared to the differences between subjects. Of these three procedures--t test, correlation coefficient, intra-class correlation coefficient--the ICC is best because it can be large only if there is no bias and the paired measurements are in good agreement, but it suffers from the same faults ii and iii as ordinary correlation coefficients. The magnitude of the ICC can be manipulated by the choice of samples to split and says nothing about the magnitude of the paired differences.
	\end{itemize}
	\newpage
	\subsubsection*{Regression Methods}
	\begin{itemize}
		\item Regression analysis is typically misused by regressing one measurement on the other and declare them equivalent if and only if the confidence interval for the regression coefficient includes 1. Some simple mathematics shows that if the measurements are comparable, the population value of the regression coefficient will be equal to the correlation coefficient between the two methods. 
		The population correlation coefficient may be close to 1, but is never 1 in practice. Thus, the only things that can be indicated by the presence of 1 in the confidence interval for the regression coefficient is (1) that the measurements are comparable but there weren't enough observations to distinguish between 1 and the population regression coefficient, or (2) the population regression coefficient is 1 and therefore, the measurements aren't comparable.
		
		\item There is a line whose slope will be 1 if the measurements are comparable. It is known as a structural equation and is the method advanced by Kelly (1985). Altman and Bland (1987) criticize it for a reason that should come as no surprise: Knowing the data are consistent with a structural equation with a slope of 1 says something about the absence of bias but *nothing* about the variability about Y = X (the difference between the measurements), which, as has already been stated, is all that really matters.
	\end{itemize}
	%-----------------------------------------------------------------------------------------------------%
%% TAM
\section{The Technology Acceptance Model}
Davis (1989) proposes the TAM model, which suggests an hypothesis as to why users may adopt particular technologies, and not others. 
According to this theory, when users are presented with a new 
technology, two important factors will influence their decision about how and when they will adopt it.
\begin{description}
	\item[Perceived usefulness (PU)] - This was defined by Fred Davis as "the degree to which a person believes that using a particular system would enhance his or her job performance".
	\item[Perceived ease-of-use (PEOU)] - Davis defined this as "the degree to which a person believes that using a particular system would be free from effort" 
\end{description}

Davis's explanations of these term can be rephrased for application to statistical analysis. 
Perceived Use could refer to the degree to which an user would deem a particular statistical method would properly establish the results of an analaysis. In the case of method comparison studies, proper indication of agreement, or lack thereof.


Perceived ease-of-use requires only applying the context of a satistical problem. A very modest statistical skill set is the only prerequistive for constructing a Bland-Altman plot, and computing limits of agreement. The main building blocks 
are simple descriptive, statistics and a knowledge of the normal distribution. These are topics that feature in almost every undergraduate statistics courses. Furthermore \citet{kikozak2014including} recommends including the Bland-Altman method itself in undergraduate teaching.
%---------------------------------------------%

In short, the user perceives the Bland-Altman methodology to be an easy-to-implement technique, that will properly address the question of agreement.

Conversely the Survival plot is a derivative of the Kaplan-Meier Curve, a non-parametric graphical technique that features in Survival Analysis. This subject area is a well known domain of statistics, but would be encountered 
on curriculums of specialist courses. 

The Mountain Plot is formally called the empirical folder cumulative distribution plot. While not particularly hard to render, the procedure is not straight-forward for the casual user. Currently there is only one software implementation, \textbf{\textit{medcalc.be}} toolkit.
	\section{Variations and Alternative Graphical Methods}
	In this section, we will look at some variations and enhancements of the Bland-Altman plot, as well as some alternative graphcial techniques. Strictly speaking, the Identity Plot is advised by Bland and Altman as a prior analysis to the Bland-Alman plot, and therefore is neither a variant nor an alternative approach. However it is worth mentioning, as it is a simple, powerful and elegant technique that is often overlooked in method comparison studies. The identity plot is a simple scatter-plot approach of measurements for both methods on either axis, with the line of equality (the $X=Y$ line, i.e. the 45 degree line through the origin). This plot can gives the analyst a cursory examination of how well the measurement methods agree. In the case of good agreement, the covariates of the plot accord closely with the line of equality.
	
	\subsection{Variants of the Bland-Altman Plot}
	In light of some potential pitfalls associated with the conventional difference plot, a series of alternative formulations for the Bland-Altman approach have been proposed.
	
	Referring to the assumption that bias and variability are constant across the range
	of measurements, \citet{BA99} address the case where there is an increase in variability as the magnitude increases. They remark 	that it is possible to ignore the issue altogether, but the limits of agreement would be wider apart than necessary when just lower magnitude measurements are considered. Conversely the limits would be too narrow should only higher magnitude measurements be used.	To address the issue, they propose the logarithmic transformation of the data. The plot is then formulated as the difference of paired log values against their mean. Bland and Altman acknowledge that this is not easy to interpret, and may not be suitable in all cases.
	
	\subsubsection*{Bland and Altman's Percentage and Ratio Plots}
	%------------------------------------------------------------- %
	\citet{BA99} offer two variations of the Bland-Altman plot intended to overcome situations where the conventional plot is inappropriate. The first variation is a plot of casewise differences as percentage of averages, and is appropriate when the variability of the differences increase as the
	magnitude increases. 
	
	%------------------------------------------------------------- %
	% % RATIO / EKSBORG
	The second variation is a plot of casewise ratios as percentage of averages. This will remove the need for
	logarithmic transformation. This approach is useful when there is an increase in variability of the differences as the magnitude of the measurement increases. \citet{Eksborg} proposed such a ratio plot,
	independently of Bland and Altman. \citet{Dewitte} commented on
	the reception of this article by saying `\textit{Strange to say, this 
		report has been overlooked}'.
	
	
	%	%----------------------------------------------------------------%
	%	\section{Dewitte et al }
	%	\begin{quote}When the standard deviation increases with concentration, Bland and Altman recommend a logarithmic y scale, whereas others propose a percent y scale (Pollock et al, 2002). Although generally there is not much difference in effect between using percentages and using a log transformation of the data, we prefer the percent plot (except when data extend over several orders of magnitude) because numbers can be read directly from the plot without the need for back-transformation.
	%	\end{quote}
	%	
	%	\begin{verbatim}
	%	absolute - small range
	%	percentage - medium range
	%	log scale - large range
	%	\end{verbatim}
	%==================================================== %

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsubsection{Bartko's Ellipse}
	
	As an enhancement on the Bland Altman Plot, \citet{Bartko} has
	expounded a confidence ellipse for the covariates. \citet{Bartko} proposes
	a bivariate confidence ellipse as a boundary for dispersion. The stated purpose is to `amplify dispersion', which presumably is for  the purposes of outlier detection. The orientation of the the ellipse is key to interpreting the results. The minor axis is related to the between-item variability whereas the major axis is related to the mean squared error (referred to here as Error Mean Square).The ellipse illustrates the size of both relative to each
	other. 
	
	
	Consequently Bartko's ellipse provides a visual aid to determining the
	relationship between variances. 
	Furthermore, the ellipse provides a visual aid to determining the relationship
	between the variance of the means $Var(a_{i})$ and the variance of the differences $Var(d_{i})$. If $\mbox{var}(a)$ is greater than $\mbox{var}(d)$, the orientation of the ellipse is horizontal. Conversely if $\mbox{var}(a)$ is less than $\mbox{var}(d)$, the orientation of the ellipse is vertical. The more horizontal the ellipse, the greater the degree of agreement between the two methods being tested.
	
	
	%(Furthermore \citet{Bartko}
	%proposes formal testing procedures, that shall be discussed in due
	%course.)
	Bartko states that the ellipse can, inter alia, be used to detect the presence of outliers (furthermore
	\citet{Bartko} proposes formal testing procedures, that shall be discussed in due course). 
	The Bland-Altman plot for the Grubbs data, complemented by Bartko's ellipse, is depicted in Figure ~\ref{GrubbsBartko1}.
	The fourth observation is shown to be outside the bounds of the ellipse, indicating that it is a potential outlier.
	
	
	\begin{centering}
		\begin{figure}[h!]
			% Requires \usepackage{graphicx}
			\includegraphics[width=130mm]{images/GrubbsBartko.jpeg}
			\caption{Bartko's Ellipse For Grubbs' Data.}
			\label{GrubbsBartko1}
		\end{figure}
	\end{centering}
	
	The limitations of using bivariate approaches to outlier detection
	in the Bland-Altman plot can demonstrated using Bartko's ellipse.
	A covariate is added to the `F vs C' comparison that has a
	difference value equal to the inter-method bias, and an average
	value that markedly deviates from the rest of the average values
	in the comparison, i.e. 786. Table 1.8 depicts a $95\%$ confidence
	ellipse for this manipulated data set. By inspection of the
	confidence interval, a conclusion would be reached that this extra
	covariate is an outlier, in spite of the fact that this
	observation is wholly consistent with the conclusion of the
	Bland-Altman plot.
	
	%\begin{figure}[h!]
	%  % Requires \usepackage{graphicx}
	%  \includegraphics[width=130mm]{GrubbsBartko2.jpeg}
	%  \caption{Bartko's Ellipse For Grubbs' Data, with an extra covariate.}\label{GrubbsBartko2}
	%\end{figure}
	
	
	Importantly, outlier classification must be informed by the logic of the
	data's formulation. In the Bland-Altman plot, the horizontal displacement of any
	observation is supported by two independent measurements. Any
	observation should not be considered an outlier on the basis of a
	noticeable horizontal displacement from the main cluster, as in
	the case with the extra covariate. Conversely, the fourth
	observation, from the original data set, should be considered an
	outlier, as it has a noticeable vertical displacement from the
	rest of the observations.
	\newpage
	
	\begin{figure}[h!]
		% Requires \usepackage{graphicx}
		\includegraphics[width=130mm]{images/GrubbsBartko2.jpeg}
		\caption{Bartko's Ellipse For Grubbs' Data, with an extra covariate.}\label{GrubbsBartko2}
	\end{figure}
	
	In the Bland-Altman plot, the horizontal displacement of any point on the plot is supported by two independent measurements. Any point should not be considered an outlier on the basis of a noticeable horizontal displacement from the main cluster, as in the case with the extra co-variate. Conversely, the fourth point, from the original data set, should be considered an
	outlier, as it has a noticeable vertical displacement from the rest of the observations.
	\newpage
	
	
	\subsubsection{Survival-Agreement Plot}
	A graphical technique for method comparison studies, that is entirely different to the Bland-Altman plot, was proposed by \citet{luiz}. This approach, known as the survival-agreement plot, is used to determine the degree of agreement using the Kaplan-Meier method, a well known graphical technique in the area of Survival Analysis. Furthermore \citet{luiz} propose that commonly used survival analysis techniques should complement this method,\textit{ providing a new analytical insight
		for agreement}. Two survival?agreement plots are used to detect the bias between to measurements of the same variable. The presence of inter-method bias is tested with the log-rank test, and its magnitude with Cox regression.
	
	%% TOLERANCE - REWRITE THIS
	
	The degree of agreement (or disagreement) of a measure is expressed as a function of several limits of tolerance, using the Kaplan-Meier method, where the failures occur exactly at absolute values of the differences between the two methods of measurement. 
	
	According to Luiz et al, the survival-agreement plot is a step function of a typical survival analysis without censored data, where the Y axis represents the proportion of discordant cases. This is equivalent to a step function where the X axis represents the absolute  observed differences and the Y axis is the proportion of the cases with at least the observed 
	difference ($x_i$). 
	
	% % PREVALENCE
	% % Implementation
	
	
	%============================================================================================================ %
	
	
	
	
	% MCS Mountain Plot Notebook
	
	\subsubsection{Mountain Plot} Krouwer and Monti have proposed a folded empirical cumulative distribution plot, otherwise known as a Mountain plot.
	
	They argue that it is suitable for detecting large, infrequent errors. This is a non-parametric method that can be used as a complement with the Bland Altman plot.  Mountain plots are created by computing a percentile
	for each ranked difference between a new method and a reference method. (Folded plots are so called because of the following transformation is performed for all percentiles above 50: percentile = 100 - percentile.) These percentiles are then plotted against the differences between the two methods.
	
	Krouwer and Monti argue that the mountain plot offers some following advantages. It is easier to find the central $95\%$ of the data, even when the data are not normally distributed. Also, comparison on different distributions can be performed with ease.



\subsection{Replicate Measurements}

Thus far, the formulation for comparison of two measurement
methods is based on one measurement by each method per subject. Should there be two or more measurements by each
method, these measurements are known as `replicate measurements'.
\citet{BXC2008} recommends the use of replicate measurements, but
acknowledges the additional computational complexity.

\citet*{BA86} address this situation via two different
approaches. The premise of the first approach is that replicate
measurements can be treated as independent measurements. The
second approach is based upon using the mean of the each group of
replicates as one single representative value. 

%\subsubsection{Mean of Replicates Limits of Agreement}

Although either approach may be used to estimate the inter-method bias, removal of the effects of replicate
measurements error leads to the underestimation of the
standard deviation of the differences.
\citet*{BA86} propose a correction for this.
% % STATE WHAT THIS CORRECTION IS

\citet{BXC2008} take issue with the limits of agreement based on
mean values of replicate measurements, since these must be interpreted as prediction
limits for the difference between means of repeated measurements by
both methods, rather than the difference of individual measurements.
\citet{BXC2008} demonstrates how the limits of agreement
calculated using the mean of replicates are `much too narrow as
prediction limits for differences between future single
measurements'. This paper also comments that, while treating the
replicate measurements as independent will cause a downward bias
on the limits of agreement calculation, this method is preferable
to the `mean of replicates' approach.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Formal Models and Tests}
While the Bland-Altman plot is useful for inspection of data, \citet{Kinsella} notes the lack of formal testing offered by
this methodology. Furthermore, \citet{Kinsella} formulates a model for
single measurement observations as a
linear mixed effects model, i.e. a model that additively combines
fixed effects and random effects:
\[
Y_{ij} =\quad \mu + \beta_{j} + u_{i} + \epsilon_{ij} \qquad i = 1,\dots,n
\qquad j=1,2\]

The true value of the measurement is represented by $\mu$ while the fixed effect due to method $j$ is $\beta_{j}$.
For simplicity these terms can be combined into single terms; $\mu_{1} = \mu+ \beta_{1}$ and $\mu_{2} = \mu + \beta_{2}$. The inter-method bias is the difference of the two fixed effect terms, $\beta_{1}-\beta_{2}$. Each individual is assumed to give rise to a random error, represented by $u_{i}$. This random effects term is assumed to have mean zero and be normally distributed with variance $\sigma^2$. There is assumed to be an attendant error for each measurement on each individual, denoted $\epsilon_{ij}$. This is also assumed to have mean zero. The variance of measurement error for both methods are not assumed to be identical for both methods variance,  hence it is denoted $\sigma^2_{j}$. The set of observations ($x_{i},y_{i}$) by methods $X$ and $Y$ are assumed to follow a bivariate normal distribution with expected values $E(x_{i})= \mu_{i}$ and $E(y_{i})= \tau_{i}$ respectively. The variance covariance of the observations $\boldsymbol{\Sigma}$ is given by

\[
\boldsymbol{\Sigma} = \left[
                        \begin{array}{cc}
                          \sigma^{2} + \sigma^{2}_{1} & \sigma^{2} \\
                          \sigma^{2} & \sigma^{2} + \sigma^{2}_{2} \\
                        \end{array}
                      \right]
\] 
% The inter-method bias is the difference of the two fixed effect terms, $\beta_{1}-\beta_{2}$.

\citet{Kinsella} demonstrates the estimation of the variance terms and relative precisions relevant to a method comparison study, with attendant confidence intervals for both. The measurement model introduced by \citet{Grubbs48,Grubbs73} provides a formal procedure for estimating the variances $\sigma^2$, $\sigma^2_{1}$ and $\sigma^2_{2}$. \citet{Grubbs48} offers estimates, commonly known as Grubbs estimators, for the various variance components. These estimates are maximum likelihood estimates, which shall be revisited in due course.
\begin{eqnarray*}
\hat{\sigma^{2}} = \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}} = Sxy\\
\hat{\sigma^{2}_{1}} = \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} =S^{2}x - Sxy  \\
\hat{\sigma^{2}_{2}} =
\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}} = S^{2}y - Sxy
\end{eqnarray*}

% The standard error of these variance estimates are:
% \begin{eqnarray}
% \mbox{var}(\sigma^{2}_{1}) = \frac{2\sigma^{4}_{1}}{n-1} +
% \frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
% }{n-1}\\
% \mbox{var}(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +
% \frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
% }{n-1}\nonumber
% \end{eqnarray}

\citet{Thompson} defines $\Delta_j = \sigma^2 / \sigma^2_j, j=1,2$, to be a measure of the
relative precision of the measurement methods, and demonstrates how to make statistical inferences about $\Delta_{j}$.
Based on the following identities,
\begin{eqnarray*}
C_{x}&=&(n-1)S^2_{x},\nonumber\\
C_{xy}&=&(n-1)S_{xy},\nonumber\\
C_{y}&=&(n-1)S^2_{y},\nonumber\\
|A| &=& C_{x}\times C_{y} - (C_{xy})^2,\nonumber
\end{eqnarray*}
\noindent the confidence interval limits of $\Delta_{1}$ are

\begin{eqnarray}
\frac{C_{xy}-
t(\frac{|A|}{n-2}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
t(\frac{|A|}{n-2}))^{\frac{1}{2}}} <
\Delta_{1} < \frac{C_{xy}+
t(\frac{|A|}{n-2}))^{\frac{1}{2}}}{C_{x}-C_{xy}-
t(\frac{|A|}{n-1}))^{\frac{1}{2}}} \nonumber
\end{eqnarray}
\\ The value $t$ is the $100(1-\alpha/2)\%$ upper quantile of
Student's $t$ distribution with $n-2$ degrees of freedom
\citep{Kinsella}. The confidence limits for $\Delta_{2}$ are found by substituting $C_{y}$ for $C_{x}$ in (1.2).
Negative lower limits are replaced by the value $0$.

%For the interval estimates for the variance components,
%\citet{Thompson} presents three relations that hold simultaneously
%with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.

%\begin{eqnarray*}
%|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
%|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
%|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
%M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
%\end{eqnarray*}

%\citet{Thompson} contains tables for $K$ and $M$.

The case-wise differences and means are calculated as $d_{i} =
x_{i}-y_{i}$ and $a_{i} = (x_{i}+y_{i})/2$  respectively. Both
$d_{i}$ and $a_{i}$ are assumed to follow a bivariate normal
distribution with $E(d_{i})= \mu_{d} = \mu_{1} - \mu_{2}$ and
$E(a_{i})= \mu_{a} = (\mu_{1} + \mu_{2})/2$, and the variance matrix
$\Sigma_{(a,d)}$ is

\begin{eqnarray}
\Sigma_{(a,d)}= \left[\begin{matrix}
\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}+
\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
\end{matrix} \right].
\end{eqnarray}

\newpage
	\section{Measurement Error Models}
	\textbf{DunnSEME} proposes a measurement error model for use in
	method comparison studies. Consider n pairs of measurements
	$X_{i}$ and $Y_{i}$ for $i=1,2,...n$.
	\begin{equation}
	X_{i} = \tau_{i}+\delta_{i}\\
	\end{equation}
	\begin{equation}
	Y_{i} = \alpha +\beta\tau_{i}+\epsilon_{i} \nonumber
	\end{equation}
	
	In the above formulation is in the form of a linear structural
	relationship, with $\tau_{i}$ and $\beta\tau_{i}$ as the true
	values , and $\delta_{i}$ and $\epsilon_{i}$ as the corresponding
	measurement errors. In the case where the units of measurement are
	the same, then $\beta =1$.
	
	\begin{equation}
	E(X_{i}) = \tau_{i}\\
	\end{equation}
	\begin{equation}
	E(Y_{i}) = \alpha +\beta\tau_{i} \nonumber
	\end{equation}
	\begin{equation}
	E(\delta_{i}) = E(\epsilon_{i}) = 0 \nonumber
	\end{equation}
	
	The value $\alpha$ is the inter-method bias between the two
	methods.
	
	\begin{eqnarray}
	z_0 &=& d = 0 \\
	z_{n+1} &=& z_n^2+c
	\end{eqnarray}
	
	\section{Model Formulation and Formal Testing}
	
	\citet{Kinsella} formulates a model for un-replicated observations
	for a method comparison study as a mixed model.
	\begin{eqnarray}
	Y_{ij} =\quad \mu_{j} + S_{i} + \epsilon_{ij} \quad i=1,2...n\quad
	j=1,2\\
	S \sim N(0,\sigma^{2}_{s})\qquad \epsilon_{ij} \sim
	N(0,\sigma^{2}_{j}) \nonumber
	\end{eqnarray}
	
	As with all mixed models, the variance of each observation is the
	sum of all the associated variance components.
	\begin{eqnarray}
	var(Y_{ij}) =\quad \sigma^{2}_{s} + \sigma^{2}_{j} \\
	cov(Y_{i1},Y_{i2})=\quad \sigma^{2}_{s} \nonumber
	\end{eqnarray}
	
	\citet{Grubbs48} offers maximum likelihood estimators, commonly
	known as Grubbs estimators, for the various variance components:
	\begin{eqnarray}
	\hat{\sigma^{2}_{s}} \quad= \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}}\quad=Sxy\\
	\hat{\sigma^{2}_{1}} \quad= \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} \quad=S^{2}x-Sxy \nonumber\\
	\hat{\sigma^{2}_{2}} \quad=
	\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}}\quad=S^{2}y-Sxy \nonumber
	\nonumber
	\end{eqnarray}
	
	The standard error of these variance estimates are:
	\begin{eqnarray}
	var(\sigma^{2}_{1}) =\quad \frac{2\sigma^{4}_{1}}{n-1} +\quad
	\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	}{n-1}\\
	var(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +\quad
	\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	}{n-1}\nonumber
	\end{eqnarray}
	
	\citet{Thompson}presents confidence intervals for the relative
	precisions of the measurement methods, $\Delta_{j}=
	\sigma^2_{S}/\sigma^2_{j}$ (where $j=1,2$), as well as the
	variances $\sigma^{2}_{S}, \sigma^{2}_{1}$ and $\sigma^{2}_{2}$.
	
	\begin{eqnarray}
	\Delta_{1} >\quad \frac{C_{xy}-
		t(|A|/n-2))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(|A|/n-2))^{\frac{1}{2}}}
	\end{eqnarray}
	where
	
	\begin{eqnarray}
	C_{x}=\quad(n-1)S^2_{x}\nonumber\\
	C_{xy}=\quad(n-1)S_{xy}\nonumber\\
	C_{y}=\quad(n-1)S^2_{y}\nonumber\\
	A=\quad C_{x}\times C_{y} - (C_{xy})^2 \nonumber
	\end{eqnarray}
	
	$t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
	distribution with $n-2$ degrees of freedom. $\Delta_{2}$ can be
	found by changing $C_{y}$ for $C_{x}$. A lower confidence limit
	can be found by calculating the square root. This inequality may
	also be used for hypothesis testing.
	
	For the interval estimates for the variance components,
	\citet{Thompson} presents three relations that hold simultaneously
	with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	\begin{eqnarray}
	|\sigma^2-C_{xy}K|\leqslant M(C_{x}C_{y})^{\frac{1}{2}}\\
	|\sigma^2_{1}-(C_{x}-C_{xy})K|\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
	|\sigma^2_{2}-(C_{y}-C_{xy})K|\leqslant
	M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	\end{eqnarray}
	
	The case-wise differences and means are $D_{i} = Y_{i1}-Y_{i2}$
	and $A_{i} = (Y_{i1}+Y_{i2})/2$  respectively. Both $D_{i}$ and
	$A_{i}$ follow a bivariate normal distribution with $E(D_{i})=
	\mu_{D} = \mu_{1} - \mu_{2}$ and $E(A_{i})= \mu_{A} = (\mu_{1} +
	\mu_{2})/2$. The variance matrix $\Sigma$ is
	
	\begin{equation}
	\Sigma = \left[\begin{matrix}
	\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
	\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}_{S}+
	\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
	\end{matrix} \right]
	\end{equation}
	
	
	
	
	
	\citet{Kinsella} demonstrates how the Grubbs estimators for the
	error variances can be calculated using the difference values,
	providing a worked example on a data set.
	\begin{eqnarray}
	\hat{\sigma^{2}_{1}}
	\quad=\sum{(y_{i1}-\bar{y{1}})(D_{i}-\bar{D})}\\
	\hat{\sigma^{2}_{2}} \quad=
	\sum{(y_{i2}-\bar{y_{2}})(D_{i}-\bar{D})} \nonumber
	\end{eqnarray}
	
	
	\subsection{Morgan Pitman}
	
	The test of the hypothesis that the variance of both methods are
	equal is based on the correlation value $\rho_{D,A}$ which is
	evaluated as follows;
	
	\begin{equation}
	\rho(D,A)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
	\end{equation}
	
	The correlation constant takes the value zero if, and only if, the
	two variances are equal. Therefore a test of the hypothesis $H:
	\sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the
	hypothesis $H: \rho(D,A) = 0$. The corresponds to the well-known
	$t$ test for a correlation coefficient with $n-2$ degrees of
	freedom.
	
	\citet{Bartko} describes the Morgan-Pitman test as identical to
	the test of the slope equal to zero in the regression of $Y_{i1}$
	on $Y_{12}$, adding that this result can be shown using
	straightforward algebra.
	
	\subsection{Bartko's Bradley-Blackwood Test}
	This is a regression based approach that performs a simulataneous
	test for the equivalence of means and variances of the respective
	methods.\\
	\begin{equation}
	D = (X_{1}-X_{2})
	\end{equation}
	\begin{equation}
	M = (X_{1} + X_{2}) /2
	\end{equation}
	The Bradley Blackwood Procedure fits D on M as follows:\\
	\begin{equation}
	D = \beta_{0} + \beta_{1}M
	\end{equation}
	\\Both beta values, the intercept and slope, are derived from the respective means and
	standard deviations of their respective data sets.\\
	We determine if the respective means and variances are equal if
	both beta values are simultaneously equal to zero. The Test is
	conducted using an F test, calculated from the results of a
	regression of D on M.
	\\We have identified this approach  to be examined to see if it can
	be used as a foundation for a test perform a test on means and
	variances individually.\\
	Russell et al have suggested this method be used in conjunction
	with a paired t-test , with estimates of slope and intercept.
	
	subsection{t-test}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%  Blackwood Bradley Model         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Blackwood Bradley Model} This is a regression based
	approach that performs a simultaneous test for the equivalence of
	means and variances of the respective methods.\\We have identified
	this approach  to be examined to see if it can be used as a
	foundation for a test perform a test on
	means and variances individually.\\
	\begin{equation}
	D = (X_{1}-X_{2})
	\end{equation}
	\begin{equation}
	M = (X_{1} + X_{2}) /2
	\end{equation}
	The Bradley Blackwood Procedure fits D on M as follows:\\
	\begin{equation}
	D = \beta_{0} + \beta_{1}M
	\end{equation}
	\\Both beta values, the intercept and slope, are derived from the respective means and
	standard deviations of their respective data sets.\\
	We determine if the respective means and variances are equal if
	both beta values are simultaneously equal to zero. The Test is
	conducted using an F test, calculated from the results of a
	regression of D on M.
	\\
	Russell et al have suggested this method be used in conjunction
	with a paired t-test , with estimates of slope and intercept.
	Bradley and Blackwood have developed a regression based approach
	assessing the agreement.
	\\
	The Bradley Blackwood test is a simultaneous test for bias and
	precision. They propose a regression approach which fits D on M,
	where D is the difference and average of a pair of results.
	
	
	
	\subsection{Pitman \& Morgan Test} This test assess the equality
	of population variances. Pitman's test tests for zero correlation
	between the sums and products.
	\\
	Correlation between differences and means is a test statistics for
	the null hypothesis of equal variances given bivariate normality.
	\section{Thompson 1963}
	
	
	
	\citet{Thompson} defines $\Delta_{j}$ to be a measure of the
	relative precision of the measurement methods, with $\Delta_{j}=
	\sigma^2_{S}/\sigma^2_{j}$(where $j=1,2$). Confidence intervals
	for $\Delta_{j}$ are also presented.
	
	\begin{eqnarray}
	\Delta_{1} > \frac{C_{xy}-
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}},
	\end{eqnarray}
	where
	
	\begin{eqnarray}
	C_{x}&=&(n-1)S^2_{x},\nonumber\\
	C_{xy}&=&(n-1)S_{xy},\nonumber\\
	C_{y}&=&(n-1)S^2_{y},\nonumber\\
	A &=& C_{x}\times C_{y} - (C_{xy})^2 . \nonumber
	\end{eqnarray}
	
	The value $t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
	distribution with $n-2$ degrees of freedom. The ratio $\Delta_{2}$
	can be found by interchanging $C_{y}$ and $C_{x}$. A lower
	confidence limit can be found by calculating the square root. The
	inequality in equation $1.10$ may also be used for hypothesis
	testing.
	
	For the interval estimates for the variance components,
	\citet{Thompson} presents three relations that hold simultaneously
	with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	
	\begin{eqnarray*}
		|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
		|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
		|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
		M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	\end{eqnarray*}
	
	\citet{Thompson} contains tables for $K$ and $M$.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Bartko's BB
	\citet{BB89} offers a formal simultaneous hypothesis test for the
	mean and variance of two paired data sets. Using simple linear
	regression of the differences of each pair against the sums, a
	line is fitted to the model, with estimates for intercept and
	slope ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$). The null
	hypothesis of this test is that the mean ($\mu$) and variance
	($\sigma^{2}$) of both data sets are equal if the slope and
	intercept estimates are equal to zero(i.e $\sigma^{2}_{1} =
	\sigma^{2}_{2}$ and $\mu_{1}=\mu_{2}$ if and only if $\beta_{0}=
	\beta_{1}=0$ )
	
	A test statistic is then calculated from the regression analysis
	of variance values \citep{BB89} and is distributed as `F' random
	variable. The degrees of freedom thereof are $\nu_{1}=2$ and
	$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
	is chosen for $\alpha\%$ significance with those same degrees of
	freedom. \citet{Bartko} amends this methodology for use in method
	comparison studies, using the averages of the pairs, as opposed to
	the sums, and their differences. This approach can facilitate
	simultaneous usage of test with the Bland-Altman methodology.
	Bartko's test statistic take the form:
	\begin{equation} F.test = \frac{(\Sigma d^{2})-SSReg}{2MSReg}
	\end{equation}
	% latex table generated in R 2.6.0 by xtable 1.5-5 package
	% Mon Aug 31 15:53:51 2009
	\begin{table}[ht]
		\begin{center}
			\begin{tabular}{lrrrrr}
				\hline
				& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
				\hline
				Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
				Residuals & 10 & 0.60 & 0.06 &  &  \\
				\hline
			\end{tabular}
			\caption{Regression ANOVA of case-wise differences and averages
				for Grubbs Data}
		\end{center}
	\end{table}
	%(calculate using R code $qf(0.95,2,10)$).
	
	For the Grubbs data, $\Sigma d^{2}=5.09 $, $SSReg = 0.60$ and
	$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
	critical value of $4.10$. Hence the means and variance of the
	Fotobalk and Counter chronometers are assumed to be simultaneously
	equal.
	
	Importantly, this methodology determines whether there is both
	inter-method bias and precision present, or alternatively if there
	is neither present. It has previously been demonstrated that there
	is a inter-method bias present, but as this procedure does not
	allow for separate testing, no conclusion can be drawn on the
	comparative precision of both methods.
	
	\subsection{Formal Testing}
	The Bland Altman plot is a simple tool for inspection of the data,
	but in itself it offers no formal testing procedure in this
	regard. To this end, the approach proposed by \citet{BA83} is a
	formal test on the Pearson correlation coefficient  of casewise
	differences and means ($\rho_{AD}$). According to the authors,
	this test is equivalent to a well established tests for equality
	of variances, known as the `Pitman Morgan Test' \citep{Pitman,
		Morgan}.
	
	For the Grubbs data, the correlation coefficient estimate
	($r_{AD}$) is 0.2625, with a 95\% confidence interval of (-0.366,
	0.726) estimated by Fishers 'r to z' transformation \citep{Cohen}.
	The null hypothesis ($\rho_{AD}$ =0) would fail to be rejected.
	Consequently the null hypothesis of equal variances of each method
	would also fail to be rejected.
	
	There has no been no further mention of this particular test in
	the subsequent article published by Bland and Altman, although
	\citet{BA99} refers to Spearmans' rank correlation coefficient.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	
	\section{Bartko's Regression and Ellipse}
	\citet{BB89} offers a formal simultaneous hypothesis test for the
	mean and variance of two paired data sets. Using simple linear
	regression of the differences of each pair against the sums, a
	line is fitted to the model, with estimates for intercept and
	slope ($\beta_{0}$ and $\beta_{1}$). The null hypothesis of this
	test is that the mean ($\mu$) and variance ($\sigma^{2}$) of both
	data sets are equal if the slope and intercept estimates are equal
	to zero(i.e $\sigma^{2}_{1} = \sigma^{2}_{2}$ and
	$\mu_{1}=\mu_{2}$ if and only if $\beta_{0}= \beta_{1}=0$ )
	
	A test statistic is then calculated from the regression analysis
	of variance values \citep{BB89} and is distributed as `F' random
	variable. The degrees of freedom thereof are $\nu_{1}=2$ and
	$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
	is chosen for $\alpha\%$ significance with those same degrees of
	freedom. \citet{Bartko} amends this metholodogy for calculation
	using the from the averages of the pairs, as opposed to the sums,
	and their differences. This would facilitate simultaneous usage of
	test with the Bland Altman methodology. Bartko's test statistic
	take the form:
	\begin{equation} F.test = \frac{(\Sigma D^{2})-SSReg}{2MSReg}
	\end{equation}
	
	\newpage
	
	% latex table generated in R 2.6.0 by xtable 1.5-5 package
	% Mon Aug 31 15:53:51 2009
	\begin{table}[ht]
		\begin{center}
			\begin{tabular}{lrrrrr}
				\hline
				& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
				\hline
				Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
				Residuals & 10 & 0.60 & 0.06 &  &  \\
				\hline
			\end{tabular}
			\caption{Regression ANOVA of case-wise differences and averages
				for Grubbs Data}
		\end{center}
	\end{table}
	
	
	
	
	For the Grubbs data, $\Sigma D^{2}=5.09 $, $SSReg = 0.60$ and
	$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
	critical value of $4.102821$ (calculate using r code
	$qf(0.95,2,10)$). Hence the means and variance of the Fotobalk and
	Counter chronometers are assumed to be simultaneously equal.
	
	Importantly, this methodology determines whether there is both
	inter-method bias and precision present, or alternatively if there
	is neither present. It has previously been demonstrated that there
	is a inter-method bias present, but as this procedure does not
	allow for seperate testing, no conclusion can be drawn on the
	comparative precision of both methods.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	\section{Formal Models and Tests}
	The Bland-Altman plot is a simple tool for inspection of data, and
	\citet{Kinsella} comments on the lack of formal testing offered by
	that methodology. \citet{Kinsella} formulates a model for
	single measurement observations for a method comparison study as a
	linear mixed effects model, i.e. model that additively combine
	fixed effects and random effects.
	\[
	Y_{ij} =\quad \mu + \beta_{j} + u_{i} + \epsilon_{ij} \qquad i = 1,\dots,n
	\qquad j=1,2\]
	
	The true value of the measurement is represented by $\mu$ while the fixed effect due to method $j$ is $\beta_{j}$.
	For simplicity these terms can be combined into single terms; $\mu_{1} = \mu+ \beta_{1}$ and $\mu_{2} = \mu + \beta_{2}$. The inter-method bias is the difference of the two fixed effect terms, $\beta_{1}-\beta_{2}$. Each of the $i$ individuals are assumed to give rise to random error, represented by $u_{i}$. This random effects terms is assumed to have mean zero and be normally distributed with variance $\sigma^2$. There is assumed to be an attendant error for each measurement on each individual, denoted $\epsilon_{ij}$. This is also assumed to have mean zero. The variance of measurement error for both methods are not assumed to be identical for both methods variance,  hence it is denoted $\sigma^2_{j}$. The set of observations ($x_{i},y_{i}$) by methods $X$ and $Y$ are assumed to follow the bivariate normal distribution with expected values $E(x_{i})= \mu_{i}$ and $E(x_{i})= \mu_{i}$ respectively. The variance covariance of the observations $\boldsymbol{\Sigma}$ is given by
	
	\[
	\boldsymbol{\Sigma} = \left[
	\begin{array}{cc}
	\sigma^{2} + \sigma^{2}_{1} & \sigma^{2} \\
	\sigma^{2} & \sigma^{2} + \sigma^{2}_{2} \\
	\end{array}
	\right]
	\]
	
	The inter-method bias is the difference of the two fixed effect terms, $\beta_{1}-\beta_{2}$.
	
	\citet{Kinsella} demonstrates the estimation of the variance terms and relative precisions relevant to a method comparison study, with attendant confidence intervals for both. The measurement model introduced by \citet{Grubbs48,Grubbs73} provides a formal procedure for estimate the variances $\sigma^2$,$\sigma^2_{1}$ and $\sigma^2_{2}$ devices. \citet{Grubbs48} offers estimates, commonly known as Grubbs estimators, for the various variance components. These estimates are maximum likelihood estimates, a statistical concept that shall be revisited in due course.
	\begin{eqnarray*}
		\hat{\sigma^{2}} = \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}} = Sxy\\
		\hat{\sigma^{2}_{1}} = \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} =S^{2}x - Sxy  \\
		\hat{\sigma^{2}_{2}} =
		\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}} = S^{2}y - Sxy
	\end{eqnarray*}
	
	% The standard error of these variance estimates are:
	% \begin{eqnarray}
	% \mbox{var}(\sigma^{2}_{1}) = \frac{2\sigma^{4}_{1}}{n-1} +
	% \frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	% }{n-1}\\
	% \mbox{var}(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +
	% \frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	% }{n-1}\nonumber
	% \end{eqnarray}
	
	\citet{Thompson} defines $\Delta_{j}$ to be a measure of the
	relative precision of the measurement methods, with $\Delta_{j}=
	\sigma^2/\sigma^2_{j}$. Thompson also demonstrates how to make statistical inferences about $\Delta_{j}$.
	Based on the following identities,
	\begin{eqnarray*}
		C_{x}&=&(n-1)S^2_{x},\nonumber\\
		C_{xy}&=&(n-1)S_{xy},\nonumber\\
		C_{y}&=&(n-1)S^2_{y},\nonumber\\
		|A| &=& C_{x}\times C_{y} - (C_{xy})^2,\nonumber
	\end{eqnarray*}
	\noindent the confidence interval limits of $\Delta_{1}$ are
	
	\begin{eqnarray}
	\Delta_{1} > \frac{C_{xy}-
		t(\frac{|A|}{n-2}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(\frac{|A|}{n-2}))^{\frac{1}{2}}} \\
	\Delta_{1} > \frac{C_{xy}+
		t(\frac{|A|}{n-2}))^{\frac{1}{2}}}{C_{x}-C_{xy}-
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}} \nonumber
	\end{eqnarray}
	\\ The value $t$ is the $100(1-\alpha/2)\%$ upper quantile of
	Student's $t$ distribution with $n-2$ degrees of freedom
	\citep{Kinsella}. The confidence limits for $\Delta_{2}$ are found by substituting $C_{y}$ for $C_{x}$ in (1.3).
	Negative lower limits are replaced by the value $0$.
	
	%For the interval estimates for the variance components,
	%\citet{Thompson} presents three relations that hold simultaneously
	%with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	%\begin{eqnarray*}
	%|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
	%|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
	%|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
	%M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	%\end{eqnarray*}
	
	%\citet{Thompson} contains tables for $K$ and $M$.
	
	The case-wise differences and means are calculated as $d_{i} =
	x_{i}-y_{i}$ and $a_{i} = (x_{i}+y_{i})/2$  respectively. Both
	$d_{i}$ and $a_{i}$ are assumed to follow a bivariate normal
	distribution with $E(d_{i})= \mu_{d} = \mu_{1} - \mu_{2}$ and
	$E(a_{i})= \mu_{a} = (\mu_{1} + \mu_{2})/2$. The variance matrix
	$\Sigma_{(a,d)}$ is
	
	\begin{eqnarray}
	\Sigma_{(a,d)}= \left[\begin{matrix}
	\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
	\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}+
	\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
	\end{matrix} \right].
	\end{eqnarray}
	
	
	
	\subsection{Morgan-Pitman Testing}
	An early contribution to formal testing in method comparison was
	made by both \citet{morgan} and \citet{pitman}, in separate
	contributions. The basis of this approach is that if the
	distribution of the original measurements is bivariate normal.
	Morgan and Pitman noted that the correlation coefficient depends
	upon the difference $\sigma^{2}_{1}- \sigma^{2}_{2}$, being zero
	if and only if $\sigma^{2}_{1}=\sigma^{2}_{2}$.
	
	The classical Pitman-Morgan test is a hypothesis test for equality
	of the variance of two data sets; $\sigma^{2}_{1} =
	\sigma^{2}_{2}$, based on the correlation value $\rho_{a,d}$ ,and
	is evaluated as follows;
	
	\begin{equation}
	\rho(a,d)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
	\end{equation}
	
	The correlation constant takes the value zero if, and only if, the two variances are equal. Therefore a test of the hypothesis $H: \sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the hypothesis $H: \rho(D,A) = 0$. The corresponds to the well-known
	$t$ test for a correlation coefficient with $n-2$ degrees of freedom. \citet{Bartko} describes the Morgan-Pitman test as identical to
	the test of the slope equal to zero in the regression of $Y_{i1}$ on $Y_{12}$, a result that can be derived using
	straightforward algebra.
	
	
	
	\section{Model Formulation and Formal Testing}
	
	\citet{Kinsella} formulates a model for un-replicated observations
	for a method comparison study as a mixed model.
	\begin{eqnarray}
	Y_{ij} =\quad \mu_{j} + S_{i} + \epsilon_{ij} \quad i=1,2...n\quad
	j=1,2\\
	S \sim N(0,\sigma^{2}_{s})\qquad \epsilon_{ij} \sim
	N(0,\sigma^{2}_{j}) \nonumber
	\end{eqnarray}
	
	As with all mixed models, the variance of each observation is the
	sum of all the associated variance components.
	\begin{eqnarray}
	var(Y_{ij}) =\quad \sigma^{2}_{s} + \sigma^{2}_{j} \\
	cov(Y_{i1},Y_{i2})=\quad \sigma^{2}_{s} \nonumber
	\end{eqnarray}
	
	\citet{Grubbs48} offers maximum likelihood estimators, commonly
	known as Grubbs estimators, for the various variance components:
	\begin{eqnarray}
	\hat{\sigma^{2}_{s}} \quad= \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}}\quad=Sxy\\
	\hat{\sigma^{2}_{1}} \quad= \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} \quad=S^{2}x-Sxy \nonumber\\
	\hat{\sigma^{2}_{2}} \quad=
	\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}}\quad=S^{2}y-Sxy \nonumber
	\nonumber
	\end{eqnarray}
	
	The standard error of these variance estimates are:
	\begin{eqnarray}
	var(\sigma^{2}_{1}) =\quad \frac{2\sigma^{4}_{1}}{n-1} +\quad
	\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	}{n-1}\\
	var(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +\quad
	\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	}{n-1}\nonumber
	\end{eqnarray}
	
	\citet{Thompson}presents confidence intervals for the relative
	precisions of the measurement methods, $\Delta_{j}=
	\sigma^2_{S}/\sigma^2_{j}$ (where $j=1,2$), as well as the
	variances $\sigma^{2}_{S}, \sigma^{2}_{1}$ and $\sigma^{2}_{2}$.
	
	\begin{eqnarray}
	\Delta_{1} >\quad \frac{C_{xy}-
		t(|A|/n-2))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(|A|/n-2))^{\frac{1}{2}}}
	\end{eqnarray}
	where
	
	\begin{eqnarray}
	C_{x}=\quad(n-1)S^2_{x}\nonumber\\
	C_{xy}=\quad(n-1)S_{xy}\nonumber\\
	C_{y}=\quad(n-1)S^2_{y}\nonumber\\
	A=\quad C_{x}\times C_{y} - (C_{xy})^2 \nonumber
	\end{eqnarray}
	
	$t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
	distribution with $n-2$ degrees of freedom. $\Delta_{2}$ can be
	found by changing $C_{y}$ for $C_{x}$. A lower confidence limit
	can be found by calculating the square root. This inequality may
	also be used for hypothesis testing.
	
	For the interval estimates for the variance components,
	\citet{Thompson} presents three relations that hold simultaneously
	with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	\begin{eqnarray}
	|\sigma^2-C_{xy}K|\leqslant M(C_{x}C_{y})^{\frac{1}{2}}\\
	|\sigma^2_{1}-(C_{x}-C_{xy})K|\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
	|\sigma^2_{2}-(C_{y}-C_{xy})K|\leqslant
	M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	\end{eqnarray}
	
	The case-wise differences and means are $D_{i} = Y_{i1}-Y_{i2}$
	and $A_{i} = (Y_{i1}+Y_{i2})/2$  respectively. Both $D_{i}$ and
	$A_{i}$ follow a bivariate normal distribution with $E(D_{i})=
	\mu_{D} = \mu_{1} - \mu_{2}$ and $E(A_{i})= \mu_{A} = (\mu_{1} +
	\mu_{2})/2$. The variance matrix $\Sigma$ is
	
	\begin{equation}
	\Sigma = \left[\begin{matrix}
	\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
	\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}_{S}+
	\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
	\end{matrix} \right]
	\end{equation}
	
	
	
	
	
	\citet{Kinsella} demonstrates how the Grubbs estimators for the
	error variances can be calculated using the difference values,
	providing a worked example on a data set.
	\begin{eqnarray}
	\hat{\sigma^{2}_{1}}
	\quad=\sum{(y_{i1}-\bar{y{1}})(D_{i}-\bar{D})}\\
	\hat{\sigma^{2}_{2}} \quad=
	\sum{(y_{i2}-\bar{y_{2}})(D_{i}-\bar{D})} \nonumber
	\end{eqnarray}
	
	
	\subsection{Morgan Pitman}
	
	The test of the hypothesis that the variance of both methods are
	equal is based on the correlation value $\rho_{D,A}$ which is
	evaluated as follows;
	
	\begin{equation}
	\rho(D,A)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
	\end{equation}
	
	The correlation constant takes the value zero if, and only if, the
	two variances are equal. Therefore a test of the hypothesis $H:
	\sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the
	hypothesis $H: \rho(D,A) = 0$. The corresponds to the well-known
	$t$ test for a correlation coefficient with $n-2$ degrees of
	freedom.
	
	\citet{Bartko} describes the Morgan-Pitman test as identical to
	the test of the slope equal to zero in the regression of $Y_{i1}$
	on $Y_{12}$, adding that this result can be shown using
	straightforward algebra.
	
	\subsection{Morgan Pitman}
	
	The test of the hypothesis that the variance of both methods are
	equal is based on the correlation value $\rho_{D,A}$ which is
	evaluated as follows;
	
	\begin{equation}
	\rho(D,A)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
	\end{equation}
	
	The correlation constant takes the value zero if, and only if, the
	two variances are equal. Therefore a test of the hypothesis $H:
	\sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the
	hypothesis $H: \rho(D,A) = 0$. The corresponds to the well-known
	$t$ test for a correlation coefficient with $n-2$ degrees of
	freedom.
	
	\citet{Bartko} describes the Morgan-Pitman test as identical to
	the test of the slope equal to zero in the regression of $Y_{i1}$
	on $Y_{12}$, adding that this result can be shown using
	straightforward algebra.
	
	
	
	\section{Model Formulation and Formal Testing}
	
	\citet{Kinsella} formulates a model for un-replicated observations
	for a method comparison study as a mixed model.
	\begin{eqnarray}
	Y_{ij} =\quad \mu_{j} + S_{i} + \epsilon_{ij} \quad i=1,2...n\quad
	j=1,2\\
	S \sim N(0,\sigma^{2}_{s})\qquad \epsilon_{ij} \sim
	N(0,\sigma^{2}_{j}) \nonumber
	\end{eqnarray}
	
	As with all mixed models, the variance of each observation is the
	sum of all the associated variance components.
	\begin{eqnarray}
	var(Y_{ij}) =\quad \sigma^{2}_{s} + \sigma^{2}_{j} \\
	cov(Y_{i1},Y_{i2})=\quad \sigma^{2}_{s} \nonumber
	\end{eqnarray}
	
	\citet{Grubbs48} offers maximum likelihood estimators, commonly
	known as Grubbs estimators, for the various variance components:
	\begin{eqnarray}
	\hat{\sigma^{2}_{s}} \quad= \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}}\quad=Sxy\\
	\hat{\sigma^{2}_{1}} \quad= \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} \quad=S^{2}x-Sxy \nonumber\\
	\hat{\sigma^{2}_{2}} \quad=
	\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}}\quad=S^{2}y-Sxy \nonumber
	\nonumber
	\end{eqnarray}
	
	The standard error of these variance estimates are:
	\begin{eqnarray}
	var(\sigma^{2}_{1}) =\quad \frac{2\sigma^{4}_{1}}{n-1} +\quad
	\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	}{n-1}\\
	var(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +\quad
	\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
	}{n-1}\nonumber
	\end{eqnarray}
	
	\citet{Thompson}presents confidence intervals for the relative
	precisions of the measurement methods, $\Delta_{j}=
	\sigma^2_{S}/\sigma^2_{j}$ (where $j=1,2$), as well as the
	variances $\sigma^{2}_{S}, \sigma^{2}_{1}$ and $\sigma^{2}_{2}$.
	
	\begin{eqnarray}
	\Delta_{1} >\quad \frac{C_{xy}-
		t(|A|/n-2))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(|A|/n-2))^{\frac{1}{2}}}
	\end{eqnarray}
	where
	
	\begin{eqnarray}
	C_{x}=\quad(n-1)S^2_{x}\nonumber\\
	C_{xy}=\quad(n-1)S_{xy}\nonumber\\
	C_{y}=\quad(n-1)S^2_{y}\nonumber\\
	A=\quad C_{x}\times C_{y} - (C_{xy})^2 \nonumber
	\end{eqnarray}
	
	$t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
	distribution with $n-2$ degrees of freedom. $\Delta_{2}$ can be
	found by changing $C_{y}$ for $C_{x}$. A lower confidence limit
	can be found by calculating the square root. This inequality may
	also be used for hypothesis testing.
	
	For the interval estimates for the variance components,
	\citet{Thompson} presents three relations that hold simultaneously
	with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	\begin{eqnarray}
	|\sigma^2-C_{xy}K|\leqslant M(C_{x}C_{y})^{\frac{1}{2}}\\
	|\sigma^2_{1}-(C_{x}-C_{xy})K|\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
	|\sigma^2_{2}-(C_{y}-C_{xy})K|\leqslant
	M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	\end{eqnarray}
	
	The case-wise differences and means are $D_{i} = Y_{i1}-Y_{i2}$
	and $A_{i} = (Y_{i1}+Y_{i2})/2$  respectively. Both $D_{i}$ and
	$A_{i}$ follow a bivariate normal distribution with $E(D_{i})=
	\mu_{D} = \mu_{1} - \mu_{2}$ and $E(A_{i})= \mu_{A} = (\mu_{1} +
	\mu_{2})/2$. The variance matrix $\Sigma$ is
	
	\begin{equation}
	\Sigma = \left[\begin{matrix}
	\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
	\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}_{S}+
	\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
	\end{matrix} \right]
	\end{equation}
	
	
	
	
	
	\citet{Kinsella} demonstrates how the Grubbs estimators for the
	error variances can be calculated using the difference values,
	providing a worked example on a data set.
	\begin{eqnarray}
	\hat{\sigma^{2}_{1}}
	\quad=\sum{(y_{i1}-\bar{y{1}})(D_{i}-\bar{D})}\\
	\hat{\sigma^{2}_{2}} \quad=
	\sum{(y_{i2}-\bar{y_{2}})(D_{i}-\bar{D})} \nonumber
	\end{eqnarray}
	
	\subsection{Paired sample T-test} \citet{Bartko} discusses the use
	of the well known paired sample $t$ test to test for inter-method
	bias; $H: \mu_{D}=0$. The test statistic is distributed a $t$
	random variable with $n-1$ degrees of freedom and is calculated as
	follows;
	
	\begin{equation}
	t^{*} = \bar{D}/ \frac{S_{D}}{\sqrt{n}}
	\end{equation}
	
	where $\bar{D}$ and $S_{D}$ is the average of the differences of
	the $n$ observations.
	
	
	
	\subsection{Paired sample T-test} \citet{Bartko} discusses the use
	of the well known paired sample $t$ test to test for inter-method
	bias; $H: \mu_{D}=0$. The test statistic is distributed a $t$
	random variable with $n-1$ degrees of freedom and is calculated as
	follows;
	
	\begin{equation}
	t^{*} = \bar{D}/ \frac{S_{D}}{\sqrt{n}}
	\end{equation}
	
	where $\bar{D}$ and $S_{D}$ is the average of the differences of
	the $n$ observations.
	
	
	
	\section{Thompson 1963}
	
	
	
	\citet{Thompson} defines $\Delta_{j}$ to be a measure of the
	relative precision of the measurement methods, with $\Delta_{j}=
	\sigma^2_{S}/\sigma^2_{j}$(where $j=1,2$). Confidence intervals
	for $\Delta_{j}$ are also presented.
	
	\begin{eqnarray}
	\Delta_{1} > \frac{C_{xy}-
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}},
	\end{eqnarray}
	where
	
	\begin{eqnarray}
	C_{x}&=&(n-1)S^2_{x},\nonumber\\
	C_{xy}&=&(n-1)S_{xy},\nonumber\\
	C_{y}&=&(n-1)S^2_{y},\nonumber\\
	A &=& C_{x}\times C_{y} - (C_{xy})^2 . \nonumber
	\end{eqnarray}
	
	The value $t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
	distribution with $n-2$ degrees of freedom. The ratio $\Delta_{2}$
	can be found by interchanging $C_{y}$ and $C_{x}$. A lower
	confidence limit can be found by calculating the square root. The
	inequality in equation $1.10$ may also be used for hypothesis
	testing.
	
	For the interval estimates for the variance components,
	\citet{Thompson} presents three relations that hold simultaneously
	with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	
	\begin{eqnarray*}
		|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
		|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
		|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
		M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	\end{eqnarray*}
	
	\citet{Thompson} contains tables for $K$ and $M$.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Bartko's BB
	\citet{BB89} offers a formal simultaneous hypothesis test for the
	mean and variance of two paired data sets. Using simple linear
	regression of the differences of each pair against the sums, a
	line is fitted to the model, with estimates for intercept and
	slope ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$). The null
	hypothesis of this test is that the mean ($\mu$) and variance
	($\sigma^{2}$) of both data sets are equal if the slope and
	intercept estimates are equal to zero(i.e $\sigma^{2}_{1} =
	\sigma^{2}_{2}$ and $\mu_{1}=\mu_{2}$ if and only if $\beta_{0}=
	\beta_{1}=0$ )
	
	A test statistic is then calculated from the regression analysis
	of variance values \citep{BB89} and is distributed as `F' random
	variable. The degrees of freedom thereof are $\nu_{1}=2$ and
	$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
	is chosen for $\alpha\%$ significance with those same degrees of
	freedom. \citet{Bartko} amends this methodology for use in method
	comparison studies, using the averages of the pairs, as opposed to
	the sums, and their differences. This approach can facilitate
	simultaneous usage of test with the Bland-Altman methodology.
	Bartko's test statistic take the form:
	\begin{equation} F.test = \frac{(\Sigma d^{2})-SSReg}{2MSReg}
	\end{equation}
	% latex table generated in R 2.6.0 by xtable 1.5-5 package
	% Mon Aug 31 15:53:51 2009
	\begin{table}[ht]
		\begin{center}
			\begin{tabular}{lrrrrr}
				\hline
				& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
				\hline
				Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
				Residuals & 10 & 0.60 & 0.06 &  &  \\
				\hline
			\end{tabular}
			\caption{Regression ANOVA of case-wise differences and averages
				for Grubbs Data}
		\end{center}
	\end{table}
	%(calculate using R code $qf(0.95,2,10)$).
	
	For the Grubbs data, $\Sigma d^{2}=5.09 $, $SSReg = 0.60$ and
	$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
	critical value of $4.10$. Hence the means and variance of the
	Fotobalk and Counter chronometers are assumed to be simultaneously
	equal.
	
	Importantly, this methodology determines whether there is both
	inter-method bias and precision present, or alternatively if there
	is neither present. It has previously been demonstrated that there
	is a inter-method bias present, but as this procedure does not
	allow for separate testing, no conclusion can be drawn on the
	comparative precision of both methods.
	
	\subsection{Formal Testing}
	The Bland Altman plot is a simple tool for inspection of the data,
	but in itself it offers no formal testing procedure in this
	regard. To this end, the approach proposed by \citet{BA83} is a
	formal test on the Pearson correlation coefficient  of casewise
	differences and means ($\rho_{AD}$). According to the authors,
	this test is equivalent to a well established tests for equality
	of variances, known as the `Pitman Morgan Test' \citep{Pitman,
		Morgan}.
	
	For the Grubbs data, the correlation coefficient estimate
	($r_{AD}$) is 0.2625, with a 95\% confidence interval of (-0.366,
	0.726) estimated by Fishers 'r to z' transformation \citep{Cohen}.
	The null hypothesis ($\rho_{AD}$ =0) would fail to be rejected.
	Consequently the null hypothesis of equal variances of each method
	would also fail to be rejected.
	
	There has no been no further mention of this particular test in
	the subsequent article published by Bland and Altman, although
	\citet{BA99} refers to Spearmans' rank correlation coefficient.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	
	\section{Thompson 1963}
	
	
	
	\citet{Thompson} defines $\Delta_{j}$ to be a measure of the
	relative precision of the measurement methods, with $\Delta_{j}=
	\sigma^2_{S}/\sigma^2_{j}$(where $j=1,2$). Confidence intervals
	for $\Delta_{j}$ are also presented.
	
	\begin{eqnarray}
	\Delta_{1} > \frac{C_{xy}-
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
		t(\frac{|A|}{n-1}))^{\frac{1}{2}}},
	\end{eqnarray}
	where
	
	\begin{eqnarray}
	C_{x}&=&(n-1)S^2_{x},\nonumber\\
	C_{xy}&=&(n-1)S_{xy},\nonumber\\
	C_{y}&=&(n-1)S^2_{y},\nonumber\\
	A &=& C_{x}\times C_{y} - (C_{xy})^2 . \nonumber
	\end{eqnarray}
	
	The value $t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
	distribution with $n-2$ degrees of freedom. The ratio $\Delta_{2}$
	can be found by interchanging $C_{y}$ and $C_{x}$. A lower
	confidence limit can be found by calculating the square root. The
	inequality in equation $1.10$ may also be used for hypothesis
	testing.
	
	For the interval estimates for the variance components,
	\citet{Thompson} presents three relations that hold simultaneously
	with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.
	
	
	\begin{eqnarray*}
		|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
		|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
		|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
		M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
	\end{eqnarray*}
	
	\citet{Thompson} contains tables for $K$ and $M$.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Bartko's BB
	\citet{BB89} offers a formal simultaneous hypothesis test for the
	mean and variance of two paired data sets. Using simple linear
	regression of the differences of each pair against the sums, a
	line is fitted to the model, with estimates for intercept and
	slope ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$). The null
	hypothesis of this test is that the mean ($\mu$) and variance
	($\sigma^{2}$) of both data sets are equal if the slope and
	intercept estimates are equal to zero(i.e $\sigma^{2}_{1} =
	\sigma^{2}_{2}$ and $\mu_{1}=\mu_{2}$ if and only if $\beta_{0}=
	\beta_{1}=0$ )
	
	A test statistic is then calculated from the regression analysis
	of variance values \citep{BB89} and is distributed as `F' random
	variable. The degrees of freedom thereof are $\nu_{1}=2$ and
	$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
	is chosen for $\alpha\%$ significance with those same degrees of
	freedom. \citet{Bartko} amends this methodology for use in method
	comparison studies, using the averages of the pairs, as opposed to
	the sums, and their differences. This approach can facilitate
	simultaneous usage of test with the Bland-Altman methodology.
	Bartko's test statistic take the form:
	\begin{equation} F.test = \frac{(\Sigma d^{2})-SSReg}{2MSReg}
	\end{equation}
	% latex table generated in R 2.6.0 by xtable 1.5-5 package
	% Mon Aug 31 15:53:51 2009
	\begin{table}[ht]
		\begin{center}
			\begin{tabular}{lrrrrr}
				\hline
				& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
				\hline
				Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
				Residuals & 10 & 0.60 & 0.06 &  &  \\
				\hline
			\end{tabular}
			\caption{Regression ANOVA of case-wise differences and averages
				for Grubbs Data}
		\end{center}
	\end{table}
	%(calculate using R code $qf(0.95,2,10)$).
	
	For the Grubbs data, $\Sigma d^{2}=5.09 $, $SSReg = 0.60$ and
	$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
	critical value of $4.10$. Hence the means and variance of the
	Fotobalk and Counter chronometers are assumed to be simultaneously
	equal.
	
	Importantly, this methodology determines whether there is both
	inter-method bias and precision present, or alternatively if there
	is neither present. It has previously been demonstrated that there
	is a inter-method bias present, but as this procedure does not
	allow for separate testing, no conclusion can be drawn on the
	comparative precision of both methods.
	
	\subsection{Formal Testing}
	The Bland Altman plot is a simple tool for inspection of the data,
	but in itself it offers no formal testing procedure in this
	regard. To this end, the approach proposed by \citet{BA83} is a
	formal test on the Pearson correlation coefficient  of casewise
	differences and means ($\rho_{AD}$). According to the authors,
	this test is equivalent to a well established tests for equality
	of variances, known as the `Pitman Morgan Test' \citep{Pitman,
		Morgan}.
	
	For the Grubbs data, the correlation coefficient estimate
	($r_{AD}$) is 0.2625, with a 95\% confidence interval of (-0.366,
	0.726) estimated by Fishers 'r to z' transformation \citep{Cohen}.
	The null hypothesis ($\rho_{AD}$ =0) would fail to be rejected.
	Consequently the null hypothesis of equal variances of each method
	would also fail to be rejected.
	
	There has no been no further mention of this particular test in
	the subsequent article published by Bland and Altman, although
	\citet{BA99} refers to Spearmans' rank correlation coefficient.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage
	
	\section{Bartko's Regression and Ellipse}
	\citet{BB89} offers a formal simultaneous hypothesis test for the
	mean and variance of two paired data sets. Using simple linear
	regression of the differences of each pair against the sums, a
	line is fitted to the model, with estimates for intercept and
	slope ($\beta_{0}$ and $\beta_{1}$). The null hypothesis of this
	test is that the mean ($\mu$) and variance ($\sigma^{2}$) of both
	data sets are equal if the slope and intercept estimates are equal
	to zero(i.e $\sigma^{2}_{1} = \sigma^{2}_{2}$ and
	$\mu_{1}=\mu_{2}$ if and only if $\beta_{0}= \beta_{1}=0$ )
	
	A test statistic is then calculated from the regression analysis
	of variance values \citep{BB89} and is distributed as `F' random
	variable. The degrees of freedom thereof are $\nu_{1}=2$ and
	$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
	is chosen for $\alpha\%$ significance with those same degrees of
	freedom. \citet{Bartko} amends this metholodogy for calculation
	using the from the averages of the pairs, as opposed to the sums,
	and their differences. This would facilitate simultaneous usage of
	test with the Bland Altman methodology. Bartko's test statistic
	take the form:
	\begin{equation} F.test = \frac{(\Sigma D^{2})-SSReg}{2MSReg}
	\end{equation}
	
	\newpage
	
	% latex table generated in R 2.6.0 by xtable 1.5-5 package
	% Mon Aug 31 15:53:51 2009
	\begin{table}[ht]
		\begin{center}
			\begin{tabular}{lrrrrr}
				\hline
				& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
				\hline
				Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
				Residuals & 10 & 0.60 & 0.06 &  &  \\
				\hline
			\end{tabular}
			\caption{Regression ANOVA of case-wise differences and averages
				for Grubbs Data}
		\end{center}
	\end{table}
	
	
	
	
	For the Grubbs data, $\Sigma D^{2}=5.09 $, $SSReg = 0.60$ and
	$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
	critical value of $4.102821$ (calculate using r code
	$qf(0.95,2,10)$). Hence the means and variance of the Fotobalk and
	Counter chronometers are assumed to be simultaneously equal.
	
	Importantly, this methodology determines whether there is both
	inter-method bias and precision present, or alternatively if there
	is neither present. It has previously been demonstrated that there
	is a inter-method bias present, but as this procedure does not
	allow for seperate testing, no conclusion can be drawn on the
	comparative precision of both methods.
	\newpage
	\subsection{Bartko's Ellipse}
	\citet{Bartko} offers a graphical complement to the Bland-Altman
	plot, in the form of a bivariate confidence ellipse.
	\citet{AltmanEllipse} provides the relevant calculations.
	
	\begin{figure}[h!]
		% Requires \usepackage{graphicx}
		\includegraphics[width=130mm]{images/GrubbsBartko.jpeg}
		\caption{Bartko's Ellipse For Grubbs Data}\label{GrubbsBartko}
	\end{figure}
	
	
	\subsection{Bartko's Bradley-Blackwood Test}
	This is a regression based approach that performs a simulataneous
	test for the equivalence of means and variances of the respective
	methods.\\
	\begin{equation}
	D = (X_{1}-X_{2})
	\end{equation}
	\begin{equation}
	M = (X_{1} + X_{2}) /2
	\end{equation}
	The Bradley Blackwood Procedure fits D on M as follows:\\
	\begin{equation}
	D = \beta_{0} + \beta_{1}M
	\end{equation}
	\\Both beta values, the intercept and slope, are derived from the respective means and
	standard deviations of their respective data sets.\\
	We determine if the respective means and variances are equal if
	both beta values are simultaneously equal to zero. The Test is
	conducted using an F test, calculated from the results of a
	regression of D on M.
	\\We have identified this approach  to be examined to see if it can
	be used as a foundation for a test perform a test on means and
	variances individually.\\
	Russell et al have suggested this method be used in conjunction
	with a paired t-test , with estimates of slope and intercept.
	
	subsection{t-test}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%  Blackwood Bradley Model         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Blackwood Bradley Model} This is a regression based
	approach that performs a simultaneous test for the equivalence of
	means and variances of the respective methods.\\We have identified
	this approach  to be examined to see if it can be used as a
	foundation for a test perform a test on
	means and variances individually.\\
	\begin{equation}
	D = (X_{1}-X_{2})
	\end{equation}
	\begin{equation}
	M = (X_{1} + X_{2}) /2
	\end{equation}
	The Bradley Blackwood Procedure fits D on M as follows:\\
	\begin{equation}
	D = \beta_{0} + \beta_{1}M
	\end{equation}
	\\Both beta values, the intercept and slope, are derived from the respective means and
	standard deviations of their respective data sets.\\
	We determine if the respective means and variances are equal if
	both beta values are simultaneously equal to zero. The Test is
	conducted using an F test, calculated from the results of a
	regression of D on M.
	\\
	Russell et al have suggested this method be used in conjunction
	with a paired t-test , with estimates of slope and intercept.
	Bradley and Blackwood have developed a regression based approach
	assessing the agreement.
	\\
	The Bradley Blackwood test is a simultaneous test for bias and
	precision. They propose a regression approach which fits D on M,
	where D is the difference and average of a pair of results.
	
	
	
	\subsection{Pitman \& Morgan Test} This test assess the equality
	of population variances. Pitman's test tests for zero correlation
	between the sums and products.
	\\
	Correlation between differences and means is a test statistics for
	the null hypothesis of equal variances given bivariate normality.
	
	\section{Bartko's Regression and Ellipse}
	\citet{BB89} offers a formal simultaneous hypothesis test for the
	mean and variance of two paired data sets. Using simple linear
	regression of the differences of each pair against the sums, a
	line is fitted to the model, with estimates for intercept and
	slope ($\beta_{0}$ and $\beta_{1}$). The null hypothesis of this
	test is that the mean ($\mu$) and variance ($\sigma^{2}$) of both
	data sets are equal if the slope and intercept estimates are equal
	to zero(i.e $\sigma^{2}_{1} = \sigma^{2}_{2}$ and
	$\mu_{1}=\mu_{2}$ if and only if $\beta_{0}= \beta_{1}=0$ )
	
	A test statistic is then calculated from the regression analysis
	of variance values \citep{BB89} and is distributed as `F' random
	variable. The degrees of freedom thereof are $\nu_{1}=2$ and
	$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
	is chosen for $\alpha\%$ significance with those same degrees of
	freedom. \citet{Bartko} amends this metholodogy for calculation
	using the from the averages of the pairs, as opposed to the sums,
	and their differences. This would facilitate simultaneous usage of
	test with the Bland Altman methodology. Bartko's test statistic
	take the form:
	\begin{equation} F.test = \frac{(\Sigma D^{2})-SSReg}{2MSReg}
	\end{equation}
	
	\newpage
	
	% latex table generated in R 2.6.0 by xtable 1.5-5 package
	% Mon Aug 31 15:53:51 2009
	\begin{table}[ht]
		\begin{center}
			\begin{tabular}{lrrrrr}
				\hline
				& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
				\hline
				Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
				Residuals & 10 & 0.60 & 0.06 &  &  \\
				\hline
			\end{tabular}
			\caption{Regression ANOVA of case-wise differences and averages
				for Grubbs Data}
		\end{center}
	\end{table}
	
	
	
	
	For the Grubbs data, $\Sigma D^{2}=5.09 $, $SSReg = 0.60$ and
	$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
	critical value of $4.102821$ (calculate using r code
	$qf(0.95,2,10)$). Hence the means and variance of the Fotobalk and
	Counter chronometers are assumed to be simultaneously equal.
	
	Importantly, this methodology determines whether there is both
	inter-method bias and precision present, or alternatively if there
	is neither present. It has previously been demonstrated that there
	is a inter-method bias present, but as this procedure does not
	allow for seperate testing, no conclusion can be drawn on the
	comparative precision of both methods.
	\newpage
	\subsection{Bartko's Ellipse}
	\citet{Bartko} offers a graphical complement to the Bland-Altman
	plot, in the form of a bivariate confidence ellipse.
	\citet{AltmanEllipse} provides the relevant calculations.
	
	\begin{figure}[h!]
		% Requires \usepackage{graphicx}
		\includegraphics[width=130mm]{images/GrubbsBartko.jpeg}
		\caption{Bartko's Ellipse For Grubbs Data}\label{GrubbsBartko}
	\end{figure}
\newpage



\subsection{Morgan Pitman Testing}
An early contribution to formal testing in method comparison was
made by both \citet{morgan} and \citet{pitman}, in separate
contributions. The basis of this approach is that the
distribution of the original measurements is bivariate normal.
Morgan and Pitman noted that the correlation coefficient depends
upon the difference $\sigma^{2}_{1}- \sigma^{2}_{2}$, being zero
if and only if $\sigma^{2}_{1}=\sigma^{2}_{2}$.

The classical Pitman-Morgan test is a hypothesis test for equality
of the variance of two data sets; $\sigma^{2}_{1} =
\sigma^{2}_{2}$, based on the correlation value $\rho_{a,d}$ ,and
is evaluated as follows;

\begin{equation}
\rho(a,d)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
\end{equation}

The correlation constant takes the value zero if, and only if, the two variances are equal. Therefore a test of the hypothesis $H: \sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the hypothesis $H: \rho(D,A) = 0$. This corresponds to the well-known
$t$ test for a correlation coefficient with $n-2$ degrees of freedom. \citet{Bartko} describes the Morgan-Pitman test as identical to
the test of the slope equal to zero in the regression of $Y_{i1}$ on $Y_{12}$, a result that can be derived using
straightforward algebra.


\subsection{Paired sample \emph{t}-test}

\citet{Bartko} discusses the use of the well known paired sample
$t$ test to test for inter-method bias; $H: \mu_{d}=0$. The test
statistic is distributed a $t$ random variable with $n-1$ degrees
of freedom and is calculated as follows,

\begin{equation}
t^{*} = \frac{\bar{d}}{ \frac{s_{d}}{\sqrt{n}}}
\end{equation}

where $\bar{d}$ and $s_{d}$ is the average of the differences of
the $n$ observations. Only if the two methods show comparable
precision then the paired sample student t-test is appropriate for
assessing the magnitude of the bias.

\subsection*{Structural Equation Modelling}
Authors, such as a \citet{lewis}, \citet{dunnSEME} and \citet{voelkel2005center}, strongly advocate the use of \textit{Structural Equation Models} for the purposes of method comparison. Conversely \citet{BA99} also states that consider structural equation models to be inappropriate.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%  Blackwood Bradley Model         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Blackwood -Bradley Model} 

\citet{BB89} have developed a regression based procedure for
assessing the agreement. This approach performs a simultaneous test for the equivalence of
means and variances of the respective methods. Using simple linear
regression of the differences of each pair against the sums, a
line is fitted to the model, with estimates for intercept and
slope ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$).
%We have identified
%this approach  to be examined to see if it can be used as a %foundation for a test perform a test on
%means and variances individually.
\begin{equation}
D = (X_{1}-X_{2})
\end{equation}
\begin{equation}
M = (X_{1} + X_{2}) /2
\end{equation}
The Bradley Blackwood Procedure fits D on M as follows:\\
\begin{equation}
D = \beta_{0} + \beta_{1}M
\end{equation}
This technique offers a formal simultaneous hypothesis test for the
mean and variance of two paired data sets.  The null
hypothesis of this test is that the mean ($\mu$) and variance
($\sigma^{2}$) of both data sets are equal if the slope and
intercept estimates are equal to zero(i.e $\sigma^{2}_{1} =
\sigma^{2}_{2}$ and $\mu_{1}=\mu_{2}$ if and only if $\beta_{0}=
\beta_{1}=0$ )

A test statistic is then calculated from the regression analysis
of variance values \citep{BB89} and is distributed as `$F$' random
variable. The degrees of freedom are $\nu_{1}=2$ and $\nu_{1}=n-2$
(where $n$ is the number of pairs). The critical value is chosen
for $\alpha\%$ significance with those same degrees of freedom.
\citet{Bartko} amends this approach for use in method
comparison studies, using the averages of the pairs, as opposed to
the sums, and their differences. This approach can facilitate
simultaneous usage of test with the Bland-Altman approach.
Bartko's test statistic take the form:
\[ F.test = \frac{(\Sigma d^{2})-SSReg}{2MSReg}
\]
% latex table generated in R 2.6.0 by xtable 1.5-5 package
% Mon Aug 31 15:53:51 2009
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{lrrrrr}
			\hline
			& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
			\hline
			Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
			Residuals & 10 & 0.60 & 0.06 &  &  \\
			\hline
		\end{tabular}
		\caption{Regression ANOVA of case-wise differences and averages
			for Grubbs Data}
	\end{center}
\end{table}
%(calculate using R code $qf(0.95,2,10)$).

For the Grubbs data, $\Sigma d^{2}=5.09 $, $SSReg = 0.60$ and
$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
critical value of $4.10$. Hence the means and variance of the
Fotobalk and Counter chronometers are assumed to be simultaneously
equal.

Importantly, this approach determines whether there is both
inter-method bias and precision present, or alternatively if there
is neither present. It has previously been demonstrated that there
is a inter-method bias present, but as this procedure does not
allow for separate testing, no conclusion can be drawn on the
comparative precision of both methods.

\subsection{Bland-Altman correlation test}

The approach proposed by \citet{BA83} is a formal test on the
Pearson correlation coefficient of case-wise differences and means ($\rho_{AD}$). According to the authors, this test is equivalent
to the `Pitman Morgan Test'. For the Grubbs data, the correlation coefficient estimate ($r_{AD}$) is 0.2625, with a 95\% confidence
interval of (-0.366, 0.726) estimated by Fishers `$r$ to $z$' transformation \citep*{Cohen}. The null hypothesis ($\rho_{AD}$ =0)
fail to be rejected. Consequently the null hypothesis of equal variances of each method would also fail to be rejected. There has
no been no further mention of this particular test in \citet{BA86}, although \citet{BA99} refers to Spearman's rank
correlation coefficient. \citet{BA99} state that they ` do not see a place for methods of analysis based on hypothesis testing'.


\subsection{Identifiability}
\citet{DunnSEME} highlights an important issue regarding using models such as structural equation modelling, which is the identifiability problem. This comes as a
result of there being too many parameters to be estimated. Therefore assumptions about some parameters, or estimators used, must be made so that others can be estimated. For example, in the literature, the variance ratio $\lambda=\frac{\sigma^{2}_{1}}{\sigma^{2}_{2}}$
must often be assumed to be equal to $1$ \citep{linnet98}. \citet{DunnSEME} considers approaches based on two methods with single measurements on each subject as inadequate for a serious
study on the measurement characteristics of the methods. This is because there would not be enough data to allow for a meaningful
analysis. There is, however, a counter-argument that in many practical settings it is very difficult to get replicate observations when, for example, the measurement method requires invasive medical
procedure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Bartko's BB
\citet{BB89} offer a formal simultaneous hypothesis test for the mean and variance of paired data sets. This approach is based upon regressing the differences of each pair on the sum of each pair, a
line is fitted to the model, with estimates for intercept and
slope ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$). The null
hypothesis of this test is that the mean ($\mu$) and variance
($\sigma^{2}$) of both data sets are equal if the slope and
intercept estimates are equal to zero (i.e $\sigma^{2}_{1} =
\sigma^{2}_{2}$ and $\mu_{1}=\mu_{2}$ if and only if $\beta_{0}=
\beta_{1}=0$ )

A test statistic is then calculated from the regression analysis
of variance values \citep{BB89} and is distributed as `$F$' random
variable. The degrees of freedom are $\nu_{1}=2$ and $\nu_{2}=n-2$
(where $n$ is the number of pairs). 
\citet{Bartko} amends this approach for use in method
comparison studies, using the averages of the pairs, as opposed to
the sums, and their differences. This approach can facilitate
simultaneous usage of test with the Bland-Altman approach.
Bartko's test statistic take the form:
\[ F.test = \frac{(\Sigma d^{2})-SSReg}{2MSReg}
\]
% latex table generated in R 2.6.0 by xtable 1.5-5 package
% Mon Aug 31 15:53:51 2009
\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
  \hline
Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
  Residuals & 10 & 0.60 & 0.06 &  &  \\
   \hline
\end{tabular}
\caption{Regression ANOVA of case-wise differences and averages
for Grubbs Data}
\end{center}
\end{table}
%(calculate using R code $qf(0.95,2,10)$).

For the Grubbs data, $\Sigma d^{2}=5.09 $, $SSReg = 0.60$ and $MSreg=0.06$. Therefore the test statistic is $3.742$, with a critical value of $4.10$. Hence the means and variance of the
Fotobalk and Counter chronometers are assumed to be simultaneously equal.

Importantly, this methodology determines whether there is both inter-method bias and precision present, or alternatively if there
is neither present. It has previously been demonstrated that there is a inter-method bias present, but as this procedure does not allow for separate testing, no conclusion can be drawn on the comparative precision of both methods.



%This application of the
%Grubbs method presumes the existence of this condition, and necessitates
%replication of observations by means external to and independent of the first
%means. The Grubbs estimators method is based on the laws of propagation of
%error. By making three independent simultaneous measurements on the same
%physical material, it is possible by appropriate mathematical manipulation of
%the sums and differences of the associated variances to obtain a valid
%estimate of the precision of the primary means. Application of the Grubbs
%estimators procedure to estimation of the precision of an apparatus uses
%the results of a physical test conducted in such a way as to obtain a series
%of sets of three independent observations.

section{Bartko's Bradley-Blackwood Test}
This is a regression based
approach that performs a simultaneous test for the equivalence of
means and variances of the respective methods.We have identified
this approach  to be examined to see if it can be used as a
foundation for a test perform a test on
means and variances individually.
\begin{equation}
D = (X_{1}-X_{2})
\end{equation}
\begin{equation}
M = (X_{1} + X_{2}) /2
\end{equation}
The Bradley Blackwood Procedure fits D on M as follows:\\
\begin{equation}
D = \beta_{0} + \beta_{1}M
\end{equation}
\begin{itemize}
	\item The Bradley Blackwood test is a simultaneous test for bias and
	precision. They propose a regression approach which fits D on M,
	where D is the difference and average of a pair of results.
	\item Both beta values, the intercept and slope, are derived from the respective means and
	standard deviations of their respective data sets.
	\item We determine if the respective means and variances are equal if
	both beta values are simultaneously equal to zero. The Test is
	conducted using an F test, calculated from the results of a
	regression of D on M.
	\item We have identified this approach  to be examined to see if it can
	be used as a foundation for a test perform a test on means and
	variances individually.
	\item Russell et al have suggested this method be used in conjunction
	with a paired t-test , with estimates of slope and intercept.
\end{itemize}
%subsection{t-test}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%  Blackwood Bradley Model         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Blackwood Bradley Model} 

Bradley and Blackwood have developed a regression based approach
assessing the agreement.




\newpage
\section{Bradley-Blackwood Test (Kevin Hayes Talk)}
%--------------------------------------------------------------------------------%
% KH - UW

This work considers the problem of testing $\mu_1$ = $\mu_2$ and $\sigma^2_1 = \sigma^2_2$ using a random sample from a bivariate normal distribution with parameters $(\mu_1, \mu_2, \sigma^2_1, \sigma^2_2, \rho)$. 

The new contribution is a decomposition of the Bradley-Blackwood test statistic (\textit{Bradley and Blackwood, 1989})for the simultaneous test of {$\mu_1$ = $\mu_2$; $\sigma^2_1 = \sigma^2_2$}  as a sum of two statistics. 

One is equivalent to the Pitman-Morgan (\textit{Pitman, 1939; Morgan, 1939}) test statistic 
for $\sigma^2_1 = \sigma^2_2$ and the other one is a new alternative to the standard paired-t test of $\mu_D = \mu_1 = \mu_2 = 0$. 

Surprisingly, the classic Student paired-t test makes no assumptions about the equality (or otherwise) of the 
variance parameters. 

The power functions for these tests are quite easy to derive, and show that when $\sigma^2_1 = \sigma^2_2$, 
the paired t-test has a slight advantage over the new alternative in terms of power, but when $\sigma^2_1 \neq \sigma^2_2$, the 
new test has substantially higher power than the paired-t test.

While Bradley and Blackwood provide a test on the joint hypothesis of equal means and equal variances their regression based approach does not separate these two issues.

The rejection of the joint hypothesis may be 
due to two groups with unequal means and unequal variances; unequal means and equal variances, or equal means and unequal variances. We propose an approach for resolving this (model selection) problem in a manner controlling the magnitudes of the relevant type I error probabilities.


\section{Regression Methods for Method Comparison}
Conventional regression models are estimated using the ordinary
least squares (OLS) technique, and are referred to as `Model I
regression' \citep{CornCoch,ludbrook97}. A key feature of Model I
models is that the independent variable is assumed to be measured
without error. However this assumption invalidates simple linear
regression for use in method comparison studies, as both methods
must be assumed to be measured with error \citep{BA83,ludbrook97}.

The use of regression models that assumes the presence of error in both variables $X$ and $Y$ have been proposed for use instead
\citep{CornCoch,ludbrook97}. These methodologies are collectively known as `Model II regression'. They differ in the method used to
estimate the parameters of the regression.

Regression estimates depend on formulation of the model. A formulation with one method considered as the $X$ variable will yield different estimates for a formulation where it is the $Y$
variable. With Model I regression, the models fitted in both cases will entirely different and inconsistent. However with Model II
regression, they will be consistent and complementary.

Regression approaches are useful for a making a detailed examination of the biases across the range of measurements, allowing bias to be decomposed into fixed bias and proportional bias.
Fixed bias describes the case where one method gives values that are consistently different to the other across the whole range. Proportional
bias describes the difference in measurements getting progressively greater, or smaller, across the range of measurements. A measurement method may have either an attendant fixed bias or proportional bias, or both. \citep{ludbrook97}. Determination of these biases shall be discussed in due course.



%================================================================================================= %
\subsection{Deming Regression}

As stated previously, the fundamental flaw of simple linear regression is that it allows for measurement error in one variable only. This causes a downward biased slope estimate.

Deming regression is a regression fitting approach that assumes error in both variables. Deming regression is recommended by \citet*{CornCoch} as the
preferred Model II regression for use in method comparison studies.
The sum of squared distances from measured sets of values to the regression line is minimized at an angles specified by the ratio $\lambda$ of the residual variance of both variables. I
When $\lambda$ is one, the angle is 45 degrees. In ordinary linear regression, the distances are minimized in the vertical directions \citep{linnet99}.
In cases involving only single measurements by each method, $\lambda$ may be unknown and is therefore assumes a value of one. While this will produce biased estimates, they are less biased than ordinary linear regression.

The Bland Altman Plot is uninformative about the comparative influence of proportional bias and fixed bias. Model II approaches, such as Deming regression,  can provide independent tests for
both types of bias.

For a given $\lambda$, \citet{Kummel} derived the following estimate that would later be used for the Deming regression slope
parameter. The intercept estimate $\alpha$ is simply estimated in the same way as in conventional linear
regression, by using the identity $\bar{Y}-\hat{\beta}\bar{X}$;
\begin{equation}
\hat{\beta} =\quad \frac{S_{yy} - \lambda S_{xx}+[(S_{yy} -
\lambda S_{xx})^{2}+ 4\lambda S^{2}_{xy}]^{1/2}}{2S_{xy}}
\end{equation},
with $\lambda$ as the variance ratio. As stated previously $\lambda$ is often unknown, and therefore must be assumed to equal one. \citet{CarollRupert} states that Deming
regression is acceptable only when the precision ratio ($\lambda$,in their paper as $\eta$) is correctly specified, but in practice this is often not the case, with the $\lambda$ being underestimated. Several candidate models, with varying variance ratios may be fitted, and estimates of the slope and intercept are produced. However no model selection information is available to determine the best fitting model.

As with conventional regression methodologies, Deming regression calculates an estimate for both the slope and intercept for the
fitted line, and standard errors thereof. Therefore there is sufficient information to carry out hypothesis tests on both
estimates, that are informative about presence of fixed and proportional bias.

A $95\%$ confidence interval for the intercept estimate can be used to test the intercept, and hence fixed bias, is equal to
zero. This hypothesis is accepted if the confidence interval for the estimate contains the value $0$ in its range. Should this be,
it can be concluded that fixed bias is not present. Conversely, if the hypothesis is rejected, then it is concluded that the
intercept is non zero, and that fixed bias is present.

Testing for proportional bias is a very similar procedure. The
$95\%$ confidence interval for the slope estimate can be used to
test the hypothesis that the slope is equal to $1$. This
hypothesis is accepted if the confidence interval for the estimate
contains the value $1$ in its range. If the hypothesis is
rejected, then it is concluded that the slope is significant
different from $1$ and that a proportional bias exists.

For convenience, a new data set shall be introduced to demonstrate
Deming regression. Measurements of transmitral volumetric flow
(MF) by doppler echocardiography, and left ventricular stroke
volume (SV) by cross sectional echocardiography in 21 patients
with aortic valve disease are tabulated in \citet{zhang}. This
data set features in the discussion of method comparison studies
in \citet[p.398]{AltmanBook} .


% latex table generated in R 2.6.0 by xtable 1.5-5 package
% Tue Sep 01 13:31:17 2009
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c||c|c|c||c|c|c|}
  \hline
 Patient & MF  & SV  & Patient & MF  & SV  & Patient & MF  & SV \\
 &($cm^{3}$)&  ($cm^{3}$) & &($cm^{3}$)&  ($cm^{3}$) & &($cm^{3}$)&  ($cm^{3}$)
 \\
  \hline
1 & 47 & 43 &  8 & 75 & 72 &  15 & 90 & 82 \\
  2 & 66 & 70 & 9 & 79 & 92 &  16 & 100 & 100 \\
  3 & 68 & 72 & 10 & 81 & 76 & 17 & 104 & 94 \\
  4 & 69 & 81 & 11 & 85 & 85 &  18 & 105 & 98 \\
  5 & 70 & 60 & 12 & 87 & 82 & 19 & 112 & 108 \\
  6 & 70 & 67 & 13 & 87 & 90 & 20 & 120 & 131 \\
  7 & 73 & 72 & 14 & 87 & 96 &  21 & 132 & 131 \\

   \hline
\end{tabular}
\caption{Transmitral volumetric flow(MF) and left ventricular
stroke volume (SV) in 21 patients. (Zhang et al 1986)}
\end{center}
\end{table}


\begin{figure}[h!]
  % Requires \usepackage{graphicx}
  \includegraphics[width=130mm]{images/ZhangDeming.jpeg}
  \caption{Deming Regression For Zhang's Data}\label{ZhangDeming}
\end{figure}


\citet{CarollRupert} states that Deming's
regression is acceptable only when the precision ratio ($\lambda$,
in their paper as $\eta$) is correctly specified, but in practice
this is often not the case, with the $\lambda$ being
underestimated.
\newpage
\section{Other Types of Studies}
\citet{lewis} categorize method comparison studies into three
different types.  The key difference between the first two is
whether or not a `gold standard' method is used. In situations
where one instrument or method is known to be `accurate and
precise', it is considered as the`gold standard' \citep{lewis}. A
method that is not considered to be a gold standard is referred to
as an `approximate method'. In calibration studies they are
referred to a criterion methods and test methods respectively.


\textbf{1. Calibration problems}. The purpose is to establish a
relationship between methods, one of which is an approximate
method, the other a gold standard. The results of the approximate
method can be mapped to a known probability distribution of the
results of the gold standard \citep{lewis}. (In such studies, the
gold standard method and corresponding approximate method are
generally referred to a criterion method and test method
respectively.) \citet*{BA83} make clear that their methodology is
not intended for calibration problems.

\bigskip \textbf{2. Comparison problems}. When two approximate
methods, that use the same units of measurement, are to be
compared. This is the case which the Bland-Altman methodology is
specfically intended for, and therefore it is the most relevant of
the three.

\bigskip \textbf{3. Conversion problems}. When two approximate
methods, that use different units of measurement, are to be
compared. This situation would arise when the measurement methods
use 'different proxies', i.e different mechanisms of measurement.
\citet{lewis} deals specifically with this issue. In the context
of this study, it is the least relevant of the three.

\citet[p.47]{DunnSEME} cautions that`gold standards' should not be
assumed to be error free. `It is of necessity a subjective
decision when we come to decide that a particular method or
instrument can be treated as if it was a gold standard'. The
clinician gold standard , the sphygmomanometer, is used as an
example thereof.  The sphygmomanometer `leaves considerable room
for improvement' \citep{DunnSEME}. \citet{pizzi} similarly
addresses the issue of glod standards, `well-established gold
standard may itself be imprecise or even unreliable'.


The NIST F1 Caesium fountain atomic clock is considered to be the
gold standard when measuring time, and is the primary time and
frequency standard for the United States. The NIST F1 is accurate
to within one second per 60 million years \citep{NIST}.

Measurements of the interior of the human body are, by definition,
invasive medical procedures. The design of method must balance the
need for accuracy of measurement with the well-being of the
patient. This will inevitably lead to the measurement error as
described by \citet{DunnSEME}. The magnetic resonance angiogram,
used to measure internal anatomy,  is considered to the gold
standard for measuring aortic dissection. Medical test based upon
the angiogram is reported to have a false positive reporting rate
of 5\% and a false negative reporting rate of 8\%. This is
reported as sensitivity of 95\% and a specificity of 92\%
\citep{ACR}.

In literature they are, perhaps more accurately, referred to as
`fuzzy gold standards' \citep{phelps}. Consequently when one of the methods is
essentially a fuzzy gold standard, as opposed to a `true' gold
standard, the comparison of the criterion and test methods should
be consider in the context of a comparison study, as well as of a
calibration study.

\newpage

\section{Methods of assessing agreement}

\begin{enumerate}
	\item Pearson's Correlation Coefficient\item Intraclass
	correlation coefficient \item Bland Altman Plot \item Bartko's
	Ellipse (1994) \item Blackwood Bradley Test \item Lin's
	Reproducibility Index \item Luiz Step function
\end{enumerate}

Bland and Altman attend to the issue of repeated measures in
$1996$.
\\
Repeated measurements on several subjects can be used to quantify
measurement error, the variation between measurements of the same
quantity on the same individual.
\\
Bland and Altman discuss two metrics for measurement error; the
within-subject standard deviation ,and the correlation
coefficient.

The above plot incorporates both the conventional limits of
agreement ( the inner pair of dashed lines), the `t' limits of
agreement ( the outer pair of dashed lines) centred around the
inter-method bias (indicated by the full line). This plot is
intended for expository purposes only, as the sample size is
small.





\subsection{Equivalence and Interchangeability}
Limits of agreement are intended to analyse equivalence. How this
is assessed is the considered judgement of the practitioner. In
\citet{BA86} an example of good agreement is cited. For two
methods of measuring `oxygen saturation', the limits of agreement
are calculated as (-2.0,2.8).A practitioner would ostensibly find
this to be sufficiently narrow.

If the limits of agreement are not clinically important, which is
to say that the differences tend not to be substantial, the two
methods may be used interchangeably. \citet{DunnSEME} takes issue
with the notion of `equivalence', remarking that while agreement
indicated equivalence, equivalence does not reflect agreement.




\section{Bland Altman Plots In Literature}
\citet{mantha} contains a study the use of Bland Altman plots of
44 articles in several named journals over a two year period. 42
articles used Bland Altman's limits of agreement, wit the other
two used correlation and regression analyses. \citet{mantha}
remarks that 3 papers, from 42 mention predefined maximum width
for limits of agreement which would not impair medical care.

The conclusion of \citet{mantha} is that there are several
inadequacies and inconsistencies in the reporting of results ,and
that more standardization in the use of Bland Altman plots is
required. The authors recommend the prior determination of limits
of agreement before the study is carried out. This contention is
endorsed by \citet{lin}, which makes a similar recommendation for
the sample size, noting that\emph{sample sizes required either was
	not mentioned or no rationale for its choice was given}.

\begin{quote}
	In order to avoid the appearance of "data dredging", both the
	sample size and the (limits of agreement) should be specified and
	justified before the actual conduct of the trial. \citep{lin}
\end{quote}

\citet{Dewitte} remarks that the limits of agreement should be
compared to a clinically acceptable difference in measurements.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%4 Inappropriate assessment of Agreement       %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Gold Standard} This is considered to be the most
accurate measurement of a particular parameter.
\section{Discussion on Method Comparison Studies}

The need to compare the results of two different measurement
techniques is common in medical statistics.
\\
\\
In particular, in medicine, new methods or devices that are
cheaper, easier to use, or less invasive, are routinely developed.
Agreement between a new method and a traditional reference or gold
standard must be evaluated before the new one is put into
practice. Various methodologies have been proposed for this
purpose in recent years.

Indications on how to deal with outliers in Bland Altman plots
\\
We wish to determine how outliers should be treated in a Bland
Altman Plot
\\
In their 1983 paper they merely state that the plot can be used to
'spot outliers'.
\\
In  their 1986 paper, Bland and Altman give an example of an
outlier. They state that it could be omitted in practice, but make
no further comments on the matter.
\\
In Bland and Altmans 1999 paper, we get the clearest indication of
what Bland and Altman suggest on how to react to the presence of
outliers. Their recommendation is to recalculate the limits
without them, in order to test the difference with the calculation
where outliers are retained.\\

The span has reduced from 77 to 59 mmHg, a noticeable but not
particularly large reduction.
\\
However, they do not recommend removing outliers. Furthermore,
they say:
\\
We usually find that this method of analysis is not too sensitive
to one or two large outlying differences.
\\
We ask if this would be so in all cases. Given that the limits of
agreement may or may not be disregarded, depending on their
perceived suitability, we examine whether it would possible that
the deletion of an outlier may lead to a calculation of limits of
agreement that are usable in all cases?
\\
Should an Outlying Observation be omitted from a data set? In
general, this is not considered prudent.
\\
Also, it may be required that the outliers are worthy of
particular attention themselves.
\\
Classifying outliers and recalculating We opted to examine this
matter in more detail. The following points have to be considered
\\how to suitably identify an outlier (in a generalized sense)
\\Would a recalculation of the limits of agreement generally
results in  a compacted range between the upper and lower limits
of agreement?
\subsection{Agreement} Bland and Altman (1986) define Perfect
agreement as 'The case where all of the pairs of rater data lie
along the line of equality'. The Line of Equality is defined as
the 45 degree line passing through the origin, or X=Y on a XY
plane.

\subsection{Lack Of Agreement}
\begin{enumerate}
	\item Constant Bias\item Proportional Bias
\end{enumerate}

\subsubsection*{Constant Bias} This is a form of systematic
deviations estimated as the average difference between the test
and the reference method


\subsubsection*{Proportional Bias} Two methods may agree on
average, but they may exhibit differences over a range of
measurements\section{Bland Altman Plot} Bland Altman have
recommended the use of graphical techniques to assess agreement.
Principally their method is calculating , for each pair of
corresponding two methods of measurement of some underlying
quantity, with no replicate measurements, the difference and mean.
Differences are then plotted against the mean.
\\
Hopkins argued that the bias in a subsequent Bland-Altman plot was
due, in part, to using least-squares regression at the calibration
phase.

\subsection{Bland Altman plots using 'Gold Standard' raters}
According to Bland and Altman, one should use the methodology
previous outlined, even when one of the raters is a Gold Standard.


\subsection{Bias Detection}
further to this method, the presence of constant bias may be
indicated if the average value differences is not equal to zero.
Bland and Altman does, however, indicate the indication of absence
of bias does not provide sufficient information to allow a
judgement as to whether or not one method can be substituted for
another.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%---------------------------------------------%
	\section{Coefficient of Repeatability}
	\subsection{Repeatability}
	As mentioned previously, \citet{Barnhart} emphasizes the importance of repeatability as part of an overall method comparison study. The coefficient of repeatability was proposed by \citet{BA99}, and is referenced in subsequent papers, such as \citet{BXC2008}. The coefficient of repeatability is a measure of how well a
	measurement method agrees with itself over replicate measurements
	\citep{BA99}. The coefficient of repeatability is a measure of how well a
	measurement method agrees with itself over replicate measurements
	\citep{BA99}. Once the the standard deviations of the differences between the two measurements (in some texts called the residual standard deviation or within-item variability) $sigma_m$ is determined, the
	computation of the coefficients of repeatability for both methods
	is straightforward. The coefficient is calculated from the (in some texts called the residual standard deviation) as  $1.96 \times \sqrt{2} \times \sigma_m$ = $2.83 \sigma_m$).
	
	\subsection{Note 1: Coefficient of Repeatability}
	The coefficient of repeatability is a measure of how well a
	measurement method agrees with itself over replicate measurements
	\citep{BA99}. Once the within-item variability is known, the
	computation of the coefficients of repeatability for both methods
	is straightforward.
	
	
	%------------------------------------------------------------------------------%
	
	\subsection{Repeatability coefficient}
	\citet{BA99} introduces the repeatability coefficient for a method, which is defined as the upper limits of a prediction interval for the absolute difference between two measurements by the same
	method on the same item under identical circumstances \citep{BXC2008}.
	
	$\sigma^2_{x}$ is the within-subject variance of method $x$. The repeatability coefficient is $2.77 \sigma_{x}$ (i.e. $1.96 \times \sqrt{2} \sigma_{x}$). For $95\%$ of subjects, two replicated measurement by the same method will be within this repeatability coefficient.
	
	
	
	\section{Repeatability}
	\subsection{What is Repeatability}
	The quality of repeatability is the ability of a measurement method to give consistent results for a particular subject. That is to say that a measurement will agree with prior and subsequent measurements of the same subject.
	
	\subsection{Repeatability}
	Repeatability (or \textit{test-retest reliability})  describes the variation in measurements taken by a single method of measurement on the same item and under the same conditions. 
	A less-than-perfect test-retest reliability causes test-retest variability. Such variability can be caused by, for example, intra-individual variability and intra-observer variability. 
	A measurement may be said to be repeatable when this variation is smaller than some agreed limit.
	
	Test-retest variability is practically used, for example, in medical monitoring of conditions. In these situations, there is often a predetermined "critical difference", and for differences in monitored values that are smaller than this critical difference, the possibility of pre-test variability as a sole cause of the difference may be considered in addition to, for examples, changes in diseases or treatments.
	
	According to the \textit{Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results}, the following conditions need to be fulfilled in the establishment of repeatability:
	\begin{itemize}
		\item	the same measurement procedure
		\item	the same observer
		\item	the same measuring instrument, used under the same conditions
		\item	the same location
		\item	repetition over a short period of time.
		\item  same objectives
	\end{itemize}
	\bigskip
	
	Repeatability is defined by the \textbf{IUPAC} as `\textit{the closeness of agreement between independent results obtained with the same method on identical test material, under the same conditions (same
		operator, same apparatus, same laboratory and after short intervals of time)}'  and is determined by taking multiple measurements on a series of subjects.
	
	A measurement method can be said to have a good level of repeatability if there is consistency in repeated measurements on the same subject using that method. Conversely, a method has poor repeatability if there is considerable variation in repeated measurements.
	
	
	
	%-----------------------------------------------------------------------------------------------------%
	\newpage
	\section{Importance of Repeatability in MCS}
	
	
	
	Barnhart emphasizes the importance of repeatability as part of an overall method comparison study. Before there can be good agreement between two methods, a method must have good agreement with itself. The coefficient of repeatability , as proposed by Bland \& Altman (1999) is an important feature of both Carstensen's and Roy's methodologies. The coefficient is calculated from the residual standard deviation (i.e. $1.96 \times \sqrt{2} \times \sigma_m$ = $2.83 \sigma_m$).
	
	
	\citet{Barnhart} emphasizes the importance of repeatability as part of an overall method comparison study. Before there can be good agreement between two methods, a method must have good agreement with itself. The coefficient of repeatability , as proposed by \citet{BA99} is an important feature of both Carstensen's and Roy's methodologies. The coefficient is calculated from the residual standard deviation (i.e. $1.96 \times \sqrt{2} \times \sigma_m$ = $2.83 \sigma_m$).
	
	\bigskip
	
	\citet{BA99} strongly recommends the simultaneous estimation of repeatability and agreement be collecting replicated data. \citet{ARoy2009} notes the lack of convenience in such calculations.
	It is important to report repeatability when assessing measurement, because it measures the purest form of random error not influenced by other factors \citep{Barnhart}.	
	
	%% Who Said Next Line
	importance of repeatability' curiously replicate measurements are rarely made in method comparison studies, so that an important aspect of comparability is often overlooked.
	
	Repeatability is important in the context of method comparison because the repeatability of two methods influence the amount of agreement which is possible between those methods. If one method has poor repeatability, the agreement is bound to be poor. If both methods have poor repeatability, agreement is even worse. If one method has poor repeatability in the sense of considerable variability, then agreement between two methods is bound to be poor \citep{ARoy2009}.
	
	\citet{barnhart} and \citet{roy} highlight the importance of reporting repeatability in method comparison, because it measures the purest random error not influenced by any external factors. Statistical procedures on within-subject variances of two methods are equivalent to tests on their respective repeatability coefficients. A formal test is introduced by \citet{roy}, which will discussed in due course.
	
	%--------------------------------------------------------------------%
	%\subsection{Bland and Altman 1999}
	As noted by Bland and Altman 1999, the repeatability of two methods of measurement can  potentially limit
	Repeatability (using Bland-Altman plot)
	The Bland-Altman plot may also be used to assess a method?s repeatability by comparing repeated measurements using one single measurement method on a sample of items.
	The plot can then also be used to check whether the variability or precision of a method is related to the size of the characteristic being measured.
	Since for the repeated measurements the same method is used, the mean difference should be zero.
	Therefore the Coefficient of Repeatability (CR) can be calculated as 1.96 (often rounded to 2) times the standard deviation of the case-wise differences.
	
	\subsection{Coefficient of Repeatability}
	The coefficient of repeatability is a measure of how well a
	measurement method agrees with itself over replicate measurements
	\citep{BA99}. Once the within-item variability is known, the
	computation of the coefficients of repeatability for both methods
	is straightforward.
	
	The British standards Insitute [$1979$] define a coefficient of
	repeatability  as \emph{the value below which the difference between two single test results....may be expected to lie within a specified probability.} Unless otherwise instructed, the
	probability is assumed to be $95\%$. 
	
	The Bland Altman method offers a measurement on the repeatability of the methods. The \emph{Coefficient of Repeatability} (CR) can be calculated as 1.96 (or 2) times the standard deviations of the differences between the two measurements ($d_2$ and $d_1$).
	
	
	
	
	
	%	If one method has poor repeatability in the sense of considerable variability, then agreement between two methods is bound to be poor \citep{ARoy2009}.
	
	
	
	
	
	
	\subsection{Repeatability coefficient from LME Models}
	\citet{BA99} introduces the repeatability coefficient for a method, which is defined as the upper limits of a prediction interval for the absolute difference between two measurements by the same
	method on the same item under identical circumstances \citep{BXC2008}.
	
	$\sigma^2_{x}$ is the within-subject variance of method $x$. The repeatability coefficient is $2.77 \sigma_{x}$ (i.e. $1.96 \times \sqrt{2} \sigma_{x}$). For $95\%$ of subjects, two replicated measurement by the same method will be within this repeatability coefficient.
	
	%% \section{Note 1: Coefficient of Repeatability}
	
	
	
	%------------------------------------------------------------------------------------------%
	\subsection{Repeatability in Bland-Altman Blood Data Analysis}
	\begin{itemize}
		\item Two readings by the same method will be within $1.96
		\sqrt{2} \sigma_w $ or $2.77 \sigma_w $ for 95\% of subjects. Thisvalue is called the repeatability coefficient.
		
		\item For observer J using the sphygmomanometer $ \sigma_w = \sqrt{37.408} = 6.116$ and so the repeatability coefficient is
		$2:77 \times 6.116 = 16:95$ mmHg.
		
		\item For the machine S,$ \sigma_w = \sqrt{83.141} = 9.118$ and the repeatability coefficient is $2:77 \times 9.118 = 25.27$ mmHg.
		
		\item Thus, the repeatability of the machine is 50\% greater than that of the observer.
	\end{itemize}
	%-------------------------------------------------------------------%
	\section{Carstensen}
	\begin{itemize}
		\item The limits of agreement are not always the only issue of
		interest � the assessment of method specific repeatability and
		reproducibility are of interest in their own right.
		
		\item Repeatability can only be assessed when replicate
		measurements by each method are available.
		
		\item The repeatability coefficient for a method is defined as the
		upper limits of a prediction interval for the absolute difference
		between two measurements by the same method on the same item under
		identical circumstances.
		
		\item If the standard deviation of a measurement is $\sigma$ the
		repeatability coefficient is $2\times\sqrt{2} \sigma = 2.83\times
		\sigma \approx 2.8 \sigma$.
		
		
		\item The repeatability of measurement methods is calculated
		differently under the two models \item Under the model assuming
		exchangeable replicates (1), the repeatability is based only on
		the residual standard deviation, i.e. $2.8\sigma_m$
		
		
		\item Under the model for linked replicates (2) there are two
		possibilities depending on the circumstances.
		
		\item If the variation between replicates within item can be
		considered a part of the repeatability it will be $2.8 \sqrt{
			\omega^2 + \sigma^2_m}$.
		
		\item However, if replicates are taken under substantially
		different circumstances, the variance component $\omega^2$ may be
		considered irrelevant in the repeatability and one would therefore
		base the repeatability on the measurement errors alone, i.e. use
		$2.8 \sigma_m$.
	\end{itemize}
	
	
	
	%\section{Reproducibility}
	% 
	%It is advisable to be able to distinguish between Repeatability and a similar concept ?Reproducibility?. Reproducibility is
	
	
	
	
	
	
	
	
	
	%%========================================================================%
	%% Phase Next Section Out
	%% Where did this come from
	
	\newpage
	
	%--------------------------------------------------------------------%
	
	%--------------------------------------------------------------------%
	\subsection{Notes from BXC Book (chapter 9)}
	The assessment of method-specific repeatability and reproducibility is of interest in its own right.
	Repeatability and reproducibility can only be assessed when replicate measurements by each method are available.
	If replicate measurements by a method are available, it is simple to estimate the measurement error for a method, using a model with fixed effects for item, then taking the residual standard deviation as measurement error standard deviation.
	However, if replicates are linked, this may produce an estimate that biased upwards.
	The repeatability coefficient (or simply repeatability) for a method is defined as the upper limit of a
	prediction interval for the absolute difference between two measurements by the same method on the same
	item under identical circumstances (see above conditions)
	
	\[y_{mir}  = \alpha_{m} + \beta_m( \mu_i + a_{ir} + c_{mi}) + e_{mir}\]
	
	The variation between measurements under identical circumstances.

\chapter{Linear Mixed effects Models}
\section{Linear Mixed effects Models}
A linear mixed effects (LME) model is a statistical model containing both fixed effects and random effects (random effects are also known as variance components). LME models are a generalization of the classical linear model, which contain fixed effects only. When the levels of factors are considered to be sampled from a population,
and each level is not of particular interest, they are considered random quantities with associated variances.
The effects of the levels, as described, are known as random effects. Random effects are represented by unobservable
normally distributed random variables. Conversely fixed effects are considered non-random and the
levels of each factor are of specific interest.
%LME models are useful models when considering repeated measurements or grouped observations.

\citet{Fisher4} introduced variance components models for use in genetical studies. Whereas an estimate for variance must take an non-negative value, an individual variance component, i.e.\ a component of the overall variance, may be negative.

The framework has developed since, including contributions from
\citet{tippett}, who extend the use of variance components into linear models, and \citet{eisenhart}, who introduced the `mixed model' terminology and formally distinguished between mixed and random effects models. \citet{Henderson:1950} devised a framework for deriving estimates for both the fixed effects and the random effects, using a set of equations that would become known as `mixed model equations' or `Henderson's equations'.
LME methodology is further enhanced by Henderson's later works \citep{Henderson53, Henderson59,Henderson63,Henderson73,Henderson84a}. The key features of Henderson's work provide the basis for the estimation techniques.

\citet{HartleyRao} demonstrated that unique estimates of the variance components could be obtained using maximum likelihood methods. However these estimates are known to be biased `downwards' (i.e.\ underestimated) , because of the assumption that the fixed estimates are known, rather than being estimated from the data. \citet{PattersonThompson} produced an alternative set of estimates, known as the restricted maximum likelihood (REML) estimates, that do not require the fixed effects to be known. Thusly there is a distinction the REML estimates and the original estimates, now commonly referred to as ML estimates.

\citet{LW82} provides a form of notation for notation for LME models that has since become the standard form, or the basis for more complex formulations. Due to computation complexity, linear mixed effects models have not seen widespread use until many well known statistical software applications began facilitating them. SAS Institute added PROC MIXED to its software suite in 1992 \citep{singer}. \citet{PB} described how to compute LME models in the \texttt{S-plus} environment.

Using Laird-Ware form, the LME model is commonly described in matrix form,
\begin{equation}
y = X\beta + Zb + \epsilon
\label{LW}
\end{equation}

\noindent where $y$ is a vector of $N$ observable random variables, $\beta$ is a vector of $p$ fixed effects, $X$ and $Z$ are $N \times p$ and $N \times q$ known matrices, and $b$ and $\epsilon$  are vectors of $q$ and $N,$ respectively, random effects such that $\mathrm{E}(b)=0, \ \mathrm{E}(\epsilon)=0$
and
\[
\mathrm{var}
\left(
              \begin{array}{c}
                b \\
                \epsilon \\
              \end{array}
            \right)
   =
\left(
         \begin{array}{cc}
           D & 0 \\
           0 & \Sigma \\
         \end{array}
       \right)
\]




where $D$ and $\Sigma$ are positive definite matrices parameterized by an unknown variance component parameter vector $ \theta.$ The variance-covariance matrix for the vector of observations $y$ is given by $V = ZDZ^{\prime}+ \Sigma.$ This implies $y \sim(X\beta, V) = (X\beta,ZDZ^{\prime}+ \Sigma)$. It is worth noting that $V$ is an $n \times n$ matrix, as the dimensionality becomes relevant later on. The notation provided here is generic, and will be adapted to accord with complex formulations that will be encountered in due course.

%\subsection{Likelihood and estimation}

% Likelihood is the hypothetical probability that an event that has already occurred would yield a specific outcome. Likelihood differs from probability in that probability refers to future occurrences, while likelihood refers to past known outcomes.

% The likelihood function ($L(\theta)$)is a fundamental concept in statistical inference. It indicates how likely a particular population is to produce an observed sample. The set of values that maximize the likelihood function are considered to be optimal, and are used as the estimates of the parameters. For computational ease, it is common to use the logarithm of the likelihood function, known simply as the log-likelihood ($\ell(\theta)$).


\subsection{Estimation}
Estimation of LME models involve two complementary estimation issues'; estimating the vectors of the fixed and random effects estimates $\hat{\beta}$ and $\hat{b}$ and estimating the variance covariance matrices $D$ and $\Sigma$.
Inference about fixed effects have become known as `estimates', while inferences about random effects have become known as `predictions'. The most common approach to obtain estimators are Best Linear Unbiased Estimator (BLUE) and Best Linear Unbiased Predictor (BLUP). For an LME model given by (\ref{LW}), the BLUE of $\hat{\beta}$ is given by
\[\hat{\beta} = (X^\prime V^{-1}X)^{-1}X^\prime V^{-1}y,\]whereas the BLUP of $\hat{b}$ is given by
\[\hat{b} = DZ^{\prime} V^{-1} (y-X\hat{\beta}).\]

\subsubsection{Henderson's equations}
Because of the dimensionality of V (i.e. $n \times n$) computing the inverse of V can be difficult. As a way around the this problem \citet{Henderson53, Henderson59,Henderson63,Henderson73,Henderson84a} offered a more simpler approach of jointly estimating $\hat{\beta}$ and $\hat{b}$.
\cite{Henderson:1950} made the (ad-hoc) distributional assumptions $y|b \sim \mathrm{N} (X \beta + Zb, \Sigma)$ and $b \sim \mathrm{N}(0,D),$ and proceeded to maximize the joint density of $y$ and $b$
\begin{equation}
\left|
\begin{array}{cc}
D & 0 \\
0 & \Sigma \\
\end{array}
  \right|^{-\frac{1}{2}}
\exp
\left\{ -\frac{1}{2}
\left(
\begin{array}{c}
                b \\
                y - X \beta -Zb \\
              \end{array}
            \right)^\prime
\left( \begin{array}{cc}
D & 0 \\
0 & \Sigma \\
\end{array}\right)^{-1}
\left(
\begin{array}{c}
                b \\
                y - X \beta -Zb \\
              \end{array}
            \right)
\right\},
\label{u&beta:JointDensity}
\end{equation}
with respect to $\beta$ and $b,$ which ultimately requires minimizing the criterion
\begin{equation}
(y - X \beta -Zb)'\Sigma^{-1}(y - X \beta -Zb) + b^\prime D^{-1}b.
\label{Henderson:Criterion}
\end{equation}
This leads to the mixed model equations
\begin{equation}
\left(\begin{array}{cc}
  X^\prime\Sigma^{-1}X & X^\prime\Sigma^{-1}Z
  \\
  Z^\prime\Sigma^{-1}X & X^\prime\Sigma^{-1}X + D^{-1}
  \end{array}\right)
\left(\begin{array}{c}
    \beta \\
  b
  \end{array}\right)
  =
\left(\begin{array}{c}
  X^\prime\Sigma^{-1}y \\
  Z^\prime\Sigma^{-1}y
  \end{array}\right).
\label{Henderson:Equations}
\end{equation}
Using these equations, obtaining the estimates requires the inversion of a matrix
of dimension $p+q \times p+q$, considerably smaller in size than $V$. \citet{Henderson1963} shows that these mixed model equations do not depend on normality and that $\hat{\beta}$ and $\hat{b}$ are the BLUE and BLUP under general conditions, provided $D$ and $\Sigma$ are known.

\cite{Robi:BLUP:1991} points out that although \cite{Henderson:1950} initially referred to the estimates $\hat{\beta}$ and $\hat{b}$ from (\ref{Henderson:Equations}) as ``joint maximum likelihood estimates", \cite{Henderson:1973} later advised that these estimates should not be referred to as ``maximum likelihood" as the function being maximized in (\ref{Henderson:Criterion}) is a joint density rather than a likelihood function. \cite{Lee:Neld:Pawi:2006} remarks that it is clear that Henderson used joint estimation for computational purposes, without recognizing the theoretical implications.

\subsubsection{Estimation of the fixed parameters}

The vector $y$ has marginal density $y \sim \mathrm{N}(X \beta,V),$ where $V = \Sigma + ZDZ^\prime$ is specified through the variance component parameters $\theta.$ The log-likelihood of the fixed parameters $(\beta, \theta)$ is
\begin{equation}
\ell (\beta, \theta|y) =
-\frac{1}{2} \log |V| -\frac{1}{2}(y -
X \beta)'V^{-1}(y -
X \beta), \label{Likelihood:MarginalModel}
\end{equation}
and for fixed $\theta$ the estimate $\hat{\beta}$ of $\beta$ is obtained as the solution of
\begin{equation}
(X^\prime V^{-1}X) {\beta} = X^\prime V^{-1}y.
\label{mle:beta:hat}
\end{equation}

Substituting $\hat{\beta}$ from (\ref{mle:beta:hat}) into $\ell(\beta, \theta|y)$ from (\ref{Likelihood:MarginalModel}) returns the \emph{profile} log-likelihood
\begin{eqnarray*}
\ell_P(\theta \mid y) &=& \ell(\hat{\beta}, \theta \mid y) \\
&=& -\frac{1}{2} \log |V| -\frac{1}{2}(y - X \hat{\beta})'V^{-1}(y - X \hat{\beta})
\end{eqnarray*}
of the variance parameter $\theta.$ Estimates of the parameters $\theta$ specifying $V$ can be found by maximizing $\ell_P(\theta \mid y)$ over $\theta.$ These are the ML estimates.

For REML estimation the \emph{restricted} log-likelihood is defined as
\[
\ell_R(\theta \mid y) =
\ell_P(\theta \mid y) -\frac{1}{2} \log |X^\prime VX |.
\]
%\subsubsection{Likelihood estimation techniques}
%Maximum likelihood and restricted maximum likelihood have become the most common strategies
%for estimating the variance component parameter $\theta.$ Maximum likelihood estimation obtains
%parameter estimates by optimizing the likelihood function.
%To obtain ML estimate the likelihood is constructed as a function of the parameters in the specified LME model.
% The maximum likelihood estimates (MLEs) of the parameters are the values of the arguments that maximize the likelihood function.

The REML approach does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function derived from a data set, transformed to remove the irrelevant influences \citep{REMLDefine}.
Restricted maximum likelihood is often preferred to maximum likelihood because REML estimation reduces the bias in the variance component by taking into account the loss of degrees of freedom that results
from estimating the fixed effects in $\boldsymbol{\beta}$. Restricted maximum likelihood also handles high correlations more effectively, and is less sensitive to outliers than maximum likelihood.  The problem with REML for model building is that the likelihoods obtained for different fixed effects are not comparable. Hence it is not valid to compare models with different fixed effects using a likelihood ratio test or AIC when REML is used to
estimate the model. Therefore models derived using ML must be used instead.

\subsubsection{Estimation of the random effects}

The established approach for estimating the random effects is to use the best linear predictor of $b$ from $y,$ which for a given $\beta$ equals $DZ^\prime V^{-1}(y - X \beta).$ In practice $\beta$ is replaced by an estimator such as $\hat{\beta}$ from (\ref{mle:beta:hat}) so that $\hat{b} = DZ^\prime V^{-1}(y - X \hat{\beta}).$ Pre-multiplying by the appropriate matrices it is straightforward to show that these estimates $\hat{\beta}$ and $\hat{b}$ satisfy the equations in (\ref{Henderson:Equations}).

\subsubsection{Algorithms for likelihood function optimization}Iterative numerical techniques are used to optimize the log-likelihood function and estimate the covariance parameters $\theta$. The procedure is subject to the constraint that $R$ and $D$ are both positive definite. The most common iterative algorithms for optimizing the likelihood function are the Newton-Raphson method, which is the preferred method, the expectation maximization (EM) algorithm and the Fisher scoring methods.

The EM algorithm, introduced by \citet{EM}, is an iterative technique for maximizing complicated likelihood functions. The algorithm alternates between performing an expectation (E) step
and the maximization (M) step. The `E' step computes the expectation of the log-likelihood evaluated using the current
estimate for the variables. In the `M' step, parameters that maximize the expected log-likelihood, found on the previous `E' step, are computed. These parameter estimates are then used to determine the distribution of the variables in the next `E' step. The algorithm alternatives between these two steps until convergence is reached.

The main drawback of the EM algorithm is its slow rate of
convergence. Consequently the EM algorithm is rarely used entirely in LME estimation,
instead providing an initial set of values that can be passed to
other optimization techniques.

The Newton Raphson (NR) method is the most common, and recommended technique for ML and
REML estimation. The NR algorithm minimizes an objective function defines as $-2$ times the log likelihood for the covariance parameters $\theta$. At every iteration the NR algorithm requires the
calculation of a vector of partial derivatives, known as the gradient, and the second derivative matrix with respect to the covariance parameters. This is known as the observed Hessian matrix. Due to the Hessian matrix, the NR algorithm is more time-consuming, but convergence is reached with fewer iterations compared to the EM algorithm. The Fisher scoring algorithm is an variant of the NR algorithm that is more numerically stable and likely to converge, but not recommended to obtain final estimates.

\subsubsection{The extended likelihood}

The desire to have an entirely likelihood-based justification for estimates of random effects, in contrast to Henderson's equation, has motivated \citet[page 429]{Pawi:in:2001} to define the \emph{extended likelihood}. He remarks ``In mixed effects modelling the extended likelihood has been called \emph{h-likelihood} (for hierarchical  likelihood) by \cite{Lee:Neld:hier:1996}, while in smoothing literature it is known as the \emph{penalized likelihood} (e.g.\ \citeauthor{Gree:Silv:nonp:1994} \citeyear{Gree:Silv:nonp:1994})." The extended likelihood can be written $L(\beta,\theta,b|y) = p(y|b;\beta,\theta) p(b;\theta)$ and adopting the same distributional assumptions used by \cite{Henderson:1950} yields the log-likelihood function

\begin{eqnarray*}
\ell_h(\beta,\theta,b|y)
& = \displaystyle -\frac{1}{2} \left\{ \log|\Sigma| + (y - X \beta -Zb)'\Sigma^{-1}( y - X \beta -Zb) \right.\\
&  \hspace{0.5in} \left. + \log|D| + b^\prime D^{-1}b \right\}.
\end{eqnarray*}
Given $\theta$, differentiating with respect to $\beta$ and $b$ returns Henderson's equations in (\ref{Henderson:Equations}).

\subsubsection{The LME model as a general linear model}
Henderson's equations in (\ref{Henderson:Equations}) can be rewritten $( T^\prime W^{-1} T ) \delta = T^\prime W^{-1} y_{a} $ using
\[
\delta = \left(\begin{array}{c}\beta \\ b \end{array}\right),
\ y_{a} = \left(\begin{array}{c}
  y \cr \psi
  \end{array}\right),
\ T = \left(\begin{array}{cc}
  X & Z  \\
  0 & I
  \end{array}\right),
\ \textrm{and} \ W = \left(\begin{array}{cc}
  \Sigma & 0  \cr
  0 &  D \end{array}\right),
\]
where \cite{Lee:Neld:Pawi:2006} describe $\psi = 0$ as quasi-data with mean $\mathrm{E}(\psi) = b.$ Their formulation suggests that the joint estimation of the coefficients $\beta$ and $b$ of the linear mixed effects model can be derived via a classical augmented general linear model $y_{a} = T\delta + \varepsilon$ where $\mathrm{E}(\varepsilon) = 0$ and $\mathrm{var}(\varepsilon) = W,$ with \emph{both} $\beta$ and $b$ appearing as fixed parameters. The usefulness of this reformulation of an LME as a general linear model will be revisited.


%------------------------------------------------------------------------------------%
\newpage                                                                    % - Section 4
%----------------------------------------------------------------------------------------%
\section{Repeated Measurements in LME models}

In many statistical analyzes, the need to determine parameter estimates where multiple measurements are available on each of a set of variables often arises. Further to \citet{lam}, \citet{hamlett} performs an analysis of the correlation of replicate measurements, for two variables of interest, using LME models.

Let $y_{Aij}$ and $y_{Bij}$ be the $j$th repeated observations of the variables of interest $A$ and $B$ taken on the $i$th subject. The number of repeated measurements for each variable may differ for each individual.
Both variables are measured on each time points. Let $n_{i}$ be the number of observations for each variable, hence $2\times n_{i}$ observations in total.

It is assumed that the pair $y_{Aij}$ and $y_{Bij}$ follow a bivariate normal distribution.
\begin{eqnarray*}
\left(
  \begin{array}{c}
    y_{Aij} \\
    y_{Bij} \\
  \end{array}
\right) \sim \mathcal{N}(
    \boldsymbol{\mu}, \boldsymbol{\Sigma})\mbox{   where } \boldsymbol{\mu} = \left(
  \begin{array}{c}
    \mu_{A} \\
    \mu_{B} \\
  \end{array}
\right)
\end{eqnarray*}

The matrix $\Sigma$ represents the variance component matrix between response variables at a given time point $j$.

\[
\boldsymbol{\Sigma} = \left( \begin{array}{cc}
                        \sigma^2_{A} & \sigma_{AB} \\
                        \sigma_{AB} & \sigma^2_{B}\\
                      \end{array}   \right)
\]

$\sigma^2_{A}$ is the variance of variable $A$, $\sigma^2_{B}$ is the variance of variable $B$ and $\sigma_{AB}$ is the covariance of the two variable. It is assumed that $\boldsymbol{\Sigma}$ does not depend on a particular time point, and is the same over all time points.

%------------------------------------------------------------------------------%
\subsection{Formulation of the Response Vector}
 Information of individual $i$ is recorded in a response vector $\boldsymbol{y}_{i}$. The response vector is constructed by stacking the response of the $2$ responses at the first instance, then the $2$ responses at the second instance, and so on. Therefore the response vector is a $2n_{i} \times 1$ column vector.
The covariance matrix of $\boldsymbol{y_{i}}$ is a $2n_{i} \times 2n_{i}$ positive definite matrix $\boldsymbol{\Omega}_{i}$.

Consider the case where three measurements are taken by both methods $A$ and $B$, $\boldsymbol{y}_{i}$ is a $6 \times 1$ random vector describing the $i$th subject.
\[
\boldsymbol{y}_{i} = (y_{i}^{A1},y_{i}^{B1},y_{i}^{A2},y_{i}^{B2},y_{i}^{A3},y_{i}^{B3}) \prime
\]

The response vector $\boldsymbol{y_{i}}$ can be formulated as an LME model according to Laird-Ware form.
\begin{eqnarray*}
\boldsymbol{y_{i}} = \boldsymbol{X_{i}\beta}  + \boldsymbol{Z_{i}b_{i}} + \boldsymbol{\epsilon_{i}}\\
\boldsymbol{b_{i}} \sim \mathcal{N}(\boldsymbol{0,D})\\
\boldsymbol{\epsilon_{i}} \sim \mathcal{N}(\boldsymbol{0,R_{i}})
\end{eqnarray*}

Information on the fixed effects are contained in a three dimensional vector $\boldsymbol{\beta} = (\beta_{0},\beta_{1},\beta_{2})\prime$. For computational purposes $\beta_{2}$ is conventionally set to zero. Consequently $\boldsymbol{\beta}$ is the solutions of the means of the two methods, i.e. $E(\boldsymbol{y}_{i})  = \boldsymbol{X}_{i}\boldsymbol{\beta}$. The variance covariance matrix $\boldsymbol{D}$ is a general $2 \times 2$ matrix, while $\boldsymbol{R}_{i}$ is a $2n_{i} \times 2n_{i}$ matrix.

%------------------------------------------------------------------------------%
\subsection{Decomposition of the response covariance matrix}

The variance covariance structure can be re-expressed in the following form,
\[
\mbox{Cov}(\mbox{y}_{i}) = \boldsymbol{\Omega_{i}} = \boldsymbol{Z}_{i}\boldsymbol{D}\boldsymbol{Z}_{i}^\prime + \boldsymbol{R_{i}}.
\]

$\boldsymbol{R_{i}}$ can be shown to be the Kronecker product of a correlation matrix $\boldsymbol{V}$ and $\boldsymbol{\Lambda}$. The correlation matrix $\boldsymbol{V}$ of the repeated measures on a given response variable is assumed to be the same for all response variables. Both \citet{hamlett} and \citet{lam} use the identity matrix, with dimensions $n_{i} \times n_{i}$ as the formulation for $\boldsymbol{V}$. \citet{roy} remarks that, with repeated measures, the response for each subject is correlated for each variable, and that such correlation must be taken into account in order to produce a valid inference on correlation estimates.  \citet{roy2006} proposes various correlation structures may be assumed for repeated measure correlations, such as the compound symmetry and autoregressive structures, as alternative to the identity matrix.

However, for the purposes of method comparison studies, the necessary estimates are currently only determinable when the identity matrix is specified, and the results in \citet{roy} indicate its use.

For the response vector described, \citet{hamlett} presents a detailed covariance matrix. A brief summary shall be presented here only. The overall variance matrix is a $6 \times 6$ matrix composed of two types of $2 \times 2$ blocks. Each block represents one separate time of measurement.

\[
\boldsymbol{\Omega}_{i} = \left(
\begin{array}{ccc}
  \boldsymbol{\Sigma} & \boldsymbol{D} & \boldsymbol{D}\\
  \boldsymbol{D} & \boldsymbol{\Sigma} & \boldsymbol{D}\\
  \boldsymbol{D} & \boldsymbol{D} & \boldsymbol{\Sigma}\\
\end{array}\right)
\]

The diagonal blocks are $\Sigma$, as described previously. The $2 \times 2$ block diagonal matrix in $\boldsymbol{\Omega}$ gives $\boldsymbol{\Sigma}$. $\boldsymbol{\Sigma}$ is the sum of the between-subject variability $\boldsymbol{D}$ and the within subject variability $\boldsymbol{\Lambda}$.

$\boldsymbol{\Omega_{i}}$ can be expressed as
\[
\boldsymbol{\Omega_{i}} = \boldsymbol{Z}_{i}\boldsymbol{D}\boldsymbol{Z}_{i}^\prime + ({\boldsymbol{I_{n_{i}}} \otimes \boldsymbol{\Lambda}}).
\]
The notation $\mbox{dim}_{n_{i}}$ means an $n_{i} \times n_{i}$ diagonal block.

\subsection{Correlation terms}
\citet{hamlett} demonstrated how the between-subject and within subject variabilities can be expressed in terms of
correlation terms.

\[
\boldsymbol{D} = \left( \begin{array}{cc}
                        \sigma^2_{A}\rho_{A} & \sigma_{A}\sigma_{b}\rho_{AB}\delta \\
                        \sigma_{A}\sigma_{b}\rho_{AB}\delta & \sigma^2_{B}\rho_{B}\\

                      \end{array}\right)
\]

\[
\boldsymbol{\Lambda} = \left(
\begin{array}{cc}
  \sigma^2_{A}(1-\rho_{A}) & \sigma_{AB}(1-\delta)  \\
    \sigma_{AB}(1-\delta) & \sigma^2_{B}(1-\rho_{B}) \\
\end{array}\right).
\]

$\rho_{A}$ describe the correlations of measurements made by the method $A$ at different times. Similarly $\rho_{B}$ describe the correlation of measurements made by the method $B$ at different times. Correlations among repeated measures within the same method are known as intra-class correlation coefficients. $\rho_{AB}$ describes the correlation of measurements taken at the same same time by both methods. The coefficient $\delta$ is added for when the measurements are taken at different times, and is a constant of less than $1$ for linked replicates. This is based on the assumption that linked replicates measurements taken at the same time would have greater correlation than those taken at different times. For unlinked replicates $\delta$ is simply $1$. \citet{hamlett} provides a useful graphical depiction of the role of each correlation coefficients.

\newpage
\section{Using LME for method comparison}
Due to the prevalence of modern statistical software, \citet{BXC2008} advocates the adoption of computer based approaches, such as LME models, to method comparison studies. \citet{BXC2008} remarks upon `by-hand' approaches advocated in \citet{BA99} discouragingly, describing them as tedious, unnecessary and `outdated'. Rather than using the `by hand' methods, estimates for required LME parameters can be read directly from program output. Furthermore, using computer approaches removes constraints associated with `by-hand' approaches, such as the need for the design to be perfectly balanced.

\subsection{Roy's Approach}

For the purposes of comparing two methods of measurement, \citet{roy} presents a framework that utilizes linear mixed effects model. This methodology provides for the formal testing of inter-method bias, between-subject variability and within-subject variability of two methods. The formulation contains a Kronecker product covariance structure in a doubly multivariate setup. By doubly multivariate set up, Roy means that the information on each patient or item is multivariate at two levels, the number of methods and number of replicated measurements. Further to \citet{lam}, it is assumed that the replicates are linked over time. However it is easy to modify to the unlinked case.

\citet{roy} sets out three criteria for two methods to be considered in agreement. Firstly that there be no significant bias. Second that there is no difference in the between-subject variabilities, and lastly that there is no significant difference in the within-subject variabilities. Roy further proposes examination of the the overall variability by considering the second and third criteria be examined jointly. Should both the second and third criteria be fulfilled, then the overall variabilities of both methods would be equal.

A formal test for inter-method bias can be implemented by examining the fixed effects of the model. This is common to well known classical linear model methodologies. The null hypotheses, that both methods have the same mean, which is tested against the alternative hypothesis, that both methods have different means.
The inter-method bias and necessary $t-$value and $p-$value are presented in computer output. A decision on whether the first of Roy's criteria is fulfilled can be based on these values.

Importantly \citet{roy} further proposes a series of three tests on the variance components of an LME model, which allow decisions on the second and third of Roy's criteria. For these tests, four candidate LME models are constructed. The differences in the models are specifically in how the the $D$ and $\Lambda$ matrices are constructed, using either an unstructured form or a compound symmetry form. To illustrate these differences, consider a generic matrix $A$,

\[
\boldsymbol{A} = \left( \begin{array}{cc}
    a_{11} & a_{12}  \\
    a_{21} & a_{22}  \\
    \end{array}\right).
\]

A symmetric matrix allows the diagonal terms $a_{11}$ and $a_{22}$ to differ. The compound symmetry structure requires that both of these terms be equal, i.e $a_{11} = a_{22}$.

The first model acts as an alternative hypothesis to be compared against each of three other models, acting as null hypothesis models, successively. The models are compared using the likelihood ratio test. Likelihood ratio tests are a class of tests based on the comparison of the values of the likelihood functions of two candidate models. 






\subsection{Correlation}
In addition to the variability tests, Roy advises that it is preferable that a correlation of greater than $0.82$ exist for two methods to be considered interchangeable. However if two methods fulfil all the other conditions for agreement, failure to comply with this one can be overlooked. Indeed Roy demonstrates that placing undue importance to it can lead to incorrect conclusions. \citet{roy} remarks that current computer implementations only gives overall correlation coefficients, but not their variances. Consequently it is not possible to carry out inferences based on all overall correlation coefficients.

%--------------------------------------------------%
\subsection{Variability test 1}
The first test determines whether or not both methods $A$ and $B$ have the same between-subject variability, further to the second of Roy's criteria.
\begin{eqnarray*}
H_{0}: \mbox{ }d_{A}  = d_{B} \\
H_{A}: \mbox{ }d_{A}  \neq d_{B}
\end{eqnarray*}
This test is facilitated by constructing a model specifying a symmetric form for $D$ (i.e. the alternative model) and comparing it with a model that has compound symmetric form for $D$ (i.e. the null model). For this test $\boldsymbol{\hat{\Lambda}}$ has a symmetric form for both models, and will be the same for both.

%---------------------------------------------%
\subsection{Variability test 2}

This test determines whether or not both methods $A$ and $B$ have the same within-subject variability, thus enabling a decision on the third of Roy's criteria.

\begin{eqnarray*}
H_{0}: \mbox{ }\lambda_{A}  = \lambda_{B} \\
H_{A}: \mbox{ }\lambda_{A}  = \lambda_{B}
\end{eqnarray*}

This model is performed in the same manner as the first test, only reversing the roles of $\boldsymbol{\hat{D}}$ and $\boldsymbol{\hat{\Lambda}}$. The null model is constructed a symmetric form for $\boldsymbol{\hat{\Lambda}}$ while the alternative model uses a compound symmetry form. This time $\boldsymbol{\hat{D}}$ has a symmetric form for both models, and will be the same for both.

As the within-subject variabilities are fundamental to the coefficient of repeatability, this variability test likelihood ratio test is equivalent to testing the equality of two coefficients of repeatability of two methods. In presenting the results of this test, \citet{roy} includes the coefficients of repeatability for both methods.

%-----------------------------------------------%
\subsection{Variability test 3}
The last of the variability test examines whether or not methods $A$ and $B$ have the same overall variability. This enables the joint consideration of second and third criteria.
\begin{eqnarray*}
H_{0}: \mbox{ }\sigma_{A}  = \sigma_{B} \\
H_{A}: \mbox{ }\sigma_{A}  = \sigma_{B}
\end{eqnarray*}

The null model is constructed a symmetric form for both $\boldsymbol{\hat{D}}$ and $\boldsymbol{\hat{\Lambda}}$ while the alternative model uses a compound symmetry form for both.

\subsection{Demonstration of Roy's testing}
Roy provides three case studies, using data sets well known in method comparison studies, to demonstrate how the methodology should be used. The first two examples used are from the `blood pressure' data set introduced by \citet{BA99}. The data set is a tabulation of simultaneous measurements of systolic blood pressure were made by each of two experienced observers (denoted `J' and `R') using a sphygmomanometer and by a semi-automatic blood pressure monitor (denoted `S'). Three sets of readings were made in quick succession. Roy compares the `J' and `S' methods in the first of her examples.

The inter-method bias between the two method is found to be $15.62$ , with a $t-$value of $-7.64$, with a $p-$value of less than $0.0001$. Consequently there is a significant inter-method bias present between methods $J$ and $S$, and the first of the Roy's three agreement criteria is unfulfilled.

Next, the first variability test is carried out, yielding maximum likelihood estimates of the between-subject variance covariance matrix, for both the null model, in compound symmetry (CS) form, and the alternative model in symmetric (symm) form. These matrices are determined to be as follows;
\[
\boldsymbol{\hat{D}}_{CS} = \left( \begin{array}{cc}
    946.50 & 784.32  \\
    784.32 & 946.50  \\
    \end{array}\right),
\hspace{1.5cm}
    \boldsymbol{\hat{D}}_{Symm} = \left( \begin{array}{cc}
    923.98 & 785.24  \\
    785.24 & 971.30  \\
    \end{array}\right).
\]

A likelihood ratio test is perform to compare both candidate models. The log-likelihood of the null model is $-2030.7$, and for the alternative model $-2030.8$. The test statistic, presented with greater precision than the log-likelihoods, is $0.1592$. The $p-$value is $0.6958$. Consequently we fail to reject the null model, and by extension, conclude that the hypothesis that methods $J$ and $S$ have the same between-subject variability. Thus the second of the criteria is fulfilled.

The second variability test determines maximum likelihood estimates of the within-subject variance covariance matrix, for both the null model, in CS form, and the alternative model in symmetric form.

\[
\boldsymbol{\hat{\Lambda}_{CS}} = \left( \begin{array}{cc}
    60.27  & 16.06  \\
    16.06  & 60.27  \\
    \end{array}\right),
\hspace{1.5cm}
\boldsymbol{\hat{\Lambda}}_{Symm} = \left( \begin{array}{cc}
    37.40 & 16.06  \\
    16.06 & 83.14  \\
    \end{array}\right).
\]

Again, A likelihood ratio test is perform to compare both candidate models. The log-likelihood of the alternative model model is $-2045.0$. As before, the null model has a log-likelihood of $-2030.7$. The test statistic is computed as $28.617$, again presented with greater precision. The $p-$value is less than $0.0001$. In this case we reject the null hypothesis of equal within-subject variability. Consequently the third of Roy's criteria is unfulfilled.
The coefficient of repeatability for methods $J$ and $S$ are found to be 16.95 mmHg and 25.28 mmHg respectively.

The last of the three variability tests is carried out to compare the overall variabilities of both methods.
With the null model the MLE of the within-subject variance covariance matrix is given below. The overall variabilities for the null and alternative models, respectively, are determined to be as follows;
\[
\boldsymbol{\hat{\Sigma}}_{CS} = \left( \begin{array}{cc}
    1007.92  & 801.65  \\
    801.65  & 1007.92  \\
    \end{array}\right),
    \hspace{1.5cm}
\boldsymbol{\hat{\Sigma}}_{Symm} = \left( \begin{array}{cc}
    961.38 & 801.40  \\
    801.40 & 1054.43  \\
    \end{array}\right),
\]

The log-likelihood of the alternative model model is $-2045.2$, and again, the null model has a log-likelihood of $-2030.7$. The test statistic is $28.884$, and the $p-$value is less than $0.0001$. The null hypothesis, that both methods have equal overall variability, is rejected. Further to the second variability test, it is known that this difference is specifically due to the difference of within-subject variabilities.

Lastly, Roy considers the overall correlation coefficient. The diagonal blocks $\boldsymbol{\hat{r}_{\Omega}}_{ii}$ of the correlation matrix indicate an overall coefficient of $0.7959$. This is less than the threshold of 0.82 that Roy recommends.

\[
\boldsymbol{\hat{r}_{\Omega}}_{ii} = \left( \begin{array}{cc}
    1  & 0.7959  \\
    0.7959  & 1  \\
    \end{array}\right)
\]

The off-diagonal blocks of the overall correlation matrix $\boldsymbol{\hat{r}_{\Omega}}_{ii'}$ present the correlation coefficients further to \citet{hamlett}.
\[
\boldsymbol{\hat{r}_{\Omega}}_{ii'} = \left( \begin{array}{cc}
    0.9611  & 0.7799  \\
    0.7799  & 0.9212  \\
    \end{array}\right).
\]

The overall conclusion of the procedure is that method $J$ and $S$ are not in agreement, specifically due to the within-subject variability, and the inter-method bias. The repeatability coefficients are substantially different, with the coefficient for method $S$ being 49\% larger than for method $J$. Additionally the overall correlation coefficient did not exceed the recommended threshold of $0.82$.


%------------------------------------------------------------------------------------%
\newpage
\section{Limits of agreement in LME models}

Limits of agreement are used extensively for assessing agreement, because they are intuitive and easy to use.
Necessarily their prevalence in literature has meant that they are now the best known measurement for agreement, and therefore any newer methodology would benefit by making reference to them.

\citet{BXC2008} uses LME models to determine the limits of agreement. Between-subject variation for method $m$ is given by $d^2_{m}$ and within-subject variation is given by $\lambda^2_{m}$.  \citet{BXC2008} remarks that for two methods $A$ and $B$, separate values of $d^2_{A}$ and $d^2_{B}$ cannot be estimated, only their average. Hence the assumption that $d_{x}= d_{y}= d$ is necessary. The between-subject variability $\boldsymbol{D}$ and within-subject variability $\boldsymbol{\Lambda}$ can be presented in matrix form,\[
\boldsymbol{D} = \left(%
\begin{array}{cc}
   d^2_{A}& 0 \\
  0 & d^2_{B} \\
\end{array}%
\right)=\left(%
\begin{array}{cc}
   d^2& 0 \\
  0 & d^2\\
\end{array}%
\right),
\hspace{1.5cm}
\boldsymbol{\Lambda} = \left(%
\begin{array}{cc}
   \lambda^2_{A}& 0 \\
  0 & \lambda^2_{B} \\
\end{array}%
\right).
\]

The variance for method $m$ is $d^2_{m}+\lambda^2_{m}$. Limits of agreement are determined using the standard deviation of the case-wise differences between the sets of measurements by two methods $A$ and $B$, given by
\begin{equation}
\mbox{var} (y_{A}-y_{B}) = 2d^2 + \lambda^2_{A}+ \lambda^2_{B}.
\end{equation}
Importantly the covariance terms in both variability matrices are zero, and no covariance component is present.

\citet{BXC2008} presents a data set `fat', which is a comparison of measurements of subcutaneous fat
by two observers at the Steno Diabetes Center, Copenhagen. Measurements are in millimeters
(mm). Each person is measured three times by each observer. The observations are considered to be `true' replicates.

A linear mixed effects model is formulated, and implementation through several software packages is demonstrated.
All of the necessary terms are presented in the computer output. The limits of agreement are therefore,
\begin{equation}
0.0449  \pm 1.96 \times  \sqrt{2 \times 0.0596^2 + 0.0772^2 + 0.0724^2} = (-0.220,  0.309).
\end{equation}

\citet{roy} has demonstrated a methodology whereby $d^2_{A}$ and $d^2_{B}$ can be estimated separately. Also covariance terms are present in both $\boldsymbol{D}$ and $\boldsymbol{\Lambda}$. Using Roy's methodology, the variance of the differences is
\begin{equation}
\mbox{var} (y_{iA}-y_{iB})= d^2_{A} + \lambda^2_{B} + d^2_{A} + \lambda^2_{B} - 2(d_{AB} + \lambda_{AB})
\end{equation}
All of these terms are given or determinable in computer output.
The limits of agreement can therefore be evaluated using
\begin{equation}
\bar{y_{A}}-\bar{y_{B}} \pm 1.96 \times \sqrt{ \sigma^2_{A} + \sigma^2_{B}  - 2(\sigma_{AB})}.
\end{equation}

For Carstensen's `fat' data, the limits of agreement computed using Roy's
method are consistent with the estimates given by \citet{BXC2008}; $0.044884  \pm 1.96 \times  0.1373979 = (-0.224,  0.314).$

\subsection{Linked replicates}

\citet{BXC2008} proposes the addition of an random effects term to their model when the replicates are linked. This term is used to describe the `item by replicate' interaction, which is independent of the methods. This interaction is a source of variability independent of the methods. Therefore failure to account for it will result in variability being wrongly attributed to the methods.

\citet{BXC2008} introduces a second data set; the oximetry study. This study done at the Royal Children�s Hospital in
Melbourne to assess the agreement between co-oximetry and pulse oximetry in small babies.

In most cases, measurements were taken by both method at three different times. In some cases there are either one or two pairs of measurements, hence the data is unbalanced. \citet{BXC2008} describes many of the children as being very sick, and with very low oxygen saturations levels. Therefore it must be assumed that a biological change can occur in interim periods, and measurements are not true replicates.

\citet{BXC2008} demonstrate the necessity of accounting for linked replicated by comparing the limits of agreement from the `oximetry' data set using a model with the additional term, and one without. When the interaction is accounted for the limits of agreement are (-9.62,14.56). When the interaction is not accounted for, the limits of agreement are (-11.88,16.83). It is shown that the failure to include this additional term results in an over-estimation of the standard deviations of differences.

Limits of agreement are determined using Roy's methodology, without adding any additional terms, are found to be consistent with the `interaction' model; $(-9.562, 14.504 )$. Roy's methodology assumes that replicates are linked. However, following Carstensen's example, an addition interaction term is added to the implementation of Roy's model to assess the effect, the limits of agreement estimates do not change. However there is a conspicuous difference in within-subject matrices of Roy's model and the modified model (denoted $1$ and $2$ respectively);
\begin{equation}
\hat{\boldsymbol{\Lambda}}_{1}= \left(\begin{array}{cc}
 16.61 &	11.67\\
11.67 & 27.65 \end{array}\right) \qquad
\boldsymbol{\hat{\Lambda}}_{2}= \left( \begin{array}{cc}
    7.55 & 2.60 \\
    2.60 & 18.59 \end{array} \right). 
\end{equation}

\noindent (The variance of the additional random effect in model $2$ is $3.01$.)

\citet{akaike} introduces the Akaike information criterion ($AIC$), a model 
selection tool based on the likelihood function. Given a data set, candidate models
are ranked according to their AIC values, with the model having the lowest AIC being considered the best fit.Two candidate models can said to be equally good if there is a difference of less than $2$ in their AIC values.

The Akaike information criterion (AIC) for both models are $AIC_{1} = 2304.226$ and $AIC_{2} = 2306.226$ , indicating little difference in models. The AIC values for the Carstensen `unlinked' and `linked' models are $1994.66$ and $1955.48$ respectively, indicating an improvement by adding the interaction term.

The $\boldsymbol{\hat{\Lambda}}$ matrices are informative as to the difference between Carstensen's unlinked and linked models. For the oximetry data, the covariance terms (given above as 11.67 and 2.6 respectively ) are of similar magnitudes to the variance terms. Conversely for the `fat' data the covariance term ($-0.00032$) is negligible. When the interaction term is added to the model, the covariance term remains negligible. (For the `fat' data, the difference in AIC values is also approximately $2$).

To conclude, Carstensen's models provided a rigorous way to determine limits of agreement, but don't provide for the computation of $\boldsymbol{\hat{D}}$ and $\boldsymbol{\hat{\Lambda}}$. Therefore the test's proposed by \citet{roy} can not be implemented. Conversely, accurate limits of agreement as determined by Carstensen's model may also be found using Roy's method. Addition of the interaction term erodes the capability of Roy's methodology to compare candidate models, and therefore shall not be adopted.

Finally, to complement the blood pressure (i.e.`J vs S') method comparison from the previous section (i.e.`J vs S'), the limits of agreement are $15.62 \pm 1.96 \times 20.33 = (-24.22, 55.46)$.)
\newpage

\chapter{Introduction}
% - A
\section{LME models in method comparison studies}
%With the greater computing power available for scientific
%analysis, it is inevitable that complex models such as linear
%mixed effects models should be applied to method comparison
%studies.
%\section{Roy's LME methodology for assessing agreement}






\citet{Barnhart} describes the sources of disagreement in a method comparison study problem as
differing population means, different between-subject variances, different within-subject variances between two methods and poor
correlation between measurements of two methods. Further to this, \citet{ARoy2009} states three criteria for two methods to be considered in agreement. Firstly that there be no significant bias. Second that there is no difference in the between-subject variabilities, and lastly that there is no significant difference in the within-subject variabilities. 	Roy further proposes examination of the the overall variability by considering the second and third criteria be examined jointly. Should both the second and third criteria be fulfilled, then the overall variabilities of both methods would be equal.

\citet{ARoy2009} further proposes examination of the the overall variability by considering the second and third criteria be examined jointly. Should both the second and third criteria be fulfilled, then the overall variabilities of both methods would be equal.
%\section{Roy's LME methodology for assessing agreement}



The LME model approach has seen increased use as a framework for method comparison studies in recent years (Lai $\&$ Shaio, Carstensen and Choudhary as examples)


Linear mixed effects (LME) models can facilitate greater
understanding of the potential causes of bias and differences in
precision between two sets of measurement. 

% LAISHIAO
\citet{LaiShiao} views
the uses of linear mixed effects models as an expansion on the
Bland-Altman methodology, rather than as a replacement.\citet{LaiShiao} view the LME Models approach as an natural expansion to the Bland ? Altman method for comparing two measurement methods. Their focus is to explain lack of agreement by means of additional covariates outside the scope of the traditional method comparison problem. \citet{LaiShiao} is interesting in that it extends the usual method comparison study question. It correctly identifies LME models as a methodoloy that can used to make such questions tractable.	

\citet{LaiShiao} extends the usual method comparison study question. It correctly identifies LME models as a methodoloy that can used to make such questions tractable.
The Data Set used in their examples is unavailable for independent use. Therefore, for the sake of consistency, a data set will be simulated based on the Blood Data that will allow for extra variables.
% BXC
\citet{BXC2008} remarks that modern statistical computation, such as that used for LME models, greatly improve the efficiency of
calculation compared to previous `by-hand' methods.

Due to the prevalence of modern statistical software, \citet{BXC2008} advocates the adoption of computer based approaches, such as LME models, to method comparison studies. \citet{BXC2008} remarks upon `by-hand' approaches advocated in \citet{BA99} discouragingly, describing them as tedious, unnecessary and `outdated'. Rather than using the `by hand' methods, estimates for required LME parameters can be read directly from program output.

\citet{BXC2008} remarks upon `by-hand' approaches advocated in \citet{BA99} discouragingly, describing them as tedious, unnecessary and `outdated'. Due to the prevalence of modern statistical software, \citet{BXC2008} advocates the adoption of computer based approaches to method comparison studies, allowing the use of LME models that would not have been feasible otherwise. Rather than using the `by hand' methods, estimates for required parameters can be gotten directly from output code. Furthermore, using computer approaches removes constraints, such as the need for the design to be perfectly balanced.
In part this is due to the increased profile of LME models, and furthermore the availability of capable software.  

Additionally a great understanding of residual analysis and influence analysis for LME models has been adchieved thanks to authors such as \citet{schab}, \citet{CPJ}, \citet{cook86} \citet{west}, amongst others. In this chapter various LME approaches to method comparison studies shall
be examined. 

Additionally LME based approaches may utilise the diagnostic and influence analysis techniques that have been developed in recent times.

\newpage
%\section{Carstensen 2004 model in the single measurement case}
%\citet{BXC2004} presents a model to describe the relationship between a value of measurement and its real value.
%The non-replicate case is considered first, as it is the context of the Bland-Altman plots.
%This model assumes that inter-method bias is the only difference between the two methods.
%
%
%\begin{equation}
%y_{mi}  = \alpha_{m} + \mu_{i} + e_{mi} \qquad  e_{mi} \sim \mathcal{N}(0,\sigma^{2}_{m})
%\end{equation}
%
%The differences are expressed as $d_{i} = y_{1i} - y_{2i}$.
%
%For the replicate case, an interaction term $c$ is added to the model, with an associated variance component.




%---Carstensen's limits of agreement
%---The between item variances are not individually computed. An estimate for their sum is used.
%---The within item variances are indivdually specified.
%---Carstensen remarks upon this in his book (page 61), saying that it is "not often used".
%---The Carstensen model does not include covariance terms for either VC matrices.
%---Some of Carstensens estimates are presented, but not extractable, from R code, so calculations have to be done by %---hand.
%--Importantly, estimates required to calculate the limits of agreement are not extractable, and therefore the calculation must be done by hand.
%---All of Roys stimates are  extractable from R code, so automatic compuation can be implemented
%---When there is negligible covariance between the two methods, Roys LoA and Carstensen's LoA are roughly the same.
%---When there is covariance between the two methods, Roy's LoA and Carstensen's LoA differ, Roys usually narrower.


%%---Estimability of Tau
%When only two methods are compared, \citet{BXC2008} notes that separate estimates of $\tau^2_m$ can not be obtained %due to the model over-specification. To overcome this, the assumption of equality, i.e. $\tau^2_1 = \tau^2_2$, is %required.

%With regards to the specification of the variance terms, Carstensen  remarks that using their approach is common, %remarking that \emph{ The only slightly non-standard (meaning ``not often used") feature is the differing residual %variances between methods }\citep{bxc2010}.



%\chapter{Limits of Agreement}

%\section{Modelling Agreement with LME Models}

% Carstensen pages 22-23


Roys uses and LME model approach to provide a set of formal tests for method comparison studies.\\


% \subsection{Laird-Ware Notation}

\section{Introduction to LME Models, Fitting LME Models to MCS Data}

In cases where there are repeated measurements by each of the two methods on the same subjects , \citet{BA99} suggest calculating
the mean for each method on each subject and use these pairs of means to compare the two methods. The estimate of bias will be unaffected using this approach, but the estimate of the standard deviation of the differences will be incorrect, \citep{BXC2004}. \citet{BXC2004} recommends that replicate measurements for each method, but recognizes that resulting data are more difficult to analyze. To this end, \citet{BXC2004} and \citet{BXC2008} recommend the use of LME models as a suitable framework for method comparison in the case of repeated measurements.

%too small, because of the reduction of the effect of repeated measurement error. Bland Altman propose a correction for this. Carstensen attends to this issue also, adding that another approach would be to treat each repeated measurement separately.
Due to computation complexity, linear mixed effects models have not seen widespread use until many well known statistical software applications began facilitating them. 

This approach has seen increased use in method comparison studies in recent years (Lai \& Shaio, Carstensen and Choudhary as examples). In part this is due to the increased profile of LME models, and furthermore the availability of capable software. Additionally LME based approaches may utilise the diagnostic and influence analysis techniques that have been developed in recent times.

In this section, we introduce the LME model, discusss how it can be applied to MCS problems, and how it is desirable in the case of replicate measurements, giving some examples from previous work (i.e. Carstensen et Al, Lai \& Shaio, and Roy).

Further to that, there will be a demonstration on fitting various types LME models using freely available software.
% To fully understand the complexities, a comparison of the \textbf{nlme} and \textbf{LME4} \texttt{R} Packages is required.

While the MCS problem is conventionally poised in the context of two methods of measurements, LME models allow for a straightforward analysis whereby several methods of measurement can be measured simulataneously. However simple models only can only indicate agreement of lack thereof, and the presence of inter-method bias. To consider more complex questions, more complex LME models are required.  Useful approaches will be introduced in a later section.

\section{Definition of Replicate Measurements (Move to Chapter 1) }




\section{Definition of Replicate measurements}
Further to \citet{BA99}, a formal definition is required of what exactly replicate measurements are

\emph{By replicates we mean two or more measurements on the same
	individual taken in identical conditions. In general this requirement means that the
	measurements are taken in quick succession.}

Roy accords with Bland and Altman?s definition of a replicate, as being two or more measurements on the same individual under identical conditions. Roy allows the assumption that replicated measurements are equi-correlated. Roy allows unequal numbers of replicates.

Replicate measurements are linked over time. However the method can be easily extended to cover situations where they are not linked over time.
%----------------------------------------------------------------------------%
\section{Model for replicate measurements}

We generalize the single measurement model for the replicate measurement case, by additionally specifying replicate values. Let $y_{mir}$ be the $r-$th replicate measurement for subject ``i" made by method ``m". Further to \citet{barnhart} fixed effect can be expressed with a single term $\alpha_{mi}$, which incorporate the true value $\mu_i$.

\[ y_{mir} = \mu_{i} + \alpha_{m} + e_{mir}  \]

Combining fixed effects \citep{barnhart}, we write,

\[ y_{mir} = \alpha_{mi} + e_{mir}.\]

The following assumptions are required

\begin{itemize}
	\item $e_{mir}$ is independent of the fixed effects with mean $\mbox{E}(e_{mir}) = 0$.
	\item Further to \citet{barnhart} between-item and within-item variances $\mbox{Var}(\alpha_{mi}) = \sigma^2_{Bm}$ and $\mbox{Var}(e_{mir}) = \sigma^2_{Wm}$
	%	\item In keeping with \citet{Roy}, these variance shall be considered as part of the between-item variance covariance matrix $\boldsymbol{D}$ and the within-item variance covariance matrix  $\boldsymbol{\Sigma}$
	%	respectively, and will be denoted accordingly ( i.e. $d^2_{m}$ and $\sigma^2_{m}$).
	%	\item Additionally, the total variability of method "m", denoted $\omega^2_m$ is the sum of the within-item and between-item variabilities.
	%	
	%	\[ \omega^2_m = d^2_{m}+ \sigma^2_{m} \]
	
\end{itemize}
\section{Carstensen's Model}



\citet{BXC2004} presents a model to describe the relationship between a value of measurement and its
real value. The non-replicate case is considered first, as it is the context of the Bland Altman plots. This model assumes that inter-method bias is the only difference between the two methods.

A measurement $y_{mi}$ by method $m$ on individual $i$ is formulated as follows;
\begin{equation}
	y_{mi}  = \alpha_{m} + \mu_{i} + e_{mi} \qquad  e_{mi} \sim
	\mathcal{N}(0,\sigma^{2}_{m})
\end{equation}
The differences are expressed as $d_{i} = y_{1i} - y_{2i}$. For the replicate case, an interaction term $c$ is added to the model, with an associated variance component. All the random effects are assumed independent, and that all replicate measurements are assumed to be exchangeable within each method.

\begin{equation}
	y_{mir}  = \alpha_{m} + \mu_{i} + c_{mi} + e_{mir}, \qquad  e_{mi}
	\sim \mathcal{N}(0,\sigma^{2}_{m}), \quad c_{mi} \sim \mathcal{N}(0,\tau^{2}_{m}).
\end{equation}
%----

Of particular importance is terms of the model, a true value for item $i$ ($\mu_{i}$).  The fixed effect of Roy's model comprise of an intercept term and fixed effect terms for both methods, with no reference to the true value of any individual item. A distinction can be made between the two models: Roy's model is a standard LME model, whereas Carstensen's model is a more complex additive model.

\bigskip
\section{Two Way ANOVA}

\citet{BXC2008} develop their model from a standard two-way analysis of variance model, reformulated for the case of replicate measurements, with random effects terms specified as appropriate. 
Their model describing $y_{mir} $, again the $r$th replicate measurement on the $i$th item by the $m$th method ($m=1,2,$ $i=1,\ldots,N,$ and $r = 1,\ldots,n$), can be written as
\begin{equation}\label{BXC-model}
	y_{mir}  = \alpha_{m} + \mu_{i} + a_{ir} + c_{mi} + \epsilon_{mir}.
\end{equation}
The fixed effects $\alpha_{m}$ and $\mu_{i}$  represent the intercept for method $m$ and the `true value' for item $i$ respectively. The random-effect terms comprise an item-by-replicate interaction term $a_{ir} \sim \mathcal{N}(0,\varsigma^{2})$, a method-by-item interaction term $c_{mi} \sim \mathcal{N}(0,\tau^{2}_{m}),$ and model error terms $\varepsilon \sim \mathcal{N}(0,\varphi^{2}_{m}).$ All random-effect terms are assumed to be independent.
For the case when replicate measurements are assumed to be exchangeable for item $i$, $a_{ir}$ can be removed.

The model expressed in (2) describes measurements by $m$ methods, where $m = \{1,2,3\ldots\}$. Based on the model expressed in (2), \citet{BXC2008} compute the limits of agreement as
\[
\alpha_1 - \alpha_2 \pm 2 \sqrt{ \tau^2_1 +  \tau^2_2 +  \varphi^2_1 +  \varphi^2_2 }
\]
\citet{BXC2008} notes that, for $m=2$,  separate estimates of $\tau^2_m$ can not be obtained. To overcome this, the assumption of equality, i.e. $\tau^2_1 = \tau^2_2$ is required.


%----------------------------------------------------------------------------
\section{Statistical Model For Replicate Measurements}
Let $y_{Aij}$ and $y_{Bij}$ be the $j$th repeated observations of the variables of interest $A$ and $B$ taken on the $i$th subject. The number of repeated measurements for each variable may differ for each individual.
Both variables are measured on each time points. Let $n_{i}$ be the number of observations for each variable, hence $2\times n_{i}$ observations in total.

It is assumed that the pair $y_{Aij}$ and $y_{Bij}$ follow a bivariate normal distribution.
\begin{eqnarray}
	\left(
	\begin{array}{c}
		y_{Aij} \\
		y_{Bij} \\
	\end{array}
	\right) \sim \mathcal{N}(
	\boldsymbol{\mu}, \boldsymbol{\Sigma})\mbox{   where } \boldsymbol{\mu} = \left(
	\begin{array}{c}
		\mu_{A} \\
		\mu_{B} \\
	\end{array}
	\right)
\end{eqnarray}
The matrix $\boldsymbol{\Sigma}$ represents the variance component matrix between response variables at a given time point $j$.
\begin{equation}
	\boldsymbol{\Sigma} = \left( \begin{array}{cc}
		\sigma^2_{A} & \sigma_{AB} \\
		\sigma_{AB} & \sigma^2_{B}\\
	\end{array}\right)
\end{equation}
$\sigma^2_{A}$ is the variance of variable $A$, $\sigma^2_{B}$ is the variance of variable $B$ and $\sigma_{AB}$ is the covariance of the two variable. It is assumed that $\boldsymbol{\Sigma}$ does not depend on a particular time point, and is the same over all time points.

\section{Exchangeable and Linked measurements}
\section{Sampling Scheme : Linked and Unlinked Replicates}
Measurements taken in quick succession by the same observer using the same instrument on the same subject can be considered true replicates. \citet{ARoy2009} notes that some measurements may not be `true' replicates.

Roy's methodology assumes the use of `true replicates'. However data may not be collected in this way. In such cases, the correlation matrix on the replicates may require a different structure, such as the autoregressive order one $AR(1)$ structure. However determining MLEs with such a structure would be computational intense, if possible at all.



\emph{
	One important feature of replicate observations is that they should be independent
	of each other. In essence, this is achieved by ensuring that the observer makes each
	measurement independent of knowledge of the previous value(s). This may be difficult
	to achieve in practice.} (Check who said this
)
%----------------------------------------------------------------------------%


%-----------------------------------------------------------------------------------------------------%
\section{Replicate measurements}
\citet{ARoy2009} accords with Bland and Altman?s definition of a replicate, as being two or more measurements on the same individual under identical conditions.
Roy allows the assumption that replicated measurements are equi-correlated.
Roy allows unequal numbers of replicates.

Replicate measurements are linked over time. However the method can be easily extended to cover situations where they are not linked over time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this model , the variances of the random effects must depend on
$m$, since the different methods do not necessarily measure on the
same scale, and different methods naturally must be assumed to
have different variances. \citet{BXC2004} attends to the issue of
comparative variances.

\newpage
\citet{BA99} also remark that an important feature of replicate observations is that they should be independent
of each other. This issue is addressed by \citet{BXC2010}, in terms of exchangeability and linkage. Carstenen advises that repeated measurements come in two \emph{substantially different} forms, depending on the circumstances of their measurement: exchangable and linked.

Repeated measurements are said to be exchangeable if no relationship exists between successive measurements across measurements. If the condition of exchangeability exists, a group of measurement of the same item determined by the same method can be re-arranged in any permutation without prejudice to proper analysis. There is no reason to believe that the true value of the underlying variable has changed over the course of the measurements.

Exchangeable repeated measurements can be treated as true replicates. For the purposes of method comparison studies the following remarks can be made. The $r-$th measurement made by method $1$ has no special correspondence to the $r-$th measurement made by method $2$, and consequently any pairing of repeated measurements are as good as each other.


%----------------------------------------------------------------------------%

Repeated measurements are said to be linked if a direct correspondence exists between successive measurements across measurements, i.e. pairing. Such measurements are commonly made with a time interval between them, but simultaneously for both methods. Paired measurements are exchangeable, but individual measurements are not.

If the paired measurements are taken
in a short period of time so that no real systemic changes can take place on each item, they can be considered true replicates.
Should enough time elapse for systemic changes, linked repeated measurements can not be treated as true replicates.


\newpage
\bibliographystyle{chicago}
\bibliography{DB-txfrbib}
\end{document}

