
\documentclass[12pt, a4paper]{report}

\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage{framed}
\usepackage{subfiles}
\usepackage{amsthm, amsmath}
\usepackage{amsbsy}
\usepackage{framed}
\usepackage[usenames]{color}
\usepackage{listings}
\lstset{% general command to set parameter(s)
	basicstyle=\small, % print whole listing small
	keywordstyle=\color{red}\itshape,
	% underlined bold black keywords
	commentstyle=\color{blue}, % white comments
	stringstyle=\ttfamily, % typewriter type for strings
	showstringspaces=false,
	numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
	frame=shadowbox,
	rulesepcolor=\color{black},
	,columns=fullflexible
} %
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{1.0cm}{0.75cm}{18.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
%\voffset=-2.5cm
%\oddsidemargin=1cm
%\textwidth = 520pt

\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
	\author{Kevin O'Brien}
	\title{Mixed Models for Method Comparison Studies}
	\tableofcontents
	
	\chapter{Fitting MCS Models with R}

Roy (2009) proposes a suite of hypothesis tests for assessing the agreement of two methods of measurement, when replicate measurements are obtained for each item, using a LME approach. (An item would commonly be a patient).  Two methods of measurement can be said to be in agreement if there is no significant difference between in three key respects. Firstly, there is no inter-method bias between the two methods, i.e. there is no persistent tendency for one method to give higher values than the other.
Secondly, both methods of measurement have the same  within-subject variability. In such a case the variance of the replicate measurements would consistent for both methods.
Lastly, the methods have equal between-subject variability.  Put simply, for the mean measurements for each case, the variances of the mean measurements from both methods are equal.
Testing for Inter-method Bias
Firstly, a practitioner would investigate whether a significant inter-method bias is present between the methods. This bias is specified as a fixed effect in the LME model.  For a practitioner who has a reasonable level of competency in R and undergraduate statistics (in particular simple linear regression model) this is a straight-forward procedure.
	\section{LME models in method comparison studies}
	%With the greater computing power available for scientific
	%analysis, it is inevitable that complex models such as linear
	%mixed effects models should be applied to method comparison
	%studies.
	
	Linear mixed effects (LME) models can facilitate greater understanding of the potential causes of bias and differences in
	precision between two sets of measurement. \citet{LaiShiao} views the uses of linear mixed effects models as an expansion on the
	Bland-Altman methodology, rather than as a replacement.
	\citet{BXC2008} remarks that modern statistical computation, such as that used for LME models, greatly improve the efficiency of
	calculation compared to previous `by-hand' methods. 
	
	Roy provides three case studies, using data sets well known in method comparison studies, to demonstrate how the methodology should be used.
	
	
	\section*{Using Roy's Test to Identify cause of Lack of agreement}
	\citet{Barnhart}  describes the sources of disagreement as
	differing population means, different between-subject variances,
	different within-subject variances between two methods and poor
	correlation between measurements of two methods.
	
	
	Roy's method considers two methods to be in agreement if three
	conditions are met.
	
	\begin{itemize}
		\item no significant bias, i.e. the difference between the two
		mean readings is not "statistically significant",
		
		\item high overall correlation coefficient,
		
		\item the agreement between the two methods by testing their
		repeatability coefficients.
		
	\end{itemize}
	
	\citet{ARoy2009} demonstrates a LME model specification, and a series of tests that look at each of these agreement criteria individually. If two methods of measuement lack agreement, the specific reason or reasons for this lack of agreement can be identified.

	\citet{ARoy2009} considers four independent hypothesis tests.
	\begin{itemize}
		\item Testing of hypotheses of differences between the means of
		two methods\item Testing of hypotheses in between subject
		variabilities in two methods, \item Testing of hypotheses of
		differences in within-subject variability of the two methods,
		\item Testing of hypotheses in differences in overall variability
		of the two methods.
	\end{itemize}
	
	Bivariate correlation coefficients have been shown to be of
	limited use in method comparison studies \citep{BA86}. However,
	recently correlation analysis has been developed to cope with
	repeated measurements, enhancing their potential usefulness. Roy
	incorporates the use of correlation into his methodology.	

\section{Roy's Framework}

\citet{ARoy2009} uses an approach based on linear mixed effects (LME) models for the purpose of comparing the agreement between two methods of measurement, where replicate measurements on items, typically individuals, by both methods are available. She provides three tests of hypothesis appropriate for evaluating the agreement between the two methods of measurement under this sampling scheme. These tests consider null hypotheses that assume: absence of inter-method bias; equality of between-subject variabilities of the two methods; equality of within-subject variabilities of the two methods. By inter-method bias we mean that a systematic difference exists between observations recorded by the two methods. Differences in between-subject variabilities of the two methods arise when one method is yielding average response levels for individuals than are more variable than the average response levels for the same sample of individuals taken by the other method.  Differences in within-subject variabilities of the two methods arise when one method is yielding responses for an individual than are more variable than the responses for this same individual taken by the other method. The two methods of measurement can be considered to agree, and subsequently can be used interchangeably, if all three null hypotheses are true.
	
A formal test for inter-method bias can be implemented by examining the fixed effects of the model. This is common to well known classical linear model methodologies. The null hypotheses, that both methods have the same mean, which is tested against the alternative hypothesis, that both methods have different means.
	
The inter-method bias and necessary $t-$value and $p-$value are presented in computer output. A decision on whether the first of ARoy2009's criteria is fulfilled can be based on these values.
	
Importantly \citet{ARoy2009} further proposes a series of three tests on the variance components of an LME model, which allow decisions on the second and third of ARoy2009's criteria. For these tests, four candidate LME models are constructed. The differences in the models are specifically in how the the $D$ and $\Lambda$ matrices are constructed, using either an unstructured form or a compound symmetry form. To illustrate these differences, consider a generic matrix $A$,
	
	\[
	\boldsymbol{A} = \left( \begin{array}{cc}
	a_{11} & a_{12}  \\
	a_{21} & a_{22}  \\
	\end{array}\right).
	\]
	
A symmetric matrix allows the diagonal terms $a_{11}$ and $a_{22}$ to differ. The compound symmetry structure requires that both of these terms be equal, i.e $a_{11} = a_{22}$.
	
The first model acts as an alternative hypothesis to be compared against each of three other models, acting as null hypothesis models, successively. The models are compared using the likelihood ratio test. Likelihood ratio tests are a class of tests based on the comparison of the values of the likelihood functions of two candidate models. LRTs can be used to test hypotheses about covariance parameters or fixed effects parameters in the context of LMEs. The test statistic for the likelihood ratio test is the difference of the log-likelihood functions, multiplied by $-2$.
	
The probability distribution of the test statistic is approximated by the $\chi^2$ distribution with ($\nu_{1} - \nu_{2}$) degrees of freedom, where $\nu_{1}$ and $\nu_{2}$ are the degrees of freedom of models 1 and 2 respectively. Each of these three test shall be examined in more detail shortly.
		
For the purposes of comparing two methods of measurement, \citet{ARoy2009} presents a methodology utilizing linear mixed effects model. This methodology provides for the formal testing of inter-method bias, between-subject variability and within-subject variability of two methods. The formulation contains a Kronecker product covariance structure in a doubly multivariate setup. By doubly multivariate set up, Roy means that the information on each patient or item is multivariate at two levels, the number of methods and number of replicated measurements. Further to \citet{lam}, it is assumed that the replicates are linked over time. However it is easy to modify to the unlinked case.
	
\citet{ARoy2009} sets out three criteria for two methods to be considered in agreement. Firstly that there be no significant bias. Second that there is no difference in the between-subject variabilities, and lastly that there is no significant difference in the within-subject variabilities. Roy further proposes examination of the the overall variability by considering the second and third criteria be examined jointly. Should both the second and third criteria be fulfilled, then the overall variabilities of both methods would be equal.	

\section{Systolic Blood Pressure Data Set}

Roy provides three case studies, using data sets well known in method comparison studies, to demonstrate how the methodology should be used. The first two examples used are from the `blood pressure' data set introduced by \citet{BA99}. The data set is a tabulation of simultaneous measurements of systolic blood pressure were made by each of two experienced observers (denoted `\textit{J}' and `\textit{R}') using a sphygmomanometer and by a semi-automatic blood pressure monitor (denoted `\textit{S}'). Three sets of readings were made in quick succession. Roy compares the `\textit{J}' and `\textit{S}' methods in the first of her examples.

\section{Implementation in \texttt{R}}
To implement an LME model in \texttt{R}, the \texttt{nlme} package is used. This package is loaded into the \texttt{R} environment using the library command, (i.e.\ \texttt{library(nlme)}). The \texttt{lme} command is used to fit LME models. The first two arguments to the \texttt{lme} function specify the fixed effect component of the model, and the data set to which the model is to be fitted. The first candidate model (`\texttt{ref.nlme}') fits an LME model on the data set `Blood'. The variable `\texttt{meth}' is assigned as the fixed effect, with the response variable `\texttt{y}' (i.e. blood pressure).
			
The third argument contain the random effects component of the formulation, describing the random effects, and their grouping structure. The \texttt{nlme} package provides a set of positive-definite matrices , the \texttt{pdMat} class, that can be used to specify a structure for the between-subject variance-covariance matrix for the random effects. For Roy's models, we will use the \texttt{pdSymm} and \texttt{pdCompSymm} to specify a symmetric structure and a compound symmetry structure respectively. A full discussion of these structures can be found in \citet[pg. 158]{PB}.
			
Similarly a variety of structures for the with-subject variance-covariance matrix can be implemented using \texttt{nlme}. To implement a particular matrix structure, one must specify both a variance function and correlation structure accordingly. Variance functions are used to model the variance structure of the within-subject errors. \texttt{varIdent} is a variance function object used to allow different variances according to the levels of a classification factor in the data. A compound symmetry structure is implemented using the \texttt{corCompSymm} class, while the symmetric form is specified by \texttt{corSymm} class. Finally, the estimation methods is specified as ``ML" or ``REML".

	\begin{description}
		\item[\texttt{y}] : Response variable
		\item[\texttt{meth}] : Method of Measurement
		\item[\texttt{item}] : Subject
		\item[\texttt{repl}] 
	\end{description}

\section{Computation of LMEs using R} 

\cite{PB} advises how to
implement LME models in statistical software (ostensibly for S and
S PLUS, but \texttt{R} is very similar). When tackling linear mixed effects
models using the R language, a statistician can call upon the
\emph{lme} command found in the \emph{nlme} package.This command
fits a LME model to the data set using either Maximum Likelihood
(ML) or Restricted Maximum Likelihood (REML). ML may be referred
to as 'full maximum likelihood' estimation.

The first two arguments for \emph{lme} are \emph{fixed} and
\emph{data}, which give the model for the expected responses (i.e.
the fixed part of the model), and the data that themodel should be
fitted from. The next argument is  \emph{random}, a one-sided
formula which describes the random effects, and the grouping
structure for the model. The  \emph{method} argument can specify
whether to use 'REML', the default setting, or 'ML'.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% MODEL SELECTION



	\section{Important Consideration for MCS}
	
	The key issue is that \texttt{nlme} allows for the particular specification of Roy's Model, specifically direct specification of the VC matrices for within subject and between subject residuals.
	
	The \texttt{lme4} package does not allow for Roy's Model, for reasons that will identified shortly.
	To advance the ideas that eminate from Roy's paper, one is required to use the \texttt{nlme} context. However, to take advantage of the infrastructure already provided for \texttt{lme4} models, one may change the research question away from that of Roy's paper. 
	To this end, an exploration of what \textbf{\textit{influence.ME}} can accomplished is merited.

	The first of Roy's candidate model can be implemented using the following code;\\
	
	\begin{framed}
		\begin{verbatim}
		ref.nlme = lme(BP ~ method-1, data = dat,
		random =  list(subject=pdSymm(~ method-1)),
		weights=varIdent(form=~1|method),
		correlation = corSymm(form=~1 | subject/obs), method="ML")
		\end{verbatim}
	\end{framed}
	
	For the blood pressure data used in \citet{ARoy2009}, all four candidate models are implemented by slight variations of this piece of code, specifying either \texttt{pdSymm} or \texttt{pdCompSymm} in the second line, and either \texttt{corSymm} or \texttt{corCompSymm} in the fourth line.
	For example, the second candidate model `MCS2' is implemented with the same code as MCS1, except for the term \texttt{pdCompSymm} in the second line, rather than \texttt{pdSymm}.
	
	\begin{framed}
		\begin{verbatim}
	    test1.nlme = lme(BP ~ method-1, data = dat,
		random = list(subject=pdCompSymm(~ method-1)),
		weights = varIdent(form=~1|method),
		correlation = corSymm(form=~1 | subject/obs), method="ML")
		\end{verbatim}
	\end{framed}
	
	Using this \texttt{R} implementation for other data sets requires that the data set is structured appropriately (i.e.\ each case of observation records the index, response, method and replicate). Once formatted properly, implementation is simply a case of re-writing the first line of code, and computing the four candidate models accordingly.
	
	

	The fixed effects estimates are the same for all four candidate models. The inter-method bias can be easily determined by inspecting a summary of any model. The summary presents estimates for all of the important parameters, but not the complete variance-covariance matrices (although some simple \texttt{R} functions can be written to overcome this). The variance estimates for the random effects for MCS2 is presented below.
	
	\begin{framed}
		\begin{verbatim}
		Random effects:
		Formula: ~method - 1 | subject
		Structure: Compound Symmetry
		StdDev Corr
		methodJ  30.765
		methodS  30.765 0.829
		Residual  6.115
		\end{verbatim}
	\end{framed}
	\vspace{1cm}
	Similarly, for computing the limits of agreement the standard deviation of the differences is not explicitly given. Again, A simple \texttt{R} function can be written to calculate the limits of agreement directly.
	

	\section{Fitting Models with the LME4 R package}
	Two LME models are fitted to the data, one using the nlme package, one with the lme4 package. These models shall be called ``\texttt{blood.nlme}" and ``\texttt{blood.lme4}" respectively.
	In both cases the method is characterized by a fixed effect, while there is a random effect for each subject.
	This random effect accounts for the replicate measurements associated with each subject.
	The differences between the estimate provided by the respective models are negligible, due to the simplicity of the model specification.
	
	
	Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the \texttt{lmer} function in the \textbf{\textit{lme4}} package for \texttt{R}. As for most model-fitting functions in \texttt{R}, the model is described in an \texttt{lmer} call by a formula, in this case including both fixed- and random-effects terms. 
	
	The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in \texttt{R}, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. 
	
	Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.
	
	

\subsection{Demonstration of Roy's testing}

	
	The inter-method bias between the two method is found to be $15.62$ , with a $t-$value of $-7.64$, with a $p-$value of less than $0.0001$. Consequently there is a significant inter-method bias present between methods $J$ and $S$, and the first of the Roy's three agreement criteria is unfulfilled.
	
	Next, the first variability test is carried out, yielding maximum likelihood estimates of the between-subject variance covariance matrix, for both the null model, in compound symmetry (CS) form, and the alternative model in symmetric (symm) form. These matrices are determined to be as follows;
	\[
	\boldsymbol{\hat{D}}_{CS} = \left( \begin{array}{cc}
	946.50 & 784.32  \\
	784.32 & 946.50  \\
	\end{array}\right),
	\hspace{1.5cm}
	\boldsymbol{\hat{D}}_{Symm} = \left( \begin{array}{cc}
	923.98 & 785.24  \\
	785.24 & 971.30  \\
	\end{array}\right).
	\]
	
	A likelihood ratio test is perform to compare both candidate models. The log-likelihood of the null model is $-2030.7$, and for the alternative model $-2030.8$. The test statistic, presented with greater precision than the log-likelihoods, is $0.1592$. The $p-$value is $0.6958$. Consequently we fail to reject the null model, and by extension, conclude that the hypothesis that methods $J$ and $S$ have the same between-subject variability. Thus the second of the criteria is fulfilled.
	
	The second variability test determines maximum likelihood estimates of the within-subject variance covariance matrix, for both the reference model, in CS form, and the alternative model in symmetric form.
	
	\[
	\boldsymbol{\hat{\Sigma}_{CS}} = \left( \begin{array}{cc}
	60.27  & 16.06  \\
	16.06  & 60.27  \\
	\end{array}\right),
	\hspace{1.5cm}
	\boldsymbol{\hat{\Sigma}}_{Symm} = \left( \begin{array}{cc}
	37.40 & 16.06  \\
	16.06 & 83.14  \\
	\end{array}\right).
	\]
	
A likelihood ratio test is perform to compare both candidate models. The log-likelihood of the alternative model model is $-2045.0$. As before, the null model has a log-likelihood of $-2030.7$. The test statistic is computed as $28.617$, again presented with greater precision. The $p-$value is less than $0.0001$. In this case we reject the null hypothesis of equal within-subject variability. Consequently the third of Roy's criteria is unfulfilled.
The coefficient of repeatability for methods $J$ and $S$ are found to be 16.95 mmHg and 25.28 mmHg respectively.

The last of the three variability tests is carried out to compare the overall variabilities of both methods.
With the null model the MLE of the within-subject variance covariance matrix is given below. The overall variabilities for the null and alternative models, respectively, are determined to be as follows;
\[
\boldsymbol{\hat{\Sigma}}_{CS} = \left( \begin{array}{cc}
1007.92  & 801.65  \\
801.65  & 1007.92  \\
\end{array}\right),
\hspace{1.5cm}
\boldsymbol{\hat{\Sigma}}_{Symm} = \left( \begin{array}{cc}
961.38 & 801.40  \\
801.40 & 1054.43  \\
\end{array}\right),
\]
	
The log-likelihood of the alternative model model is $-2045.2$, and again, the null model has a log-likelihood of $-2030.7$. The test statistic is $28.884$, and the $p-$value is less than $0.0001$. The null hypothesis, that both methods have equal overall variability, is rejected. Further to the second variability test, it is known that this difference is specifically due to the difference of within-subject variabilities.

Lastly, Roy considers the overall correlation coefficient. The diagonal blocks $\boldsymbol{\hat{r}_{\Omega}}_{ii}$ of the correlation matrix indicate an overall coefficient of $0.7959$. This is less than the threshold of 0.82 that Roy recommends.


\begin{equation}
\boldsymbol{\hat{r}_{\Omega}}_{ii} = \left( \begin{array}{cc}
1  & 0.7959  \\
0.7959  & 1  \\
\end{array}\right)
\end{equation}
	
	The off-diagonal blocks of the overall correlation matrix $\boldsymbol{\hat{r}_{\Omega}}_{ii'}$ present the correlation coefficients further to \citet{hamlett}.
	\[
	\boldsymbol{\hat{r}_{\Omega}}_{ii'} = \left( \begin{array}{cc}
	0.9611  & 0.7799  \\
	0.7799  & 0.9212  \\
	\end{array}\right).
	\]
	
The overall conclusion of the procedure is that method $J$ and $S$ are not in agreement, specifically due to the within-subject variability, and the inter-method bias. The repeatability coefficients are substantially different, with the coefficient for method $S$ being 49\% larger than for method $J$. Additionally the overall correlation coefficient did not exceed the recommended threshold of $0.82$.
	
%--------------------------------------------------%
\chapter{Roy's Model}
	\citet{ARoy2009} proposes an LME model with Kronecker product covariance structure in a doubly multivariate setup. Response for $i$th subject can be written as
	\[ y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + b_{1i}z_{i1}  + b_{2i}z_{i2} + \epsilon_i \]
	\begin{itemize}
		\item $\beta_1$ and $\beta_2$ are fixed effects corresponding to both methods. ($\beta_0$ is the intercept.)
		\item $b_{1i}$ and $b_{2i}$ are random effects corresponding to both methods.
	\end{itemize}
	
	Overall variability between the two methods ($\Omega$) is sum of between-subject ($D$) and within-subject variability ($\Sigma$),
	\[
	\mbox{Block } \boldsymbol{\Omega}_i = \left[ \begin{array}{cc} d^2_1 & d_{12}\\ d_{12} & d^2_2\\ \end{array} \right]
	+ \left[\begin{array}{cc} \sigma^2_1 & \sigma_{12}\\ \sigma_{12} & \sigma^2_2\\ \end{array}\right].
	\]
	%============================================== %
	%- F:
	
	

	\section{Model Terms for Roy's Techniques}
$\boldsymbol{b}_{i}$ is a $m-$dimensional vector comprised of
		the random effects.
		\begin{equation}
		\boldsymbol{b}_{i} = \left( \begin{array}{c}
		b_{1i} \\
		b_{21}  \\
		\end{array}\right)
		\end{equation}
		
$\boldsymbol{V}$ represents the correlation matrix of the replicated measurements on a given method.
		$\boldsymbol{\Sigma}$ is the within-subject VC matrix.
		
$\boldsymbol{V}$ and $\boldsymbol{\Sigma}$ are positive
		definite matrices. The dimensions of $\boldsymbol{V}$ and
		$\boldsymbol{\Sigma}$ are $3 \times 3 ( = p \times p )$ and $ 2 \times
		2 (= k \times k)$.
		
It is assumed that $\boldsymbol{V}$ is the same for both methods and $\boldsymbol{\Sigma}$ is
		the same for all replications.
$\boldsymbol{V} \bigotimes \boldsymbol{\Sigma}$ creates a $ 6 \times 6 ( = kp \times
		kp)$ matrix.
		$\boldsymbol{R}_{i}$ is a sub-matrix of this.

\section{Model terms}
It is important to note the following characteristics of this model.
Let the number of replicate measurements on each item $i$ for both methods be $n_i$, hence $2 \times n_i$ responses. However, it is assumed that there may be a different number of replicates made for different items. Let the maximum number of replicates be $p$. An item will have up to $2p$ measurements, i.e. $\max(n_{i}) = 2p$.
		
		% \item $\boldsymbol{y}_i$ is the $2n_i \times 1$ response vector for measurements on the $i-$th item.
		% \item $\boldsymbol{X}_i$ is the $2n_i \times  3$ model matrix for the fixed effects for observations on item $i$.
		% \item $\boldsymbol{\beta}$ is the $3 \times  1$ vector of fixed-effect coefficients, one for the true value for item $i$, and one effect each for both methods.
Later on $\boldsymbol{X}_i$ will be reduced to a $2 \times 1$ matrix, to allow estimation of terms. This is due to a shortage of rank. The fixed effects vector can be modified accordingly.
 $\boldsymbol{Z}_i$ is the $2n_i \times  2$ model matrix for the random effects for measurement methods on item $i$.
 $\boldsymbol{b}_i$ is the $2 \times  1$ vector of random-effect coefficients on item $i$, one for each method.
 $\boldsymbol{\epsilon}$  is the $2n_i \times  1$ vector of residuals for measurements on item $i$.
 $\boldsymbol{G}$ is the $2 \times  2$ covariance matrix for the random effects.
 $\boldsymbol{R}_i$ is the $2n_i \times  2n_i$ covariance matrix for the residuals on item $i$.
The expected value is given as $\mbox{E}(\boldsymbol{y}_i) = \boldsymbol{X}_i\boldsymbol{\beta}.$ \citep{hamlett}
 The variance of the response vector is given by $\mbox{Var}(\boldsymbol{y}_i)  = \boldsymbol{Z}_i \boldsymbol{G} \boldsymbol{Z}_i^{\prime} + \boldsymbol{R}_i$ \citep{hamlett}.

	
	

	\section{Introduction}
	


\subsection{Roy's Variability Tests}
	Variability tests proposed by \citet{ARoy2009} affords the opportunity to expand upon Carstensen's approach.
	
	The first test allows of the comparison the begin-subject variability of two methods. Similarly, the second test
	assesses the within-subject variability of two methods. A third test is a test that compares the overall variability of the two methods.
	
	The tests are implemented by fitting a specific LME model, and three variations thereof, to the data. These three variant models introduce equality constraints that act null hypothesis cases.
	
	Other important aspects of the method comparison study are consequent. The limits of agreement are computed using the results of the first model.

	
	\citet{ARoy2009} proposes a LME based approach with Kronecker
	product covariance structure with doubly multivariate setup to
	assess the agreement between two methods. This method is designed
	such that the data may be unbalanced and with unequal numbers of
	replications for each subject.
	
	The maximum likelihood estimate of the between-subject variance
	covariance matrix of two methods is given as $D$. The estimate for
	the within-subject variance covariance matrix is $\hat{\Sigma}$.
	The estimated overall variance covariance matrix `Block
	$\Omega_{i}$' is the addition of $\hat{D}$ and $\hat{\Sigma}$.
	
	
	\begin{equation}
	\mbox{Block  }\Omega_{i} = \hat{G} + \hat{\Sigma}
	\end{equation}

	

	





		
	\section{Roy's Hypotheses Tests}
	
	In order to express Roy's LME model in matrix notation we gather all $2n_i$ observations specific to item $i$ into a single vector  $\boldsymbol{y}_{i} = (y_{1i1},y_{2i1},y_{1i2},\ldots,y_{mir},\ldots,y_{1in_{i}},y_{2in_{i}})^\prime.$ The LME model can be written
	\[
	\boldsymbol{y_{i}} = \boldsymbol{X_{i}\beta} + \boldsymbol{Z_{i}b_{i}} + \boldsymbol{\epsilon_{i}},
	\]
	where $\boldsymbol{\beta}=(\beta_0,\beta_1,\beta_2)^\prime$ is a vector of fixed effects, and $\boldsymbol{X}_i$ is a corresponding $2n_i\times 3$ design matrix for the fixed effects. The random effects are expressed in the vector $\boldsymbol{b}=(b_1,b_2)^\prime$, with $\boldsymbol{Z}_i$ the corresponding $2n_i\times 2$ design matrix. The vector $\boldsymbol{\epsilon}_i$ is a $2n_i\times 1$ vector of residual terms.
	
	It is assumed that $\boldsymbol{b}_i \sim N(0,\boldsymbol{G})$, $\boldsymbol{\epsilon}_i$ is a matrix of random errors distributed as $N(0,\boldsymbol{R}_i)$ and that the random effects and residuals are independent of each other.
	% Assumptions made on the structures of $\boldsymbol{G}$ and $\boldsymbol{R}_i$ will be discussed in due course.
	
	% \texttt{finish}
	
	$\boldsymbol{G}$ is the variance covariance matrix for the random effects $\boldsymbol{b}$.
	i.e. between-item sources of variation. The between-item variance covariance matrix $\boldsymbol{G}$ is constructed as follows:
	
	\[ \mbox{Var}  \left[
	\begin{array}{c}
	b_1   \\
	b_2  \\
	\end{array}
	\right] =  \boldsymbol{G} =\left(
	\begin{array}{cc}
	g^2_1  & g_{12} \\
	g_{12} & g^2_2 \\
	\end{array}
	\right) \]
	It is important to note that no special assumptions about the structure of $\boldsymbol{G}$ are made. An example of such an assumption would be that $\boldsymbol{G}$ is the product of a scalar value and the identity matrix.
	
	$\boldsymbol{R}_{i}$ is the variance covariance matrix for the residuals, i.e. the within-item sources of variation between both methods. Computational analysis of linear mixed effects models allow for the explicit analysis of both $\boldsymbol{G}$ and $\boldsymbol{R_i}$.
	

	
	
	
	The overall variability between
	the two methods is the sum of between-item variability
	$\boldsymbol{G}$ and within-item variability
	$\boldsymbol{\Sigma}$. \citet{ARoy2009} denotes the overall variability
	as ${\mbox{Block - }\boldsymbol \Omega_{i}}$. The overall
	variation for methods $1$ and $2$ are given by
	
	\begin{center}
		\[\left(\begin{array}{cc}
		\omega^2_1  & \omega_{12} \\
		\omega_{12} & \omega^2_2 \\
		\end{array}  \right)
		=  \left(
		\begin{array}{cc}
		g^2_1  & g_{12} \\
		g_{12} & g^2_2 \\
		\end{array} \right)+
		\left(
		\begin{array}{cc}
		\sigma^2_1  & \sigma_{12} \\
		\sigma_{12} & \sigma^2_2 \\
		\end{array}\right)
		\]
	\end{center}
	The computation of the limits of agreement require that the variance of the difference of measurements. This variance is easily computable from the estimate of the ${\mbox{Block - }\boldsymbol \Omega_{i}}$ matrix. Lack of agreement can arise if there is a disagreement in overall variabilities. This may be due to due to the disagreement in either between-item
	variabilities or within-item variabilities, or both. \citet{ARoy2009} allows for a formal test of each.
	
%========================================================%

The first of Roy's candidate model can be implemented using the following code;\\

\begin{verbatim}
MCS1 = lme(BP ~ method-1, data = dat,
random =  list(subject=pdSymm(~ method-1)),
weights=varIdent(form=~1|method),
correlation = corSymm(form=~1 | subject/obs), method="ML")
\end{verbatim}
\hrule
\vspace{1cm}
For the blood pressure data used in \citet{ARoy2009}, all four candidate models are implemented by slight variations of this piece of code, specifying either \texttt{pdSymm} or \texttt{pdCompSymm} in the second line, and either \texttt{corSymm} or \texttt{corCompSymm} in the fourth line.
For example, the second candidate model `MCS2' is implemented with the same code as MCS1, except for the term \texttt{pdCompSymm} in the second line, rather than \texttt{pdSymm}.

\begin{verbatim}
MCS2 = lme(BP ~ method-1, data = dat,
random = list(subject=pdCompSymm(~ method-1)),
weights = varIdent(form=~1|method),
correlation = corSymm(form=~1 | subject/obs), method="ML")
\end{verbatim}

\vspace{1cm}
Using this \texttt{R} implementation for other data sets requires that the data set is structured appropriately (i.e.\ each case of observation records the index, response, method and replicate). Once formatted properly, implementation is simply a case of re-writing the first line of code, and computing the four candidate models accordingly.

To perform a likelihood ratio test for two candidate models, simply use the \texttt{anova} command with the names of the candidate models as arguments. The following piece of code implement the first of Roy's variability tests.


\begin{verbatim}
> anova(MCS1,MCS2)
Model df    AIC    BIC  logLik   Test L.Ratio p-value
MCS1     1  8 4077.5 4111.3 -2030.7
MCS2     2  7 4075.6 4105.3 -2030.8 1 vs 2 0.15291  0.6958
>
\end{verbatim}

The fixed effects estimates are the same for all four candidate models. The inter-method bias can be easily determined by inspecting a summary of any model. The summary presents estimates for all of the important parameters, but not the complete variance-covariance matrices (although some simple \texttt{R} functions can be written to overcome this). The variance estimates for the random effects for MCS2 is presented below.

\begin{verbatim}
Random effects:
Formula: ~method - 1 | subject
Structure: Compound Symmetry
StdDev Corr
methodJ  30.765
methodS  30.765 0.829
Residual  6.115
\end{verbatim}

Similarly, for computing the limits of agreement the standard deviation of the differences is not explicitly given. Again, A simple \texttt{R} function can be written to calculate the limits of agreement directly.

%------------------------------------------------------------------------------------%


	
	


	\section{Basic Models Fits}
	Further to \citet{PB}, several simple LME models are constructed
	for the blood pressure data. This data set is the subject of a
	method comparison study in \citet{BA99}.
	
\subsection{Roy's Reference Model}
Conventionally LME models can be tested using Likelihood Ratio Tests, wherein a reference model is compared to a nested model.


\citet{ARoy2009} presents two nested models that specify the condition of equality as required, with a third nested model for an additional test. There three formulations share the same structure, and can be specified by making slight alterations of the code for the Reference Model.

\subsection{Model Fit 1}
	
	This is a simple model with no interactions. There is a fixed effect for each method and a random effect for each subject.
	\begin{equation*}
	y_{ijk} = \beta_{j}  + b_{i} + \epsilon_{ijk}, \qquad i=1,\dots,2, j=1,\dots,85, k=1,\dots,3
	\end{equation*}
	
	\begin{eqnarray*}
		b_{i} \sim \mathcal{N}(0,\sigma^2_{b}), \qquad \epsilon_{i} \sim \mathcal{N}(0,\sigma^2)
	\end{eqnarray*}
	\begin{framed}
		\begin{verbatim}
		> Ref.Fit = lme(y ~ meth-1, data = dat,   #Symm , Symm#
		+     random = list(item=pdSymm(~ meth-1)), 
		+     weights=varIdent(form=~1|meth),
		+     correlation = corSymm(form=~1 | item/repl), 
		+     method="ML")
		\end{verbatim}
	\end{framed}
		
	\begin{verbatim}
	Linear mixed-effects model fit by REML
	Data: dat
	Log-restricted-likelihood: -2155.853
	Fixed: BP ~ method
	(Intercept)     methodS
	127.40784    15.61961
	
	Random effects:
	Formula: ~1 | subject
	(Intercept) Residual
	StdDev:    29.39085 12.44454
	
	Number of Observations: 510
	Number of Groups: 85
	\end{verbatim}
	
	The following output was obtained.

	\begin{verbatim}
	Linear mixed-effects model fit by REML
	Data: dat
	Log-restricted-likelihood: -2047.714
	Fixed: BP ~ method
	(Intercept)     methodS
	127.40784    15.61961
	
	Random effects:
	Formula: ~1 | subject
	(Intercept)
	StdDev:    28.28452
	
	Formula: ~1 | method %in% subject
	(Intercept) Residual
	StdDev:    12.61562 7.763666
	
	Number of Observations: 510
	Number of Groups:
	subject method %in% subject
	85                 170
	\end{verbatim}
	
Nested Model (Between-Item Variability)
\begin{framed}
	\begin{verbatim}
	> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
	+     random = list(item=pdCompSymm(~ meth-1)),
	+     correlation = corSymm(form=~1 | item/repl), 
	+     method="ML")
	\end{verbatim}
\end{framed}
	\subsection{Variability test 1}
	This is a test on whether both methods $A$ and $B$ have the same between-subject variability or not.
	\begin{eqnarray}
	H_{0}: \mbox{ }d_{A}  = d_{B} \\
	H_{A}: \mbox{ }d_{A}  \neq d_{B}
	\end{eqnarray}
	When implemented using \texttt{R}, this test is facilitated by constructing a model specifying a symmetric form for $D$ (i.e. the alternative model) and comparing it with a model that has compound symmetric form for $D$ (i.e. the null model). For this test $\boldsymbol{\hat{\Sigma}}$ has a symmetric form for both models, and will be the same for both.
	
	%--------------------------------------------------%
	\subsubsection{Bland-Altman's blood data}
	With the alternative model, the MLE of the between-subject variance covariance matrix is given by
	\begin{equation}
	\boldsymbol{\hat{G}_{Symm}} = \left( \begin{array}{cc}
	923.98 & 785.24  \\
	785.24 & 971.30  \\
	\end{array}\right)
	\end{equation}
	
	With the refence model the MLE is as follows:
	
	\begin{equation}
	\boldsymbol{\hat{G}_{CS}} = \left( \begin{array}{cc}
	946.50 & 784.32  \\
	784.32 & 946.50  \\
	\end{array}\right)
	\end{equation}
	A likelihood ratio test is perform to determine which model is more suitable. The outcome of this test is presented in the following \texttt{R} code.
	\begin{framed}
	\begin{verbatim}
	> anova(MCS1,MCS2)
	>
	>
	Model df    AIC    BIC  logLik   Test L.Ratio p-value
	MCS1     1  8 4077.5 4111.3 -2030.7
	MCS2     2  7 4075.6 4105.3 -2030.8 1 vs 2 0.15291  0.6958
	\end{verbatim}
	\end{framed}
	
	The test statistic is the difference of the $-2$ log likelihoods; $0.153$. The $p-$value is $0.696$. Therefore we fail to reject the hypothesis that both have the same between-subject variabilities.
	
	%---------------------------------------------%
	\subsection{Model Fit 2}
\subsubsection{Nested Model (Between-Item Variability)}
\begin{framed}
	\begin{verbatim}
	> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
	+     random = list(item=pdCompSymm(~ meth-1)),
	+     correlation = corSymm(form=~1 | item/repl), 
	+     method="ML")
	\end{verbatim}
\end{framed}

	

	This is a simple model, this time with an interaction effect.
	There is a fixed effect for each method. This model has random effects at two levels $b_{i}$ for the subject, and
	another, $b_{ij}$, for the respective method within each subject.
	\begin{equation*}
	y_{ijk} = \beta_{j}  + b_{i} + b_{ij} + \epsilon_{ijk}, \qquad i=1,\dots,2, j=1,\dots,85, k=1,\dots,3
	\end{equation*}
	\begin{eqnarray*}
		b_{i} \sim \mathcal{N}(0,\sigma^2_{1}), \qquad b_{ij} \sim \mathcal{N}(0,\sigma^2_{2}), \qquad \epsilon_{i} \sim \mathcal{N}(0,\sigma^2)
	\end{eqnarray*}
	
	In this model, the random interaction terms all have the same variance $\sigma^2_{2}$. These terms are assumed to be independent of each other, even
	within the same subject.
	
	\begin{verbatim}
	Linear mixed-effects model fit by REML
	Data: dat
	Log-restricted-likelihood: -2047.714
	Fixed: BP ~ method
	(Intercept)     methodS
	127.40784    15.61961
	
	Random effects:
	Formula: ~1 | subject
	(Intercept)
	StdDev:    28.28452
	
	Formula: ~1 | method %in% subject
	(Intercept) Residual
	StdDev:    12.61562 7.763666
	
	Number of Observations: 510
	Number of Groups:
	subject method %in% subject
	85                 170
	\end{verbatim}

\subsection{Variability test 2}
	
	This is a test on whether both methods $A$ and $B$ have the same within-subject variability or not.
	
	\begin{eqnarray}
	H_{0}: \mbox{ }\lambda_{A}  = \lambda_{B} \\
	H_{A}: \mbox{ }\lambda_{A}  = \lambda_{B}
	\end{eqnarray}
	
	This model is performed in the same manner as the first test, only reversing the roles of $\boldsymbol{\hat{G}}$ and $\boldsymbol{\hat{\Sigma}}$. The null model is constructed  a symmetric form for $\boldsymbol{\hat{\Sigma}}$ while the alternative model uses a compound symmetry form. This time $\boldsymbol{\hat{G}}$ has a symmetric form for both models, and will be the same for both.
	
	
	\subsubsection{Bland-Altman's Blood Data}
	For the null model the MLE of the within-subject variance covariance matrix is given below.
	
	\begin{equation}
	\boldsymbol{\hat{\Lambda}_{Symm}} = \left( \begin{array}{cc}
	37.40 & 16.06  \\
	16.06 & 83.14  \\
	\end{array}\right)
	\end{equation}
	With the alternative model the MLE is as follows:
	
	\begin{equation}
	\boldsymbol{\hat{\Lambda}_{CS}} = \left( \begin{array}{cc}
	60.27  & 16.06  \\
	16.06  & 60.27  \\
	\end{array}\right)
	\end{equation}
	
	A likelihood ratio test is perform to determine which model is more suitable.
	The outcome of this test is that it can be assumed that they have equal
	The test statistic is the difference of the $-2$ log likelihoods; $28.617$. The $p-$value is less than $0.0001$. In this case we reject the null hypothesis that both models have the same within-subject variabilities.
	
	%-----------------------------------------------%
	\subsection{Model Fit 3}
	
	This model is a more general model, compared to 'model fit 2'. This model treats the random interactions for each subject as a vector and
	allows the variance-covariance matrix for that vector to be estimated from the set of all positive-definite matrices.
	$\boldsymbol{y_{i}}$ is the entire response vector for the $i$th subject.
	$\boldsymbol{X_{i}}$ and $\boldsymbol{Z_{i}}$  are the fixed- and random-effects design matrices respectively.
	\begin{equation*}
	\boldsymbol{y_{i}} = \boldsymbol{X_{i}\beta}  + \boldsymbol{Z_{i}b_{i}} + \boldsymbol{\epsilon_{i}}, \qquad i=1,\dots,85
	\end{equation*}
	\begin{eqnarray*}
		\boldsymbol{Z_{i}} \sim \mathcal{N}(\boldsymbol{0,\Psi}),\qquad
		\boldsymbol{\epsilon_{i}} \sim \mathcal{N}(\boldsymbol{0,\sigma^2\Lambda})
	\end{eqnarray*}
	

	\subsection{Variability Test 3}
	This is a test on whether both methods $A$ and $B$ have the same overall variability or not.
	\begin{eqnarray}
	H_{0}: \mbox{ }\sigma_{A}  = \sigma_{B} \\
	H_{A}: \mbox{ }\sigma_{A}  = \sigma_{B}
	\end{eqnarray}
	
	The null model is constructed a symmetric form for both $\boldsymbol{\hat{D}}$ and $\boldsymbol{\hat{\Lambda}}$ while the alternative model uses a compound symmetry form for both.
	
	\subsubsection{Bland-Altman's Blood Data}
	With the null model the MLE of the within-subject variance covariance matrix is given below.
	
	\begin{equation}
	\boldsymbol{\hat{\Sigma}_{Symm}} = \left( \begin{array}{cc}
	961.38 & 801.40  \\
	801.40 & 1054.43  \\
	\end{array}\right)
	\end{equation}
	
	With the alternative model the MLE is as follows:
	\begin{equation}
	\boldsymbol{\hat{\Sigma}_{CS}} = \left( \begin{array}{cc}
	1007.92  & 801.65  \\
	801.65  & 1007.92  \\
	\end{array}\right)
	\end{equation}
	
	Again a likelihood ratio test is used to determine the most suitable of the two candidate models.
	The test statistic is the difference of the $-2$ log likelihoods; $28.884$. The $p-$value is less than $0.0001$. We again reject the null hypothesis. Each model has a different overall variability, a foregone conclusion from the second variability test.
	
	
	
	\subsection{Nested Model (Overall Variability)}
	Additionally there is a third nested model, that can be used to test overall variability, substantively a a joint test for between-item and within-item variability. The motivation for including such a test in the suite is not clear, although it does circumvent the need for multiple comparison procedures in certain circumstances, hence providing a simplified procedure for non-statisticians.
	
	\begin{framed}
		\begin{verbatim}
		> NMO.fit = lme(y ~ meth-1, data = dat,   #CS , CS# 
		+     random = list(item=pdCompSymm(~ meth-1)), 
		+     correlation = corCompSymm(form=~1 | item/repl), 
		+     method="ML")
		\end{verbatim}
	\end{framed}

	\section{Likelihood Ratio Test}
	A general method for comparing nested models fit by maximum liklihood is the liklihood ratio 
	test. This test can be used for models fit by REML (restricted maximum liklihood), but only if the 
	fixed terms in the two models are invariant, and both models have been fit by REML. Otherwise, 
	the argument: method=”ML” must be employed (ML = maximum liklihood). 
	
	Example of a liklihood ratio test used to compare two models: 
	
	
	
	The output will contain a p-value, and this should be used in conjunction with the AIC scores to 
	judge which model is preferred. Lower AIC scores are better. 
	
	Generally, liklihood ratio tests should be used to evaluate the significance of terms on the 
	random effects portion of two nested models, and should not be used to determine the 
	significance of the fixed effects. 
	
	A simple way to more reliably test for the significance of fixed effects in an LME model is to use 
	conditional F-tests, as implemented with the simple “anova” function. 
	

	
	
	
	
	
\subsection{LRTs with \texttt{R}}
Likelihood ratio tests are very simple to implement in \texttt{R}, simply use the `\texttt{anova()}'
commands. Sample output will be given for each variability test. The likelihood ratio
test is the procedure used to compare the fit of two models. For each candidate model,
the `-2 log likelihood' (M2LL) is computed. The test statistic for each of the three
hypothesis tests is the difference of the M2LL for each pair of models. If the p-value
in each of the respective tests exceed as significance level chosen by the analyst, then
the null model must be rejected.

\begin{equation}
-??2 ln \Lambda_d = [\mbox{M2LL under H0 model}] - [\mbox{M2LL under HA model}] 
\end{equation}

These test statistics follow a chi-square distribution with the degrees of freedom
computed as the difference of the LRT degrees of freedom.
\begin{equation}	
\nu_ = [ \mbox{LRT df under H0 model}] - [\mbox{ LRT df under HA model}]
\end{equation}	

%	\begin{framed}
%		\begin{verbatim}
%	
%		> anova(MCS1,MCS2)
%	
%		Model df AIC BIC logLik Test L.Ratio p-value
%		MCS1 1 8 4077.5 4111.3 -2030.7
%		MCS2 2 7 4075.6 4105.3 -2030.8 1 vs 2 0.15291 0.6958
%		
%		\end{verbatim}
%	\end{framed}
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Model   &      df &   AIC  & BIC      & logLik & Test & L.Ratio & p-value \\ \hline
		MCS1    &       8 & 4077.5 & 4111.3 & -2030.7  &       &         &        \\ \hline
		MCS2    &       7 & 4075.6 & 4105.3 & -2030.8  & 1 vs 2 & 0.15291 & 0.6958 \\
		\hline 
	\end{tabular} 
\end{center}
\begin{framed}	
	\begin{verbatim}
	
	#ANOVAs
	test1 = anova(fit1,fit2) # Between-Subject Variabilities
	test2 = anova(fit1,fit3) # Within-Subject Variabilities
	test3 = anova(fit1,fit4) # Overall Variabilities
	
	\end{verbatim}
\end{framed}

	To perform a likelihood ratio test for two candidate models, simply use the \texttt{anova()} command with the names of the candidate models as arguments. The following piece of code implement the first of Roy's variability tests.
	
	\begin{framed}
		\begin{verbatim}
		> anova(MCS1,MCS2)
		Model df    AIC    BIC  logLik   Test L.Ratio p-value
		MCS1     1  8 4077.5 4111.3 -2030.7
		MCS2     2  7 4075.6 4105.3 -2030.8 1 vs 2 0.15291  0.6958
		>
		\end{verbatim}
	\end{framed}


%Repeatability (Barnhart pg 7)
%ISOs definition: Closeness of agreement netween measures under the same conditions. (i.e. True Replicates)

The matter of how well two methods of measurement are said to be “in agreement” is a frequently posed question in statistical literature. A useful, and broadly consistent, set of definitions of what this “agreement” entail is put forth by Barnhart et al and Roy (2009). 
As pointed out by earlier contributors to the subject ( commonly referred to as “Method Comparison Studies”)

Shared with previous contributions (Bland and Altman, Carstensen) is the condition that there should no systematic  tendency for one of the methods to consistently provide a value higher that than of the other method. If such a tendency did exist, we would refer to it as an inter-method bias.

In earlier literature, the emphasis was placed up on single measurements simultaneously by each of the methods of measurement. Several different approaches, such as the Bland-Altman plot, and Orthogonal Regression (a special case of Deming Regression where the residual variances are assumed to be equal) have been proposed. Arguably, for the single replicate case, the established methodologies are sufficient for assessing agreement between two methods.

In subsequent contributions, the matter of assessing agreement in the presence  of replicate measurements was addressed. Some approaches extended already established approaches (Bland-Altam 1999).  Other contributions were based on methodologies not seen previously in Method comparison Study Literature  (for example, Carstensen et al 2008 and Roy 2009, using LME models). 

\section{Worked Eamples : LikelihoodRatio Tests}


The likelihood Ratio test is very simple to implement in \texttt{R}. All that is required it to specify the reference model and the relevant nested mode as arguments to the command \texttt{anova()}.
	
The figure below displays the three tests described by Roy (2009).
	
	\begin{framed}
		\begin{verbatim}
		> # Between-Subject Variabilities
		> testB    = anova(Ref.Fit,NMB.fit) 
		>         
		> # Within-Subject Variabilities                
		> testW   = anova(Ref.Fit,NMW.fit) 
		>                       
		> # Overall Variabilities
		> testO     = anova(Ref.Fit,NMO.fit)                        
		
		
		\end{verbatim}
	\end{framed}
	
	
	
	
	%-----------------------------------------------------------------------------------------------------%
	
	
	
	\begin{framed}   
		\begin{verbatim}
		> anova(MCS1,MCS2)
		>
		>
		Model df    AIC    BIC  logLik   Test L.Ratio p-value
		MCS1     1  8 4077.5 4111.3 -2030.7
		MCS2     2  7 4075.6 4105.3 -2030.8 1 vs 2 0.15291  0.6958
		\end{verbatim}
	\end{framed}
	
	\subsection{Nested Model (Between-Item Variability)}
	\begin{framed}
		\begin{verbatim}
		> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
		+     random = list(item=pdCompSymm(~ meth-1)),
		+     correlation = corSymm(form=~1 | item/repl), 
		+     method="ML")
		\end{verbatim}
	\end{framed}
	
	
	
	
	%-------------------------------------------------
	\begin{itemize}
		\item \textbf{Blood (JSR) data:} 
		\item \textbf{PEFR Data:} ARoy20092009
		\item \textbf{Oximetry data:} BXC2004
		\item \textbf{Fat data:} BXC2004
		\item \textbf{Trig Gerber Data:} BXC2008
		\item \textbf{Nadler Hurley:}
		\item \textbf{Hamlett:}
	\end{itemize}



	\section{results}
	
	Using Carstensen's method, the standard deviations of the casewise
	differences were computed as 20.4314,20.2682,2.2608
	respectively. Using Roy's model, these deviations are estimated to
	be 20.3275, 20.1632, 2.2528 respectively.
	
	Similarly for the \texttt{fat} and \texttt{ox} data Carstensen computes the
	difference deviations as and 6.1686 , whereas under
	Roy's model they are estimated to be 6.139275202 and
	respectively.
	
	However, using the PEFR and cardiac data, differences emerge.
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		Data & Carstensen & Roy \\
		\hline
		Fat &  0.1352 & 0.1373\\
		Ox & 6.1686 & 6.1392 \\
		Blood JS & 20.4314 & 20.3275
		\\
		Blood JR & 2.26088 & 2.2528
		\\
		Blood RS & 20.2682 & 20.16326412
		\\
		Hamlett & 0.9031 & 0.8922
		
		\\
		\hline
	\end{tabular}
	\end{center}

%============================================================================================================ %	





\section{Roy's Candidate Models}
	


\begin{framed}	
\begin{verbatim}
		
		> Ref.Fit = lme(y ~ meth-1, data = dat,   #Symm , Symm#
		+     random = list(item=pdSymm(~ meth-1)), 
		+     weights=varIdent(form=~1|meth),
		+     correlation = corSymm(form=~1 | item/repl), 
		+     method="ML")
\end{verbatim}
\end{framed}
		Roy(2009) presents two nested models that specify the condition of equality as required, with a third nested model for an additional test. There three formulations share the same structure, and can be specified by making slight alterations of the code for the Reference Model.
		Nested Model (Between-Item Variability)


\begin{framed}	\begin{verbatim}
		> NMB.fit  = lme(y ~ meth-1, data = dat,   #CS , Symm#
		+     random = list(item=pdCompSymm(~ meth-1)),
		+     correlation = corSymm(form=~1 | item/repl), 
		+     method="ML")
		
		
		Nested Model (Within –item Variability)
		> NMW.fit = lme(y ~ meth-1, data = dat,   #Symm , CS# 
		+     random = list(item=pdSymm(~ meth-1)),
		+     weights=varIdent(form=~1|meth), 
		+     correlation = corCompSymm(form=~1 | item/repl), 
		+     method="ML")
\end{verbatim}
\end{framed}
		

	
	\begin{framed}	\begin{verbatim}	
		
		
		
		Comparison of ML and REML fits
		Fit 1 (ML)
		
		Dataset: Blood RS
		
		Fixed : 127.3126 , 143.0275
		
		AIC: 4075.594
		
		Between Subject Variability
		
		Fit1r (REML)
		
		Dataset: Blood RS
		
		Fixed : 127.3126 , 143.0275
		
		AIC: 4068.172
		
		Between Subject Variability
		
		
		
		\end{verbatim}
	\end{framed}

	
\section{Roy's PEFR Examples}

	
	%--------------------------------------------------------------------------Example 1b  ----  JSR Data %
	
	To complete the study, the relevant values are provided for the $R \mbox{vs} S$ comparison also.
	
	%--------------------------------------------------------------------------Example 2  ----  PEFR Data %
	
	The second data set, a comparison of two peak expiratory flow rate measurements, is referenced by \citet{BA86}.
	
	
	%--------------------------------------------------------------------------Example 3 Cardiac Ejection Fraction Data %
	The last case study is also based on a data set from  \citet{BA99}. It contains the measurements of left ventricular cardiac eject fraction, measured by impedance cartography and radionuclide ventriculography, on twelve patients.
	The number of replicated differs for each patient.
	
	The bias is shown to be $0.7040$, with a p-value of $0.0204$. The MLEa of the between-method and within-method variance-covariance matrices of methods $RV$ and $IC$ are given by
	
	\begin{equation}\hat{D}=\left(
	\begin{array}{cc}
	1.6323 & 1.1427 \\
	1.1427 & 1.4498 \\
	\end{array}
	\right),
	\end{equation}
	
	
	
	\begin{equation}\hat{\Sigma}=\left(
	\begin{array}{cc}
	1.6323 & 1.1427 \\
	1.1427 & 1.4498 \\
	\end{array}
	\right).
	\end{equation}
	
	\citet{ARoy2009} notes that these are the same estimate for variance as given by \citet{BA99}.
	
	
	The repeatability coefficients are determined to be $0.9080$ for the RV method and $1.0293$ for the IC method.
	
	From the estimated $\boldsymbol{\Omega_{i}}$ correlation matrix, the overall correlation coefficient is $0.7100$.
	The overall correllation coefficients between two methods RV and IC are $0.9384$ and $0.9131$ respectively.
	
\citet{ARoy2009} concludes that is appropriate to switch between the two methods if needed.
	

	
\citet{ARoy2009} recommends to not switch between the two method.
	

	
\chapter{Other Data Sets}
\section{IC/RV comparison}
		
For the the RV-IC comparison, $\hat{D}$ is given by


\begin{equation}
\hat{D}= \left[ \begin{array}{cc}
1.6323 & 1.1427  \\
1.1427 & 1.4498 \\
\end{array} \right]
\end{equation}

The estimate for the within-subject variance covariance matrix is
given by
\begin{equation}
\hat{\Sigma}= \left[ \begin{array}{cc}
0.1072 & 0.0372  \\
0.0372 & 0.1379  \\
\end{array}\right]
\end{equation}
The estimated overall variance covariance matrix for the the 'RV
vs IC' comparison is given by
\begin{equation}
Block \Omega_{i}= \left[ \begin{array}{cc}
1.7396 & 1.1799  \\
1.1799 & 1.5877  \\
\end{array} \right].
\end{equation}

The power of the
likelihood ratio test may depends on specific sample size and the
specific number of  replications, and the author proposes
simulation studies to examine this further.

	\section{PEFR and Cardiac}
	
	
	Two further data sets applied to both methodologies are the``Cardiac" and ``PEFR" , which are both contained on Carstensen's MethComp package. This data is from Bland and Altman (1986): two measurements of peak expiratory flow rate (PEFR) are compared. One of these measurements uses a ``Large" meter and the other a ``Mini" meter.
	
	Two measurements were made with a Wright peak flow meter and two with a mini Wright meter, in random order.  All measurements were taken by the same observer, using the same two instruments. (These data were collected to demonstrate the statistical method and provide no evidence on the comparability of these two instruments.)
	

	
\bibliographystyle{chicago}
\bibliography{2017bib}
\end{document}
